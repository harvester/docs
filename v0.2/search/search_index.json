{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Harvester Intro \u00b6 Harvester is an open source hyper-converged infrastructure (HCI) software built on Kubernetes. It is an open source alternative to vSphere and Nutanix. Overview \u00b6 Harvester implements HCI on bare metal servers. Here are some notable features of the Harvester: VM lifecycle management including SSH-Key injection, Cloud-init and, graphic and serial port console VM live migration support Supporting VM backup and restore Distributed block storage Multiple NICs in the VM connecting to the management network or VLANs Virtual Machine and cloud-init templates Built-in Rancher integration and the Harvester node driver PXE/iPXE boot support The following diagram gives a high-level architecture of Harvester: Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes. KubeVirt is a virtual machine management add-on for Kubernetes. K3OS is a Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster. Hardware Requirements \u00b6 To get the Harvester server up and running the following minimum hardware is required: Type Requirements CPU x86_64 only. Hardware assisted virtualization required. 4 cores minimum, 16 cores or above preferred Memory 8 GB minimum, 32 GB or above preferred Disk Capacity 120 GB minimum, 500 GB or above preferred Disk Performance 5,000+ random IOPS per disk(SSD/NVMe). Management nodes (first 3 nodes) must be fast enough for Etcd . Network Card 1 Gbps Ethernet minimum, 10Gbps Ethernet recommended Network Switch Trunking of ports required for VLAN support Quick start \u00b6 You can use the ISO to install Harvester directly on the bare-metal server to form a Harvester cluster. Users can add one or many compute nodes to join the existing cluster. To get the Harvester ISO, download it from the Github releases. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device that the Harvester will be formatted to. Configure the hostname and select the network interface for the management network, the IP address can either be configured via DHCP or static method. Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default ssh user is rancher . (Optional) you can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here, otherwise, leave this blank. (Optional) If you need to customize the host with cloud-init config, enter the HTTP URL here. Confirm the installation options and the Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete it will restart the host and a console UI with management URL and status will be displayed. (You can Use F12 to switch between Harvester console and the Shell) The default URL of the web interface is https://your-host-ip:30443 . User will be prompted to set the password for the default admin user on the first-time login.","title":"Harvester Intro"},{"location":"#harvester-intro","text":"Harvester is an open source hyper-converged infrastructure (HCI) software built on Kubernetes. It is an open source alternative to vSphere and Nutanix.","title":"Harvester Intro"},{"location":"#overview","text":"Harvester implements HCI on bare metal servers. Here are some notable features of the Harvester: VM lifecycle management including SSH-Key injection, Cloud-init and, graphic and serial port console VM live migration support Supporting VM backup and restore Distributed block storage Multiple NICs in the VM connecting to the management network or VLANs Virtual Machine and cloud-init templates Built-in Rancher integration and the Harvester node driver PXE/iPXE boot support The following diagram gives a high-level architecture of Harvester: Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes. KubeVirt is a virtual machine management add-on for Kubernetes. K3OS is a Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster.","title":"Overview"},{"location":"#hardware-requirements","text":"To get the Harvester server up and running the following minimum hardware is required: Type Requirements CPU x86_64 only. Hardware assisted virtualization required. 4 cores minimum, 16 cores or above preferred Memory 8 GB minimum, 32 GB or above preferred Disk Capacity 120 GB minimum, 500 GB or above preferred Disk Performance 5,000+ random IOPS per disk(SSD/NVMe). Management nodes (first 3 nodes) must be fast enough for Etcd . Network Card 1 Gbps Ethernet minimum, 10Gbps Ethernet recommended Network Switch Trunking of ports required for VLAN support","title":"Hardware Requirements"},{"location":"#quick-start","text":"You can use the ISO to install Harvester directly on the bare-metal server to form a Harvester cluster. Users can add one or many compute nodes to join the existing cluster. To get the Harvester ISO, download it from the Github releases. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device that the Harvester will be formatted to. Configure the hostname and select the network interface for the management network, the IP address can either be configured via DHCP or static method. Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default ssh user is rancher . (Optional) you can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here, otherwise, leave this blank. (Optional) If you need to customize the host with cloud-init config, enter the HTTP URL here. Confirm the installation options and the Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete it will restart the host and a console UI with management URL and status will be displayed. (You can Use F12 to switch between Harvester console and the Shell) The default URL of the web interface is https://your-host-ip:30443 . User will be prompted to set the password for the default admin user on the first-time login.","title":"Quick start"},{"location":"authentication/","text":"Authentication \u00b6 Available as of v0.2.0 With ISO installation mode, user will be prompted to set the password for the default admin user on the first-time login. The Harvester login page is shown below: Developer Mode \u00b6 In developer mode , which is intended only for development and testing purposes, more authentication modes are configurable using the environment variable HARVESTER_AUTHENTICATION_MODE . By default, the Harvester Dashboard uses the local auth mode for authentication. The default username and password is admin/password . The currently supported options are localUser (the same as local auth mode) and kubernetesCredentials . If the kubernetesCredentials authentication option is used, either a kubeconfig file or bearer token can provide access to Harvester.","title":"Authentication"},{"location":"authentication/#authentication","text":"Available as of v0.2.0 With ISO installation mode, user will be prompted to set the password for the default admin user on the first-time login. The Harvester login page is shown below:","title":"Authentication"},{"location":"authentication/#developer-mode","text":"In developer mode , which is intended only for development and testing purposes, more authentication modes are configurable using the environment variable HARVESTER_AUTHENTICATION_MODE . By default, the Harvester Dashboard uses the local auth mode for authentication. The default username and password is admin/password . The currently supported options are localUser (the same as local auth mode) and kubernetesCredentials . If the kubernetesCredentials authentication option is used, either a kubeconfig file or bearer token can provide access to Harvester.","title":"Developer Mode"},{"location":"harvester-network/","text":"Harvester Network \u00b6 Harvester is built on Kubernetes, which uses CNI as an interface between network providers and Kubernetes pod networking. Naturally, we implement the Harvester network based on CNI. Moreover, the Harvester UI integrates the Harvester network to provide a user-friendly way to configure networks for VMs. By version 0.2, Harvester supports two kinds of networks: management network VLAN Implementation \u00b6 Management Network \u00b6 Harvester adopts flannel as the default CNI to implement the management network. It's an internal network, which means the user can only access the VM's management network within its cluster nodes or pods. VLAN \u00b6 Harvester network-controller leverages the multus and bridge CNI plugins to implement the VLAN. Below is a use case of the VLAN in Harvester. Harvester network-controller uses a bridge for a node and a pair of veth for a VM to implement the VLAN. The bridge acts as a switch to forward the network traffic from or to VMs and the veth pair is like the connected ports between vms and switch. VMs within the same VLAN is able to communicate with each other, while the VMs within different VLANs can't. The external switch ports connected with the hosts or other devices(such as DHCP server) should be set as trunk or hybrid type and permit the specified VLANs. Users can use VLAN with PVID (default 1) to communicate with any normal untagged traffic. Enabling VLAN in the Harvester UI \u00b6 Enable VLAN via going to Setting > vlan to enable VLAN and input a valid default physical NIC name for the VLAN. The first physical NIC name of each Harvester node always defaults to eth0. It is recommended to choose a separate NIC for the VLAN other than the one used for the management network (the one selected during the Harvester installation) for better network performance and isolation. Note: Modifying the default VLAN network setting will not change the existing configured host networks. (optional) Users can always customize each node's VLAN network configuration via going to the HOST > Network tab. A new VLAN network is created by going to the Advanced > Networks page and clicking the Create button. The network is configured when the VM is created. Only the first network interface card will be enabled by default. Users can either choose to use a management network or VLAN network. Note: You will need to select the Install guest agent option in the Advanced Options tab to get the VLAN network IP address from the Harvester UI. Users can choose to add one or multiple network interface cards. Additional network interface card configurations can be set via cloud-init network data, e.g.: version : 1 config : - type : physical name : enp1s0 # name is varies upon OS image subnets : - type : dhcp - type : physical name : enp2s0 subnets : - type : DHCP","title":"Harvester Network"},{"location":"harvester-network/#harvester-network","text":"Harvester is built on Kubernetes, which uses CNI as an interface between network providers and Kubernetes pod networking. Naturally, we implement the Harvester network based on CNI. Moreover, the Harvester UI integrates the Harvester network to provide a user-friendly way to configure networks for VMs. By version 0.2, Harvester supports two kinds of networks: management network VLAN","title":"Harvester Network"},{"location":"harvester-network/#implementation","text":"","title":"Implementation"},{"location":"harvester-network/#management-network","text":"Harvester adopts flannel as the default CNI to implement the management network. It's an internal network, which means the user can only access the VM's management network within its cluster nodes or pods.","title":"Management Network"},{"location":"harvester-network/#vlan","text":"Harvester network-controller leverages the multus and bridge CNI plugins to implement the VLAN. Below is a use case of the VLAN in Harvester. Harvester network-controller uses a bridge for a node and a pair of veth for a VM to implement the VLAN. The bridge acts as a switch to forward the network traffic from or to VMs and the veth pair is like the connected ports between vms and switch. VMs within the same VLAN is able to communicate with each other, while the VMs within different VLANs can't. The external switch ports connected with the hosts or other devices(such as DHCP server) should be set as trunk or hybrid type and permit the specified VLANs. Users can use VLAN with PVID (default 1) to communicate with any normal untagged traffic.","title":"VLAN"},{"location":"harvester-network/#enabling-vlan-in-the-harvester-ui","text":"Enable VLAN via going to Setting > vlan to enable VLAN and input a valid default physical NIC name for the VLAN. The first physical NIC name of each Harvester node always defaults to eth0. It is recommended to choose a separate NIC for the VLAN other than the one used for the management network (the one selected during the Harvester installation) for better network performance and isolation. Note: Modifying the default VLAN network setting will not change the existing configured host networks. (optional) Users can always customize each node's VLAN network configuration via going to the HOST > Network tab. A new VLAN network is created by going to the Advanced > Networks page and clicking the Create button. The network is configured when the VM is created. Only the first network interface card will be enabled by default. Users can either choose to use a management network or VLAN network. Note: You will need to select the Install guest agent option in the Advanced Options tab to get the VLAN network IP address from the Harvester UI. Users can choose to add one or multiple network interface cards. Additional network interface card configurations can be set via cloud-init network data, e.g.: version : 1 config : - type : physical name : enp1s0 # name is varies upon OS image subnets : - type : dhcp - type : physical name : enp2s0 subnets : - type : DHCP","title":"Enabling VLAN in the Harvester UI"},{"location":"import-image/","text":"Import Images \u00b6 To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Note: The image name will be auto-filled using the URL address's filename. You can always customize it when required. Currently, we support qcow2, raw, and ISO images. Todo Uploading images from UI to the Harvester cluster is not supported yet. The feature request is being tracked on #570 . Description and labels are optional.","title":"Import Images"},{"location":"import-image/#import-images","text":"To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Note: The image name will be auto-filled using the URL address's filename. You can always customize it when required. Currently, we support qcow2, raw, and ISO images. Todo Uploading images from UI to the Harvester cluster is not supported yet. The feature request is being tracked on #570 . Description and labels are optional.","title":"Import Images"},{"location":"upgrade/","text":"Upgrading Harvester \u00b6 Available as of v0.2.0 Harvester provides two ways to upgrade. Note Upgrading from v0.1.0 is not supported. Upgrading from/to any RC releases is not supported. Due to pending OS changes in the roadmap, the upgradability to the GA release cannot be guaranteed at this moment. Zero downtime upgrade is supported when the Harvester cluster includes 3 or more nodes. If the cluster has less than 3 nodes, you can still perform upgrades but there'll be downtime during the upgrade. Upgrade nodes one by one. Upgrade using the ISO image \u00b6 To get the Harvester ISO of a newer version, download it from the Github releases. Go to the Hosts page in Harvester UI. Find the node you are about to upgrade. Click on the action dropdown. Click on the Enable Maintenance Mode action. Wait until all the VMs on the node are migrated to other nodes and the node is in Maintenance Mode status. Shut down the server, and boot the server from the ISO disk of the newer Harvester version. Select the Harvester Installer in the grub menu. Select Upgrade Harvester and confirm in the prompt. Wait until the upgrade completes. The node will be rebooted and show Ready again in the terminal console. Go to the Hosts page in Harvester UI. Find the node that just completed the upgrade. Click on the action dropdown. Click on the Disable Maintenance Mode action. For the rest nodes of the cluster, repeat steps 2 to 9 to upgrade them one by one. Upgrade in the UI \u00b6 Important Internet access is required to perform a live upgrade in the UI. Go to the Dashboard page in Harvester UI. When newer versions are available, an upgrade button will be shown in the top-right corner. Click upgrade. Select a version to upgrade. Click upgrade. Wait until the upgrade to complete. You can view the upgrade progress by clicking the circle icon in the top navigation bar.","title":"Upgrading Harvester"},{"location":"upgrade/#upgrading-harvester","text":"Available as of v0.2.0 Harvester provides two ways to upgrade. Note Upgrading from v0.1.0 is not supported. Upgrading from/to any RC releases is not supported. Due to pending OS changes in the roadmap, the upgradability to the GA release cannot be guaranteed at this moment. Zero downtime upgrade is supported when the Harvester cluster includes 3 or more nodes. If the cluster has less than 3 nodes, you can still perform upgrades but there'll be downtime during the upgrade. Upgrade nodes one by one.","title":"Upgrading Harvester"},{"location":"upgrade/#upgrade-using-the-iso-image","text":"To get the Harvester ISO of a newer version, download it from the Github releases. Go to the Hosts page in Harvester UI. Find the node you are about to upgrade. Click on the action dropdown. Click on the Enable Maintenance Mode action. Wait until all the VMs on the node are migrated to other nodes and the node is in Maintenance Mode status. Shut down the server, and boot the server from the ISO disk of the newer Harvester version. Select the Harvester Installer in the grub menu. Select Upgrade Harvester and confirm in the prompt. Wait until the upgrade completes. The node will be rebooted and show Ready again in the terminal console. Go to the Hosts page in Harvester UI. Find the node that just completed the upgrade. Click on the action dropdown. Click on the Disable Maintenance Mode action. For the rest nodes of the cluster, repeat steps 2 to 9 to upgrade them one by one.","title":"Upgrade using the ISO image"},{"location":"upgrade/#upgrade-in-the-ui","text":"Important Internet access is required to perform a live upgrade in the UI. Go to the Dashboard page in Harvester UI. When newer versions are available, an upgrade button will be shown in the top-right corner. Click upgrade. Select a version to upgrade. Click upgrade. Wait until the upgrade to complete. You can view the upgrade progress by clicking the circle icon in the top navigation bar.","title":"Upgrade in the UI"},{"location":"dev/dev-mode/","text":"Developer Mode Installation \u00b6 Developer mode (dev mode) is intended to be used for testing and development purposes. Note: This video shows the dev mode installation. Requirements \u00b6 For dev mode, it is assumed that Multus is installed across your cluster and a corresponding NetworkAttachmentDefinition CRD was created. For dev mode, if you are using an RKE cluster, please ensure the ipv4.ip_forward is enabled for the CNI plugin so that the pod network works as expected. #94 . Install as an App \u00b6 Harvester can be installed on a Kubernetes cluster in the following ways: Install with the Helm CLI Install as a Rancher catalog app, in which case the harvester/harvester repo is added to the Rancher Catalog as a Helm v3 app Please refer to the Harvester Helm chart for more details on installing and configuring the Helm chart. Requirements \u00b6 The Kubernetes node must have hardware virtualization support. To validate the support, use this command: cat /proc/cpuinfo | grep vmx Option 1: Install using Helm \u00b6 Clone the GitHub repository: $ git clone https://github.com/harvester/harvester.git --depth=1 Go to the Helm chart: $ cd harvester/deploy/charts Install the Harvester chart with the following commands: ### To install the chart with the release name `harvester`: ## Create the target namespace $ kubectl create ns harvester-system ## Install the chart to the target namespace $ helm install harvester harvester \\ --namespace harvester-system \\ --set longhorn.enabled = true,minio.persistence.storageClass = longhorn Option 2: Install using Rancher \u00b6 Tip: You can create a testing Kubernetes environment in Rancher using the Digital Ocean cloud provider. For details, see this section. Add the Harvester repo https://github.com/harvester/harvester to your Rancher catalogs by clicking Global > Tools > Catalogs. Specify the URL and name. Set the branch to stable if you need a stable release version. Set the Helm version to be Helm v3 . Click Create. Navigate to your project-level Apps. Click Launch and choose the Harvester app. (Optional) You can modify the configurations if needed. Otherwise, use the default options. Click Launch and wait for the app's components to be ready. Click the /index.html link to navigate to the Harvester UI: Digital Ocean Test Environment \u00b6 Digital Ocean is one of the cloud providers who support nested virtualization by default. You can create a testing Kubernetes environment in Rancher using the Digital Ocean cloud provider. We recommend using the 8 core, 16 GB RAM node, which will have nested virtualization enabled by default. This screenshot shows how to create a Rancher node template that would allow Rancher to provision such a node in Digital Ocean: For more information on how to launch Digital Ocean nodes with Rancher, refer to the Rancher documentation.","title":"Developer Mode Installation"},{"location":"dev/dev-mode/#developer-mode-installation","text":"Developer mode (dev mode) is intended to be used for testing and development purposes. Note: This video shows the dev mode installation.","title":"Developer Mode Installation"},{"location":"dev/dev-mode/#requirements","text":"For dev mode, it is assumed that Multus is installed across your cluster and a corresponding NetworkAttachmentDefinition CRD was created. For dev mode, if you are using an RKE cluster, please ensure the ipv4.ip_forward is enabled for the CNI plugin so that the pod network works as expected. #94 .","title":"Requirements"},{"location":"dev/dev-mode/#install-as-an-app","text":"Harvester can be installed on a Kubernetes cluster in the following ways: Install with the Helm CLI Install as a Rancher catalog app, in which case the harvester/harvester repo is added to the Rancher Catalog as a Helm v3 app Please refer to the Harvester Helm chart for more details on installing and configuring the Helm chart.","title":"Install as an App"},{"location":"dev/dev-mode/#requirements_1","text":"The Kubernetes node must have hardware virtualization support. To validate the support, use this command: cat /proc/cpuinfo | grep vmx","title":"Requirements"},{"location":"dev/dev-mode/#option-1-install-using-helm","text":"Clone the GitHub repository: $ git clone https://github.com/harvester/harvester.git --depth=1 Go to the Helm chart: $ cd harvester/deploy/charts Install the Harvester chart with the following commands: ### To install the chart with the release name `harvester`: ## Create the target namespace $ kubectl create ns harvester-system ## Install the chart to the target namespace $ helm install harvester harvester \\ --namespace harvester-system \\ --set longhorn.enabled = true,minio.persistence.storageClass = longhorn","title":"Option 1: Install using Helm"},{"location":"dev/dev-mode/#option-2-install-using-rancher","text":"Tip: You can create a testing Kubernetes environment in Rancher using the Digital Ocean cloud provider. For details, see this section. Add the Harvester repo https://github.com/harvester/harvester to your Rancher catalogs by clicking Global > Tools > Catalogs. Specify the URL and name. Set the branch to stable if you need a stable release version. Set the Helm version to be Helm v3 . Click Create. Navigate to your project-level Apps. Click Launch and choose the Harvester app. (Optional) You can modify the configurations if needed. Otherwise, use the default options. Click Launch and wait for the app's components to be ready. Click the /index.html link to navigate to the Harvester UI:","title":"Option 2: Install using Rancher"},{"location":"dev/dev-mode/#digital-ocean-test-environment","text":"Digital Ocean is one of the cloud providers who support nested virtualization by default. You can create a testing Kubernetes environment in Rancher using the Digital Ocean cloud provider. We recommend using the 8 core, 16 GB RAM node, which will have nested virtualization enabled by default. This screenshot shows how to create a Rancher node template that would allow Rancher to provision such a node in Digital Ocean: For more information on how to launch Digital Ocean nodes with Rancher, refer to the Rancher documentation.","title":"Digital Ocean Test Environment"},{"location":"install/harvester-configuration/","text":"Harvester Configuration \u00b6 Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is an configuration example: server_url : https://someserver:6443 token : TOKEN_VALUE os : ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username hostname : myhost modules : - kvm - nvme sysctl : kernel.printk : \"4 4 1 7\" kernel.kptr_restrict : \"1\" dns_nameservers : - 8.8.8.8 - 1.1.1.1 ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org wifi : - name : home passphrase : mypassword - name : nothome passphrase : somethingelse password : rancher environment : http_proxy : http://myserver https_proxy : http://myserver install : mode : create mgmtInterface : eth0 force_efi : true device : /dev/vda silent : true iso_url : http://myserver/test.iso poweroff : true no_format : true debug : true tty : ttyS0 Configuration Reference \u00b6 Below is a reference of all configuration keys. Warning Security Risks : The configuration file contains credentials which should be kept secretly. Please do not make the configuration file publicly accessible at the moment. server_url \u00b6 The URL of the Harvester server to join as an agent. This configuration is mandatory when the installation is in JOIN mode. It tells the Harvester installer where the main server is. Example server_url : https://someserver:6443 install : mode : join token \u00b6 The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match from what server has. Example token : myclustersecret Or a node token token : \"K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4\" os.ssh_authorized_keys \u00b6 A list of SSH authorized keys that should be added to the default user rancher . SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME} . This is done by downloading the keys from https://github.com/${USERNAME}.keys . Example os : ssh_authorized_keys : - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D\" - \"github:ibuildthecloud\" os.hostname \u00b6 Set the system hostname. This value will be overwritten by DHCP if DHCP supplies a hostname for the system. If DHCP doesn't offer a hostname and this value is empty, a random hostname will be generated. Example os : hostname : myhostname os.modules \u00b6 A list of kernel modules to be loaded on start. Example os : modules : - kvm - nvme os.sysctls \u00b6 Kernel sysctl to setup on start. These are the same configuration you'd typically find in /etc/sysctl.conf . Must be specified as string values. os : sysctl : kernel.printk : 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict : \"1\" # force the YAML parser to read as a string os.ntp_servers \u00b6 Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Example os : ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org os.dns_nameservers \u00b6 Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS. Example os : dns_nameservers : - 8.8.8.8 - 1.1.1.1 os.wifi \u00b6 Simple wifi configuration. All that is accepted is name and passphrase . Example: os : wifi : - name : home passphrase : mypassword - name : nothome passphrase : somethingelse os.password \u00b6 The password for the default user rancher . By default there is no password for the rancher user. If you set a password at runtime it will be reset on next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to just change your password on a Linux system and copy the value of the second field from /etc/shadow . You can also encrypt a password using openssl passwd -1 . Example os : password : \"$1$tYtghCfK$QHa51MS6MVAcfUKuOzNKt0\" Or clear text os : password : supersecure os.environment \u00b6 Environment variables to be set on k3s and other processes like the boot process. Primary use of this field is to set the http proxy. Example os : environment : http_proxy : http://myserver https_proxy : http://myserver install.mode \u00b6 Harvester installer mode: create : Creating a new Harvester installer join : Join an existing Harvester installer. Need to specify server_url . Example install : mode : create install.mgmtInterface \u00b6 The interface that used to build VM fabric network. Example install : mgmtInterface : eth0 install.force_efi \u00b6 Force EFI installation even when EFI is not detected. Default: false . install.device \u00b6 The device to install the OS. install.silent \u00b6 Reserved. install.iso_url \u00b6 ISO to download and install from if booting from kernel/vmlinuz and not ISO. install.poweroff \u00b6 Shutdown the machine after install instead of rebooting install.no_format \u00b6 Do not partition and format, assume layout exists already. install.debug \u00b6 Run installation with more logging and configure debug for installed system. install.tty \u00b6 The tty device used for console. Example install : tty : ttyS0,115200n8","title":"Harvester Configuration"},{"location":"install/harvester-configuration/#harvester-configuration","text":"Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is an configuration example: server_url : https://someserver:6443 token : TOKEN_VALUE os : ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username hostname : myhost modules : - kvm - nvme sysctl : kernel.printk : \"4 4 1 7\" kernel.kptr_restrict : \"1\" dns_nameservers : - 8.8.8.8 - 1.1.1.1 ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org wifi : - name : home passphrase : mypassword - name : nothome passphrase : somethingelse password : rancher environment : http_proxy : http://myserver https_proxy : http://myserver install : mode : create mgmtInterface : eth0 force_efi : true device : /dev/vda silent : true iso_url : http://myserver/test.iso poweroff : true no_format : true debug : true tty : ttyS0","title":"Harvester Configuration"},{"location":"install/harvester-configuration/#configuration-reference","text":"Below is a reference of all configuration keys. Warning Security Risks : The configuration file contains credentials which should be kept secretly. Please do not make the configuration file publicly accessible at the moment.","title":"Configuration Reference"},{"location":"install/harvester-configuration/#server_url","text":"The URL of the Harvester server to join as an agent. This configuration is mandatory when the installation is in JOIN mode. It tells the Harvester installer where the main server is. Example server_url : https://someserver:6443 install : mode : join","title":"server_url"},{"location":"install/harvester-configuration/#token","text":"The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match from what server has. Example token : myclustersecret Or a node token token : \"K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4\"","title":"token"},{"location":"install/harvester-configuration/#osssh_authorized_keys","text":"A list of SSH authorized keys that should be added to the default user rancher . SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME} . This is done by downloading the keys from https://github.com/${USERNAME}.keys . Example os : ssh_authorized_keys : - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D\" - \"github:ibuildthecloud\"","title":"os.ssh_authorized_keys"},{"location":"install/harvester-configuration/#oshostname","text":"Set the system hostname. This value will be overwritten by DHCP if DHCP supplies a hostname for the system. If DHCP doesn't offer a hostname and this value is empty, a random hostname will be generated. Example os : hostname : myhostname","title":"os.hostname"},{"location":"install/harvester-configuration/#osmodules","text":"A list of kernel modules to be loaded on start. Example os : modules : - kvm - nvme","title":"os.modules"},{"location":"install/harvester-configuration/#ossysctls","text":"Kernel sysctl to setup on start. These are the same configuration you'd typically find in /etc/sysctl.conf . Must be specified as string values. os : sysctl : kernel.printk : 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict : \"1\" # force the YAML parser to read as a string","title":"os.sysctls"},{"location":"install/harvester-configuration/#osntp_servers","text":"Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Example os : ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org","title":"os.ntp_servers"},{"location":"install/harvester-configuration/#osdns_nameservers","text":"Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS. Example os : dns_nameservers : - 8.8.8.8 - 1.1.1.1","title":"os.dns_nameservers"},{"location":"install/harvester-configuration/#oswifi","text":"Simple wifi configuration. All that is accepted is name and passphrase . Example: os : wifi : - name : home passphrase : mypassword - name : nothome passphrase : somethingelse","title":"os.wifi"},{"location":"install/harvester-configuration/#ospassword","text":"The password for the default user rancher . By default there is no password for the rancher user. If you set a password at runtime it will be reset on next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to just change your password on a Linux system and copy the value of the second field from /etc/shadow . You can also encrypt a password using openssl passwd -1 . Example os : password : \"$1$tYtghCfK$QHa51MS6MVAcfUKuOzNKt0\" Or clear text os : password : supersecure","title":"os.password"},{"location":"install/harvester-configuration/#osenvironment","text":"Environment variables to be set on k3s and other processes like the boot process. Primary use of this field is to set the http proxy. Example os : environment : http_proxy : http://myserver https_proxy : http://myserver","title":"os.environment"},{"location":"install/harvester-configuration/#installmode","text":"Harvester installer mode: create : Creating a new Harvester installer join : Join an existing Harvester installer. Need to specify server_url . Example install : mode : create","title":"install.mode"},{"location":"install/harvester-configuration/#installmgmtinterface","text":"The interface that used to build VM fabric network. Example install : mgmtInterface : eth0","title":"install.mgmtInterface"},{"location":"install/harvester-configuration/#installforce_efi","text":"Force EFI installation even when EFI is not detected. Default: false .","title":"install.force_efi"},{"location":"install/harvester-configuration/#installdevice","text":"The device to install the OS.","title":"install.device"},{"location":"install/harvester-configuration/#installsilent","text":"Reserved.","title":"install.silent"},{"location":"install/harvester-configuration/#installiso_url","text":"ISO to download and install from if booting from kernel/vmlinuz and not ISO.","title":"install.iso_url"},{"location":"install/harvester-configuration/#installpoweroff","text":"Shutdown the machine after install instead of rebooting","title":"install.poweroff"},{"location":"install/harvester-configuration/#installno_format","text":"Do not partition and format, assume layout exists already.","title":"install.no_format"},{"location":"install/harvester-configuration/#installdebug","text":"Run installation with more logging and configure debug for installed system.","title":"install.debug"},{"location":"install/harvester-configuration/#installtty","text":"The tty device used for console. Example install : tty : ttyS0,115200n8","title":"install.tty"},{"location":"install/iso-install/","text":"ISO Installation \u00b6 To get the Harvester ISO, download it from the Github releases. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device that the Harvester will be formatted to. Configure the hostname and select the network interface for the management network, the IP address can either be configured via DHCP or static method. Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default ssh user is rancher . (Optional) you can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here, otherwise, leave this blank. (Optional) If you need to customize the host with cloud-init config, enter the HTTP URL here. Confirm the installation options and the Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete it will restart the host and a console UI with management URL and status will be displayed. (You can Use F12 to switch between Harvester console and the Shell) The default URL of the web interface is https://your-host-ip:30443 . User will be prompted to set the password for the default admin user on the first-time login.","title":"ISO Installation"},{"location":"install/iso-install/#iso-installation","text":"To get the Harvester ISO, download it from the Github releases. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device that the Harvester will be formatted to. Configure the hostname and select the network interface for the management network, the IP address can either be configured via DHCP or static method. Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default ssh user is rancher . (Optional) you can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here, otherwise, leave this blank. (Optional) If you need to customize the host with cloud-init config, enter the HTTP URL here. Confirm the installation options and the Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete it will restart the host and a console UI with management URL and status will be displayed. (You can Use F12 to switch between Harvester console and the Shell) The default URL of the web interface is https://your-host-ip:30443 . User will be prompted to set the password for the default admin user on the first-time login.","title":"ISO Installation"},{"location":"install/pxe-boot-install/","text":"PXE Boot Install \u00b6 Starting from version 0.2.0 , Harvester can be installed in a mass manner. This document provides an example to do the automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If NIC cards don't come with iPXE firmware, iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit https://github.com/harvester/ipxe-examples. Preparing HTTP Servers \u00b6 An HTTP server is required to serve boot files. Please ensure these servers are set up correctly before continuing. Let's assume an NGINX HTTP server's IP is 10.100.0.10 , and it serves /usr/share/nginx/html/ folder at the path http://10.100.0.10/ . Preparing Boot Files \u00b6 Download the required files from https://github.com/harvester/harvester/releases. Choose an appropriate version. The ISO: harvester-amd64.iso The kernel: harvester-vmlinuz-amd64 The initrd: harvester-initrd-amd64 Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. e.g., sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-initrd-amd64 /usr/share/nginx/html/harvester/ Preparing iPXE boot scripts \u00b6 When performing automatic installation, there are two modes: CREATE : we are installing a node to construct an initial Harvester cluster. JOIN : we are installing a node to join an existing Harvester cluster. Prerequisite \u00b6 Nodes need to have at least 8G of RAM because the full ISO file is loaded into tmpfs during the installation. CREATE mode \u00b6 Warning Security Risks : The configuration file below contains credentials which should be kept secretly. Please do not make the configuration file publicly accessible at the moment. Create a Harvester configuration file config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml token : token os : hostname : node1 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin password : rancher install : mode : create mgmt_interface : eth0 device : /dev/sda iso_url : http://10.100.0.10/harvester-amd64.iso For machines that needs to be installed as CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel vmlinuz k3os.mode=install console=ttyS0 console=tty1 harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd initrd boot Let's assume the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create JOIN mode \u00b6 Warning Security Risks : The configuration file below contains credentials which should be kept secretly. Please do not make the configuration file publicly accessible at the moment. Create a Harvester configuration file config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml server_url : https://10.100.0.130:6443 token : token os : hostname : node2 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin dns_nameservers : - 1.1.1.1 - 8.8.8.8 password : rancher install : mode : join mgmt_interface : eth0 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-amd64.iso Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel vmlinuz k3os.mode=install console=ttyS0 console=tty1 harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd initrd boot Let's assume the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join . TROUBLESHOOTING Sometimes the installer might be not able to fetch the Harvester configuration file because the network stack is not ready yet. To work around this, please add a boot_cmd parameter to the iPXE script, e.g., #!ipxe kernel vmlinuz k3os.mode=install console=ttyS0 console=tty1 harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml boot_cmd=\"echo include_ping_test=yes >> /etc/conf.d/net-online\" initrd initrd boot DHCP server configuration \u00b6 Here is an example to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16 ; subnet 10 .100.0.0 netmask 255 .255.255.0 { option routers 10 .100.0.10 ; option domain-name-servers 192 .168.2.1 ; range 10 .100.0.100 10 .100.0.253 ; } group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } group { # join group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-join-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-join\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node2 { hardware ethernet 52 :54:00:69:d5:92 ; } } The config file declares a subnet and two groups. The first group is for hosts to boot with CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client, it also offers the iPXE image according to client architecture. Please prepare those images and a tftp server first. Harvester configuration \u00b6 For more information about Harvester configuration, please refer to the Harvester configuration . Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, the user can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file. UEFI HTTP Boot support \u00b6 UEFI firmware supports loading a boot image from HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform the automatic installation. Serve the iPXE program \u00b6 Download the iPXE uefi program from http://boot.ipxe.org/ipxe.efi and make ipxe.efi can be downloaded from the HTTP server. e.g.: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi DHCP server configuration \u00b6 If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provides the iPXE program URL when it sees such a request. Here is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } elsif substring ( option vendor-class-identifier, 0 , 10 ) = \"HTTPClient\" { # UEFI HTTP Boot option vendor-class-identifier \"HTTPClient\" ; filename \"http://10.100.0.10/harvester/ipxe.efi\" ; } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from URL http://10.100.0.10/harvester/ipxe-create-efi . The iPXE script for UEFI boot \u00b6 It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. Here is an updated version of iPXE script for CREATE mode. #!ipxe kernel vmlinuz initrd=initrd k3os.mode=install console=ttyS0 console=tty1 harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd initrd boot The parameter initrd=initrd is required for initrd to be chrooted.","title":"PXE Boot Install"},{"location":"install/pxe-boot-install/#pxe-boot-install","text":"Starting from version 0.2.0 , Harvester can be installed in a mass manner. This document provides an example to do the automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If NIC cards don't come with iPXE firmware, iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit https://github.com/harvester/ipxe-examples.","title":"PXE Boot Install"},{"location":"install/pxe-boot-install/#preparing-http-servers","text":"An HTTP server is required to serve boot files. Please ensure these servers are set up correctly before continuing. Let's assume an NGINX HTTP server's IP is 10.100.0.10 , and it serves /usr/share/nginx/html/ folder at the path http://10.100.0.10/ .","title":"Preparing HTTP Servers"},{"location":"install/pxe-boot-install/#preparing-boot-files","text":"Download the required files from https://github.com/harvester/harvester/releases. Choose an appropriate version. The ISO: harvester-amd64.iso The kernel: harvester-vmlinuz-amd64 The initrd: harvester-initrd-amd64 Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. e.g., sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-initrd-amd64 /usr/share/nginx/html/harvester/","title":"Preparing Boot Files"},{"location":"install/pxe-boot-install/#preparing-ipxe-boot-scripts","text":"When performing automatic installation, there are two modes: CREATE : we are installing a node to construct an initial Harvester cluster. JOIN : we are installing a node to join an existing Harvester cluster.","title":"Preparing iPXE boot scripts"},{"location":"install/pxe-boot-install/#prerequisite","text":"Nodes need to have at least 8G of RAM because the full ISO file is loaded into tmpfs during the installation.","title":"Prerequisite"},{"location":"install/pxe-boot-install/#create-mode","text":"Warning Security Risks : The configuration file below contains credentials which should be kept secretly. Please do not make the configuration file publicly accessible at the moment. Create a Harvester configuration file config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml token : token os : hostname : node1 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin password : rancher install : mode : create mgmt_interface : eth0 device : /dev/sda iso_url : http://10.100.0.10/harvester-amd64.iso For machines that needs to be installed as CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel vmlinuz k3os.mode=install console=ttyS0 console=tty1 harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd initrd boot Let's assume the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create","title":"CREATE mode"},{"location":"install/pxe-boot-install/#join-mode","text":"Warning Security Risks : The configuration file below contains credentials which should be kept secretly. Please do not make the configuration file publicly accessible at the moment. Create a Harvester configuration file config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml server_url : https://10.100.0.130:6443 token : token os : hostname : node2 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin dns_nameservers : - 1.1.1.1 - 8.8.8.8 password : rancher install : mode : join mgmt_interface : eth0 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-amd64.iso Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel vmlinuz k3os.mode=install console=ttyS0 console=tty1 harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd initrd boot Let's assume the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join . TROUBLESHOOTING Sometimes the installer might be not able to fetch the Harvester configuration file because the network stack is not ready yet. To work around this, please add a boot_cmd parameter to the iPXE script, e.g., #!ipxe kernel vmlinuz k3os.mode=install console=ttyS0 console=tty1 harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml boot_cmd=\"echo include_ping_test=yes >> /etc/conf.d/net-online\" initrd initrd boot","title":"JOIN mode"},{"location":"install/pxe-boot-install/#dhcp-server-configuration","text":"Here is an example to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16 ; subnet 10 .100.0.0 netmask 255 .255.255.0 { option routers 10 .100.0.10 ; option domain-name-servers 192 .168.2.1 ; range 10 .100.0.100 10 .100.0.253 ; } group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } group { # join group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-join-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-join\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node2 { hardware ethernet 52 :54:00:69:d5:92 ; } } The config file declares a subnet and two groups. The first group is for hosts to boot with CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client, it also offers the iPXE image according to client architecture. Please prepare those images and a tftp server first.","title":"DHCP server configuration"},{"location":"install/pxe-boot-install/#harvester-configuration","text":"For more information about Harvester configuration, please refer to the Harvester configuration . Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, the user can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file.","title":"Harvester configuration"},{"location":"install/pxe-boot-install/#uefi-http-boot-support","text":"UEFI firmware supports loading a boot image from HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform the automatic installation.","title":"UEFI HTTP Boot support"},{"location":"install/pxe-boot-install/#serve-the-ipxe-program","text":"Download the iPXE uefi program from http://boot.ipxe.org/ipxe.efi and make ipxe.efi can be downloaded from the HTTP server. e.g.: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi","title":"Serve the iPXE program"},{"location":"install/pxe-boot-install/#dhcp-server-configuration_1","text":"If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provides the iPXE program URL when it sees such a request. Here is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } elsif substring ( option vendor-class-identifier, 0 , 10 ) = \"HTTPClient\" { # UEFI HTTP Boot option vendor-class-identifier \"HTTPClient\" ; filename \"http://10.100.0.10/harvester/ipxe.efi\" ; } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from URL http://10.100.0.10/harvester/ipxe-create-efi .","title":"DHCP server configuration"},{"location":"install/pxe-boot-install/#the-ipxe-script-for-uefi-boot","text":"It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. Here is an updated version of iPXE script for CREATE mode. #!ipxe kernel vmlinuz initrd=initrd k3os.mode=install console=ttyS0 console=tty1 harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd initrd boot The parameter initrd=initrd is required for initrd to be chrooted.","title":"The iPXE script for UEFI boot"},{"location":"rancher/node-driver/","text":"Create Cluster \u00b6 Now users can access the Rancher UI from Harvester, spin up Kubernetes clusters on top of the Harvester cluster, and manage them there. Important VLAN network is required for Harvester node driver From the Global view, click Add Cluster . Click Harvester . Select a Template . Fill out the rest of the form for creating a cluster. Click Create . See launching kubernetes and provisioning nodes in an infrastructure provider for more info. Create Node Template \u00b6 You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure Account Access . For Harvester embedding Rancher, you can choose Internal Harvester , which will use the harvester.harvester-system as the default Host , 8443 as the default Port . Configure Instance Options Configure the CPU, memory, disk, and disk bus. Select an OS image that is compatible with the cloud-init config. Select a network that the node driver is able to connect to, currently only VLAN is supported. Enter the SSH User, the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu . Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more info. How to add Harvester Node Driver in dev mode \u00b6 Developer mode doesn't come with Harvester node driver pre-installed. Following the steps below to install it: Navigate to the Rancher UI. From the Global view, choose Tools > Drivers in the navigation bar. From the Drivers page, select the Node Drivers tab. In versions before v2.2.0, you can select Node Drivers directly in the navigation bar. Click Add Node Driver . Enter Download URL ( docker-machine-driver-harvester ) and Custom UI URL ( ui-driver-harvester ). Add domains to the Whitelist Domains . Click Create .","title":"Node driver"},{"location":"rancher/node-driver/#create-cluster","text":"Now users can access the Rancher UI from Harvester, spin up Kubernetes clusters on top of the Harvester cluster, and manage them there. Important VLAN network is required for Harvester node driver From the Global view, click Add Cluster . Click Harvester . Select a Template . Fill out the rest of the form for creating a cluster. Click Create . See launching kubernetes and provisioning nodes in an infrastructure provider for more info.","title":"Create Cluster"},{"location":"rancher/node-driver/#create-node-template","text":"You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure Account Access . For Harvester embedding Rancher, you can choose Internal Harvester , which will use the harvester.harvester-system as the default Host , 8443 as the default Port . Configure Instance Options Configure the CPU, memory, disk, and disk bus. Select an OS image that is compatible with the cloud-init config. Select a network that the node driver is able to connect to, currently only VLAN is supported. Enter the SSH User, the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu . Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more info.","title":"Create Node Template"},{"location":"rancher/node-driver/#how-to-add-harvester-node-driver-in-dev-mode","text":"Developer mode doesn't come with Harvester node driver pre-installed. Following the steps below to install it: Navigate to the Rancher UI. From the Global view, choose Tools > Drivers in the navigation bar. From the Drivers page, select the Node Drivers tab. In versions before v2.2.0, you can select Node Drivers directly in the navigation bar. Click Add Node Driver . Enter Download URL ( docker-machine-driver-harvester ) and Custom UI URL ( ui-driver-harvester ). Add domains to the Whitelist Domains . Click Create .","title":"How to add Harvester Node Driver in dev mode"},{"location":"rancher/rancher-integration/","text":"Rancher Integration \u00b6 Available as of v0.2.0 Rancher is an open source multi-cluster management platform. Harvester has integrated Rancher into its HCI mode installation by default. Enable Rancher Dashboard \u00b6 Users can enable the Rancher dashboard by going to the Harvester Settings page. Click the actions of the rancher-enabled setting. Select the Enable option and click the save button. On the top-right corner the Rancher dashboard button will appear. Click the Rancher button, and it will open a new tab to navigate to the Rancher dashboard. For more detail about how to use the Rancher, you may refer to this doc . Creating K8s Clusters using the Harvester Node Driver \u00b6 Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage Kubernetes clusters. In the ISO mode, the Harvester driver has been added by default. Users can reference this doc for more details.","title":"Rancher Integration"},{"location":"rancher/rancher-integration/#rancher-integration","text":"Available as of v0.2.0 Rancher is an open source multi-cluster management platform. Harvester has integrated Rancher into its HCI mode installation by default.","title":"Rancher Integration"},{"location":"rancher/rancher-integration/#enable-rancher-dashboard","text":"Users can enable the Rancher dashboard by going to the Harvester Settings page. Click the actions of the rancher-enabled setting. Select the Enable option and click the save button. On the top-right corner the Rancher dashboard button will appear. Click the Rancher button, and it will open a new tab to navigate to the Rancher dashboard. For more detail about how to use the Rancher, you may refer to this doc .","title":"Enable Rancher Dashboard"},{"location":"rancher/rancher-integration/#creating-k8s-clusters-using-the-harvester-node-driver","text":"Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage Kubernetes clusters. In the ISO mode, the Harvester driver has been added by default. Users can reference this doc for more details.","title":"Creating K8s Clusters using the Harvester Node Driver"},{"location":"vm/access-to-the-vm/","text":"Access to the VM \u00b6 Once the VM is up and running, it can be accessed using either VNC or the serial console from the Harvester UI. Optionally, connect directly from your computer's SSH client. Access with the UI \u00b6 VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM (e.g., when using the Ubuntu minimal cloud image), the VM can be accessed with the serial console. Access using SSH \u00b6 Use the address in a terminal emulation client (such as Putty) or use the following command line to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@<ip-address-or-hostname>","title":"Access to the VM"},{"location":"vm/access-to-the-vm/#access-to-the-vm","text":"Once the VM is up and running, it can be accessed using either VNC or the serial console from the Harvester UI. Optionally, connect directly from your computer's SSH client.","title":"Access to the VM"},{"location":"vm/access-to-the-vm/#access-with-the-ui","text":"VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM (e.g., when using the Ubuntu minimal cloud image), the VM can be accessed with the serial console.","title":"Access with the UI"},{"location":"vm/access-to-the-vm/#access-using-ssh","text":"Use the address in a terminal emulation client (such as Putty) or use the following command line to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@<ip-address-or-hostname>","title":"Access using SSH"},{"location":"vm/backup-restore/","text":"VM Backup & Restore \u00b6 Available as of v0.2.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server) and they can be used to either restore a new VM or replace an existing VM. Note A backup target must be set up. For more information, see Backup Target Setup . If the BackupTarget has not been set, you\u2019ll be presented with a prompt message. Backup Target Setup \u00b6 A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings > backup-target . Parameter Type Description Type string Choose S3 or NFS Endpoint string EndPoint is a hostname or an IP address. Can be left empty for AWS S3. BucketName string Name of the bucket BucketRegion string Region of the bucket AccessKeyID string AccessKeyID is like a user-id that uniquely identifies your account. SecretAccessKey string SecretAccessKey is the password to your account. Certificate string Paste the certificate if you want to use a self-signed SSL certificate of your s3 server VirtualHostedStyle bool Use virtual-hosted-style access only, e.g., Alibaba Cloud(Aliyun) OSS Create a VM backup \u00b6 Once the backup target is set, go to the Virtual Machines page. Click Take Backup of the VM actions to create a new VM backup. Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. A notification message will be promoted, and users can go to the Advanced > Backups page to view all VM backups. The ReadyToUse status will be set to true once the Backup is complete. Users can either choose to restore a new VM or replace an existing VM using this backup. Restore a new VM using a backup \u00b6 To restore a new VM from a backup, follow these steps: Go to the Backups page. Specify the new VM name and click Create . A new VM will be restored using the backup volumes and metadata, and users can access it from the Virtual Machines page. Replace an Existing VM using a backup \u00b6 You can replace an existing VM using the backup with the same VM backup target . You can choose to either delete the previous volumes or retain them. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the Backups page. Click Create . The restore process can be viewed from the Virtual Machines page.","title":"VM Backup & Restore"},{"location":"vm/backup-restore/#vm-backup-restore","text":"Available as of v0.2.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server) and they can be used to either restore a new VM or replace an existing VM. Note A backup target must be set up. For more information, see Backup Target Setup . If the BackupTarget has not been set, you\u2019ll be presented with a prompt message.","title":"VM Backup &amp; Restore"},{"location":"vm/backup-restore/#backup-target-setup","text":"A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings > backup-target . Parameter Type Description Type string Choose S3 or NFS Endpoint string EndPoint is a hostname or an IP address. Can be left empty for AWS S3. BucketName string Name of the bucket BucketRegion string Region of the bucket AccessKeyID string AccessKeyID is like a user-id that uniquely identifies your account. SecretAccessKey string SecretAccessKey is the password to your account. Certificate string Paste the certificate if you want to use a self-signed SSL certificate of your s3 server VirtualHostedStyle bool Use virtual-hosted-style access only, e.g., Alibaba Cloud(Aliyun) OSS","title":"Backup Target Setup"},{"location":"vm/backup-restore/#create-a-vm-backup","text":"Once the backup target is set, go to the Virtual Machines page. Click Take Backup of the VM actions to create a new VM backup. Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. A notification message will be promoted, and users can go to the Advanced > Backups page to view all VM backups. The ReadyToUse status will be set to true once the Backup is complete. Users can either choose to restore a new VM or replace an existing VM using this backup.","title":"Create a VM backup"},{"location":"vm/backup-restore/#restore-a-new-vm-using-a-backup","text":"To restore a new VM from a backup, follow these steps: Go to the Backups page. Specify the new VM name and click Create . A new VM will be restored using the backup volumes and metadata, and users can access it from the Virtual Machines page.","title":"Restore a new VM using a backup"},{"location":"vm/backup-restore/#replace-an-existing-vm-using-a-backup","text":"You can replace an existing VM using the backup with the same VM backup target . You can choose to either delete the previous volumes or retain them. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the Backups page. Click Create . The restore process can be viewed from the Virtual Machines page.","title":"Replace an Existing VM using a backup"},{"location":"vm/create-vm/","text":"How to Create a VM \u00b6 Create one or more virtual machines from the Virtual Machines page. Choose the option to create either one or multiple VM instances. The VM name is required. (Optional) you can select to use the VM template. By default we have added ISO, raw, and Windows image templates. Configure the CPU and Memory of the VM. Select a custom VM image. Select SSH keys or upload a new one. To add more disks to the VM, go to the Volumes tab. The default disk will be the root disk. To configure networks, go to the Networks tab. The Management Network is added by default. It is also possible to add secondary networks to the VMs using vlan networks (configured on Advanced > Networks ). Optional: Configure advanced options like hostname and cloud-init data in the Advanced Options section. Cloud config examples \u00b6 Config for the password of the default user: #cloud-config password : password chpasswd : { expire : False } ssh_pwauth : True Network-data configuration using DHCP: version : 1 config : - type : physical name : eth0 subnets : - type : dhcp - type : physical name : eth1 subnets : - type : dhcp You can also use the Cloud Config Template feature to include a pre-defined cloud-init config for the VM. Networks \u00b6 Management Network \u00b6 A management network represents the default vm eth0 interface configured by the cluster network solution that is present in each VM. By default, a VM can be accessed via the management network. Secondary Network \u00b6 It is also possible to connect VMs using additional networks with Harvester's built-in vlan networks .","title":"How to Create a VM"},{"location":"vm/create-vm/#how-to-create-a-vm","text":"Create one or more virtual machines from the Virtual Machines page. Choose the option to create either one or multiple VM instances. The VM name is required. (Optional) you can select to use the VM template. By default we have added ISO, raw, and Windows image templates. Configure the CPU and Memory of the VM. Select a custom VM image. Select SSH keys or upload a new one. To add more disks to the VM, go to the Volumes tab. The default disk will be the root disk. To configure networks, go to the Networks tab. The Management Network is added by default. It is also possible to add secondary networks to the VMs using vlan networks (configured on Advanced > Networks ). Optional: Configure advanced options like hostname and cloud-init data in the Advanced Options section.","title":"How to Create a VM"},{"location":"vm/create-vm/#cloud-config-examples","text":"Config for the password of the default user: #cloud-config password : password chpasswd : { expire : False } ssh_pwauth : True Network-data configuration using DHCP: version : 1 config : - type : physical name : eth0 subnets : - type : dhcp - type : physical name : eth1 subnets : - type : dhcp You can also use the Cloud Config Template feature to include a pre-defined cloud-init config for the VM.","title":"Cloud config examples"},{"location":"vm/create-vm/#networks","text":"","title":"Networks"},{"location":"vm/create-vm/#management-network","text":"A management network represents the default vm eth0 interface configured by the cluster network solution that is present in each VM. By default, a VM can be accessed via the management network.","title":"Management Network"},{"location":"vm/create-vm/#secondary-network","text":"It is also possible to connect VMs using additional networks with Harvester's built-in vlan networks .","title":"Secondary Network"},{"location":"vm/live-migration/","text":"Live Migration \u00b6 Live migration means moving a virtual machine to a different host without downtime. Note Live migration is not allowed when the virtual machine is using a management network of bridge interface type. To support live migration, 3 or more hosts in the Harvester cluster are required due to a known issue . Starting a migration \u00b6 Go to the Virtual Machines page. Find the virtual machine that you want to migrate and select Vertical \u22ee (... ) > Migrate . Choose the node that you want to migrate the virtual machine to. Click Apply . Aborting a migration \u00b6 Go to the Virtual Machines page. Find the virtual machine that is in migrating status and you want to abort the migration. Select Vertical \u22ee (... ) > Abort Migration . Migration timeouts \u00b6 Completion timeout \u00b6 The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages/disk blocks at a higher rate than these can be copied, which will prevent the migration process from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout which is 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds. Progress timeout \u00b6 Live migration will also be aborted when it is noticed that copying memory doesn't make any progress in 150s.","title":"Live Migration"},{"location":"vm/live-migration/#live-migration","text":"Live migration means moving a virtual machine to a different host without downtime. Note Live migration is not allowed when the virtual machine is using a management network of bridge interface type. To support live migration, 3 or more hosts in the Harvester cluster are required due to a known issue .","title":"Live Migration"},{"location":"vm/live-migration/#starting-a-migration","text":"Go to the Virtual Machines page. Find the virtual machine that you want to migrate and select Vertical \u22ee (... ) > Migrate . Choose the node that you want to migrate the virtual machine to. Click Apply .","title":"Starting a migration"},{"location":"vm/live-migration/#aborting-a-migration","text":"Go to the Virtual Machines page. Find the virtual machine that is in migrating status and you want to abort the migration. Select Vertical \u22ee (... ) > Abort Migration .","title":"Aborting a migration"},{"location":"vm/live-migration/#migration-timeouts","text":"","title":"Migration timeouts"},{"location":"vm/live-migration/#completion-timeout","text":"The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages/disk blocks at a higher rate than these can be copied, which will prevent the migration process from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout which is 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds.","title":"Completion timeout"},{"location":"vm/live-migration/#progress-timeout","text":"Live migration will also be aborted when it is noticed that copying memory doesn't make any progress in 150s.","title":"Progress timeout"}]}