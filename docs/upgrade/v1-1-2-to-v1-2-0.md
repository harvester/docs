---
sidebar_position: 2
sidebar_label: Upgrade from v1.1.2 to v1.2.0
title: "Upgrade from v1.1.2 to v1.2.0"
---

<head>
  <link rel="canonical" href="https://docs.harvesterhci.io/v1.2/upgrade/v1-1-2-to-v1-2-0"/>
</head>


## General information

:::tip

Before you start an upgrade, you can run the pre-check script to make sure the cluster is in a stable state. For more details, please visit this [URL](https://github.com/harvester/upgrade-helpers/tree/main/pre-check/v1.1.x) for the script.
:::

Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to [start an upgrade](./automatic.md#start-an-upgrade).

For the air-gap env upgrade, please refer to [prepare an air-gapped upgrade](./automatic.md#prepare-an-air-gapped-upgrade).


## Known issues

---

### 1. An upgrade can't start and reports `"validator.harvesterhci.io" denied the request: managed chart rancher-monitoring is not ready, please wait for it to be ready`

If a cluster is configured with a **storage network**, an upgrade can't start with the following message.

![](/img/v1.2/upgrade/known_issues/3839-error.png)

- Related issue:
  - [[Doc] upgrade stuck while upgrading system service with alertmanager and prometheus](https://github.com/harvester/harvester/issues/3839)
- Workaround:
  - https://github.com/harvester/harvester/issues/3839#issuecomment-1534438192

---

### 2. An upgrade is stuck in `Creating Upgrade Repository`

During an upgrade, **Creating Upgrade Repository** is stuck in the **Pending** state:

  ![](/img/v1.2/upgrade/known_issues/4246-pending.png)

Please perform the following steps to check if the cluster runs into the issue:

1. Check the upgrade repository pod:

    ![](/img/v1.2/upgrade/known_issues/4246-upgrade-repo-pod.png)

    If the `virt-launcher-upgrade-repo-hvst-<upgrade-name>` pod stays in `ContainerCreating`, your cluster might have run into this issue. In this case, proceed with step 2.

2. Check the upgrade repository volume in the Longhorn GUI.

     1. [Go to Longhorn GUI](../troubleshooting/harvester.md#access-embedded-rancher-and-longhorn-dashboards).
     2. Navigate to the **Volume** page.
     3. Check the upgrade repository VM volume. It should be attached to a pod called `virt-launcher-upgrade-repo-hvst-<upgrade-name>`. If one of the volume's replicas stays in `Stopped` (gray color), the cluster is running into the issue.

      ![](/img/v1.2/upgrade/known_issues/4246-pending-replica.png)

- Related issue:
  - [[BUG] upgrade stuck on create upgrade VM](https://github.com/harvester/harvester/issues/4246)
- Workaround:
  - Delete the `Stopped` replica from Longhorn GUI. Or,
  - [Start over the upgrade](./troubleshooting.md#start-over-an-upgrade).

---

### 3. An upgrade is stuck when pre-draining a node

Starting from v1.1.0, Harvester will wait for all volumes to become healthy (when node count >= 3) before upgrading a node. Generally, you can check volumes' health if an upgrade is stuck in the "pre-draining" state.

Visit ["Access Embedded Longhorn"](../troubleshooting/harvester.md#access-embedded-rancher-and-longhorn-dashboards) to see how to access the embedded Longhorn GUI.

You can also check the pre-drain job logs. Please refer to [Phase 4: Upgrade nodes](./troubleshooting.md#phase-4-upgrade-nodes) in the troubleshooting guide.

---

### 4. An upgrade is stuck in upgrading the first node: Job was active longer than the specified deadline

An upgrade fails, as shown in the screenshot below:

![](/img/v1.2/upgrade/known_issues/2894-deadline.png)


- Related issue:
  - [[BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline](https://github.com/harvester/harvester/issues/2894)
- Workaround:
  - https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690


---

### 5. An upgrade is stuck in the Pre-drained state

You might see an upgrade is stuck in the "pre-drained" state:

![](/img/v1.2/upgrade/known_issues/3730-stuck.png)

This could be caused by a misconfigured PDB. To check if that's the case, perform the following steps:

1. Assume the stuck node is `harvester-node-1`.
1. Check the `instance-manager-e` or `instance-manager-r` pod names on the stuck node:

    ```
    $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager
    instance-manager-r-d4ed2788          1/1     Running   0              3d8h
    ```

    The output above shows that the `instance-manager-r-d4ed2788` pod is on the node. 

1. Check Rancher logs and verify that the `instance-manager-e` or `instance-manager-r` pod can't be drained:

    ```
    $ kubectl logs deployment/rancher -n cattle-system
    ...
    2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def
    2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788
    2023-03-28T17:10:55.080933607Z error when evicting pods/"instance-manager-r-d4ed2788" -n "longhorn-system" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
    ```

1. Run the command to check if there is a PDB associated with the stuck node:

    ```
    $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels."longhorn.io/node"=="harvester-node-1") | .metadata.name'
    instance-manager-r-466e3c7f
    ```

1. Check the owner of the instance manager to this PDB:

    ```
    $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID'
    harvester-node-2
    ```

    If the output doesn't match the stuck node (in this example output, `harvester-node-2` doesn't match the stuck node `harvester-node-1`), then we can conclude this issue happens.

1. Before applying the workaround, check if all volumes are healthy:

    ```
    kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == "attached")| .status.robustness'
    ```

    The output should all be `healthy`. If this is not the case, you might want to uncordon nodes to make the volume healthy again.

1.  Remove the misconfigured PDB:

    ```
    kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system
    ```

- Related issue:
  - [[BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0->v1.1.2-rc4](https://github.com/harvester/harvester/issues/3730 )

---

### 6. An upgrade is stuck in the Upgrading System Service state

If you notice the upgrade is stuck in the **Upgrading System Service** state for a long period of time, you might need to investigate if the upgrade is stuck in the `apply-manifests` phase.

![](/img/v1.2/upgrade/known_issues/4484-apply-manifests-stuck.png)

1. Check the log of the `apply-manifests` pod to see if the following messages repeat.

    ```
    $ kubectl -n harvester-system logs hvst-upgrade-md6wr-apply-manifests-wqslg --tail=10
    Tue Sep  5 10:20:39 UTC 2023
    there are still 1 pods in cattle-monitoring-system to be deleted
    Tue Sep  5 10:20:45 UTC 2023
    there are still 1 pods in cattle-monitoring-system to be deleted
    Tue Sep  5 10:20:50 UTC 2023
    there are still 1 pods in cattle-monitoring-system to be deleted
    Tue Sep  5 10:20:55 UTC 2023
    there are still 1 pods in cattle-monitoring-system to be deleted
    Tue Sep  5 10:21:00 UTC 2023
    there are still 1 pods in cattle-monitoring-system to be deleted
    ```

1. Check if the `prometheus-rancher-monitoring-prometheus-0` pod is stuck with the status `Terminating`.

    ```
    $ kubectl -n cattle-monitoring-system get pods                                  
    NAME                                         READY   STATUS        RESTARTS   AGE
    prometheus-rancher-monitoring-prometheus-0   0/3     Terminating   0          19d
    ```

1. Find the UID of the terminating pod with the following command:

    ```
    $ kubectl -n cattle-monitoring-system get pod prometheus-rancher-monitoring-prometheus-0 -o jsonpath='{.metadata.uid}'
    33f43165-6faa-4648-927d-69097901471c
    ```

1. Get access to any node of the cluster via the console or SSH.
1. Search for the related log messages in `/var/lib/rancher/rke2/agent/logs/kubelet.log` using the pod's UID.

    ```
    E0905 10:26:18.769199   17399 reconciler.go:208] "operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \"pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\" (UniqueName: \"kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\") pod \"33f43165-6faa-4648-927d-69097901471c\" (UID: \"33f43165-6faa-4648-927d-69097901471c\") : UnmountVolume.NewUnmounter failed for volume \"pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\" (UniqueName: \"kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\") pod \"33f43165-6faa-4648-927d-69097901471c\" (UID: \"33f43165-6faa-4648-927d-69097901471c\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json]: open /var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json: no such file or directory" err="UnmountVolume.NewUnmounter failed for volume \"pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\" (UniqueName: \"kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\") pod \"33f43165-6faa-4648-927d-69097901471c\" (UID: \"33f43165-6faa-4648-927d-69097901471c\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json]: open /var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json: no such file or directory"
    ```

     If kubelet continues to complain about the volume failing to unmount, apply the following workaround to allow the upgrade to proceed.

1. Forcibly remove the pod stuck with the status `Terminating` with the following command:

    ```
    kubectl delete pod prometheus-rancher-monitoring-prometheus-0 -n cattle-monitoring-system  --force
    ```

- Related issue
  - [[BUG] The rancher-monitoring Pod stuck at terminating status when upgrading from v1.1.2 to v1.2.0-rc6](https://github.com/harvester/harvester/issues/4484)

---
