{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Harvester Intro \u00b6 Harvester is an open source hyper-converged infrastructure (HCI) software built on Kubernetes. It is an open alternative to using a proprietary HCI stack that incorporates the design and ethos of Cloud Native . Overview \u00b6 Harvester implements HCI on bare metal servers. Harvester is designed to use local, direct attached storage instead of complex external SANs. It ships as an integrated bootable appliance image that can be deployed directly to servers through an ISO or PXE boot artifact. Here are some notable features of the Harvester: VM lifecycle management including SSH-Key injection, Cloud-init and, graphic and serial port console VM live migration support Supporting VM backup and restore Distributed block storage Multiple NICs in the VM connecting to the management network or VLANs Virtual Machine and cloud-init templates Rancher integration with multi-cluster management and the Harvester node driver PXE/iPXE boot support Virtual IP and bond NIC support Monitoring integration The following diagram gives a high-level architecture of Harvester: Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes. KubeVirt is a virtual machine management add-on for Kubernetes. Elemental for openSUSE Leap 15.3 is a Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster. Hardware Requirements \u00b6 To get the Harvester server up and running the following minimum hardware is required: Type Requirements CPU x86_64 only. Hardware-assisted virtualization is required. 8 cores minimum, 16 cores or above preferred Memory 32 GB minimum, 64 GB or above preferred Disk Capacity 120 GB minimum, 500 GB or above preferred Disk Performance 5,000+ random IOPS per disk(SSD/NVMe). Management nodes (first 3 nodes) must be fast enough for Etcd . Network Card 1 Gbps Ethernet minimum, 10Gbps Ethernet recommended Network Switch Trunking of ports required for VLAN support Quick start \u00b6 You can install Harvester via ISO installation or PXE Boot Installation. Instructions are provided in sections below. ISO Installation \u00b6 You can use the ISO to install Harvester directly on the bare-metal server to form a Harvester cluster. Users can add one or many compute nodes to join the existing cluster. To get the Harvester ISO, download it from the Github releases . During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device that the Harvester will be formatted to. Configure the hostname and select the network interface for the management network, by default harvester will create a bond NIC named harvester-mgmt , and the IP address can either be configured via DHCP or static method. (Optional) Configure the DNS servers, use commas as a delimiter. Config the Virtual IP which you can use for accessing the cluster or joining the cluster by the other nodes. Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default ssh user is rancher . (Optional) Config the NTP Servers of the node if needed, default to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here, otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with cloud-init config, enter the HTTP URL here. Confirm the installation options and the Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete it will restart the host and a console UI with management URL and status will be displayed. (You can Use F12 to switch between Harvester console and the Shell) The default URL of the web interface is https://your-virtual-ip . User will be prompted to set the password for the default admin user on the first-time login. Other Installation Methods \u00b6 Harvester can be installed automatically, please refer to PXE Boot Install for detailed instructions. More usage examples is available at harvester/ipxe-examples .","title":"Harvester Intro"},{"location":"#harvester-intro","text":"Harvester is an open source hyper-converged infrastructure (HCI) software built on Kubernetes. It is an open alternative to using a proprietary HCI stack that incorporates the design and ethos of Cloud Native .","title":"Harvester Intro"},{"location":"#overview","text":"Harvester implements HCI on bare metal servers. Harvester is designed to use local, direct attached storage instead of complex external SANs. It ships as an integrated bootable appliance image that can be deployed directly to servers through an ISO or PXE boot artifact. Here are some notable features of the Harvester: VM lifecycle management including SSH-Key injection, Cloud-init and, graphic and serial port console VM live migration support Supporting VM backup and restore Distributed block storage Multiple NICs in the VM connecting to the management network or VLANs Virtual Machine and cloud-init templates Rancher integration with multi-cluster management and the Harvester node driver PXE/iPXE boot support Virtual IP and bond NIC support Monitoring integration The following diagram gives a high-level architecture of Harvester: Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes. KubeVirt is a virtual machine management add-on for Kubernetes. Elemental for openSUSE Leap 15.3 is a Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster.","title":"Overview"},{"location":"#hardware-requirements","text":"To get the Harvester server up and running the following minimum hardware is required: Type Requirements CPU x86_64 only. Hardware-assisted virtualization is required. 8 cores minimum, 16 cores or above preferred Memory 32 GB minimum, 64 GB or above preferred Disk Capacity 120 GB minimum, 500 GB or above preferred Disk Performance 5,000+ random IOPS per disk(SSD/NVMe). Management nodes (first 3 nodes) must be fast enough for Etcd . Network Card 1 Gbps Ethernet minimum, 10Gbps Ethernet recommended Network Switch Trunking of ports required for VLAN support","title":"Hardware Requirements"},{"location":"#quick-start","text":"You can install Harvester via ISO installation or PXE Boot Installation. Instructions are provided in sections below.","title":"Quick start"},{"location":"#iso-installation","text":"You can use the ISO to install Harvester directly on the bare-metal server to form a Harvester cluster. Users can add one or many compute nodes to join the existing cluster. To get the Harvester ISO, download it from the Github releases . During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device that the Harvester will be formatted to. Configure the hostname and select the network interface for the management network, by default harvester will create a bond NIC named harvester-mgmt , and the IP address can either be configured via DHCP or static method. (Optional) Configure the DNS servers, use commas as a delimiter. Config the Virtual IP which you can use for accessing the cluster or joining the cluster by the other nodes. Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default ssh user is rancher . (Optional) Config the NTP Servers of the node if needed, default to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here, otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with cloud-init config, enter the HTTP URL here. Confirm the installation options and the Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete it will restart the host and a console UI with management URL and status will be displayed. (You can Use F12 to switch between Harvester console and the Shell) The default URL of the web interface is https://your-virtual-ip . User will be prompted to set the password for the default admin user on the first-time login.","title":"ISO Installation"},{"location":"#other-installation-methods","text":"Harvester can be installed automatically, please refer to PXE Boot Install for detailed instructions. More usage examples is available at harvester/ipxe-examples .","title":"Other Installation Methods"},{"location":"authentication/","text":"Authentication \u00b6 After installation, user will be prompted to set the password for the default admin user on the first-time login. Note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","title":"Authentication"},{"location":"authentication/#authentication","text":"After installation, user will be prompted to set the password for the default admin user on the first-time login. Note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","title":"Authentication"},{"location":"import-image/","text":"Import Images \u00b6 Currently supports the following three ways to create an image Upload Images via URL \u00b6 To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. Note The image name will be auto-filled using the URL address's filename. You can always customize it when required. Upload image via Local File \u00b6 Note Please do not refresh the page until the file upload is finished Currently, we support qcow2, raw, and ISO images. Create Image via Volume \u00b6 You can go to the volumes page, click Export Image . Enter image name to create image.","title":"Import Images"},{"location":"import-image/#import-images","text":"Currently supports the following three ways to create an image","title":"Import Images"},{"location":"import-image/#upload-images-via-url","text":"To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. Note The image name will be auto-filled using the URL address's filename. You can always customize it when required.","title":"Upload Images via URL"},{"location":"import-image/#upload-image-via-local-file","text":"Note Please do not refresh the page until the file upload is finished Currently, we support qcow2, raw, and ISO images.","title":"Upload image via Local File"},{"location":"import-image/#create-image-via-volume","text":"You can go to the volumes page, click Export Image . Enter image name to create image.","title":"Create Image via Volume"},{"location":"upgrade/","text":"Upgrading Harvester \u00b6 Note Upgrade is not supported from previous version to v0.3.0 version. We're investigating a manual upgrade process starting with v0.3.0. Will keep the community posted. One click upgrade will be supported starting with v1.0.0 release.","title":"Upgrading Harvester"},{"location":"upgrade/#upgrading-harvester","text":"Note Upgrade is not supported from previous version to v0.3.0 version. We're investigating a manual upgrade process starting with v0.3.0. Will keep the community posted. One click upgrade will be supported starting with v1.0.0 release.","title":"Upgrading Harvester"},{"location":"dev/dev-mode/","text":"Developer Mode Installation \u00b6 Developer mode (dev mode) is intended to be used for testing and development purposes. This video shows the dev mode installation. Requirements \u00b6 We assume Multus is installed across your cluster and a corresponding NetworkAttachmentDefinition CRD was created. If you are using an RKE cluster, please ensure the ipv4.ip_forward is enabled for the CNI plugin so that the pod network works as expected, related issue: #94 . Install as an App \u00b6 Harvester can be installed on a Kubernetes cluster in the following ways: Install with the Helm CLI Install as a Rancher catalog app, in which case the harvester/harvester repo is added to the Rancher Catalog as a Helm v3 app Please refer to the Harvester Helm chart for more details on installing and configuring the Helm chart. Requirements \u00b6 The Kubernetes node must have hardware virtualization support. To validate the support, use this command: cat /proc/cpuinfo | grep vmx Option 1: Install using Helm \u00b6 Clone the GitHub repository: git clone https://github.com/harvester/harvester.git --depth = 1 Go to the Helm chart: cd harvester/deploy/charts Install the Harvester chart with the following commands: ### To install the chart with the release name `harvester`: ## Create the target namespace kubectl create ns harvester-system ## Install the chart to the target namespace helm install harvester harvester \\ --namespace harvester-system \\ --set longhorn.enabled = true,minio.persistence.storageClass = longhorn Option 2: Install using Rancher \u00b6 Tip You can create a testing Kubernetes environment in Rancher using the Digital Ocean cloud provider. For details, see this section . Add the Harvester repo https://github.com/harvester/harvester to your Rancher catalogs by clicking Global > Tools > Catalogs . Specify the URL and name. Set the branch to stable if you need a stable release version. Set the Helm version to be Helm v3 . Click Create . Navigate to your project-level Apps. Click Launch and choose the Harvester app. (Optional) You can modify the configurations if needed. Otherwise, use the default options. Click Launch and wait for the app's components to be ready. Click the /index.html link to navigate to the Harvester UI, as shown in the figure below: Digital Ocean Test Environment \u00b6 Digital Ocean supports nested virtualization by default. You can create a testing Kubernetes environment in Rancher using the Digital Ocean cloud provider. We recommend using the 8 core, 16 GB RAM node, which will have nested virtualization enabled by default. This screenshot shows how to create a Rancher node template that would allow Rancher to provision such a node in Digital Ocean: For more information on how to launch Digital Ocean nodes with Rancher, refer to the Rancher documentation.","title":"Developer Mode Installation"},{"location":"dev/dev-mode/#developer-mode-installation","text":"Developer mode (dev mode) is intended to be used for testing and development purposes. This video shows the dev mode installation.","title":"Developer Mode Installation"},{"location":"dev/dev-mode/#requirements","text":"We assume Multus is installed across your cluster and a corresponding NetworkAttachmentDefinition CRD was created. If you are using an RKE cluster, please ensure the ipv4.ip_forward is enabled for the CNI plugin so that the pod network works as expected, related issue: #94 .","title":"Requirements"},{"location":"dev/dev-mode/#install-as-an-app","text":"Harvester can be installed on a Kubernetes cluster in the following ways: Install with the Helm CLI Install as a Rancher catalog app, in which case the harvester/harvester repo is added to the Rancher Catalog as a Helm v3 app Please refer to the Harvester Helm chart for more details on installing and configuring the Helm chart.","title":"Install as an App"},{"location":"dev/dev-mode/#requirements_1","text":"The Kubernetes node must have hardware virtualization support. To validate the support, use this command: cat /proc/cpuinfo | grep vmx","title":"Requirements"},{"location":"dev/dev-mode/#option-1-install-using-helm","text":"Clone the GitHub repository: git clone https://github.com/harvester/harvester.git --depth = 1 Go to the Helm chart: cd harvester/deploy/charts Install the Harvester chart with the following commands: ### To install the chart with the release name `harvester`: ## Create the target namespace kubectl create ns harvester-system ## Install the chart to the target namespace helm install harvester harvester \\ --namespace harvester-system \\ --set longhorn.enabled = true,minio.persistence.storageClass = longhorn","title":"Option 1: Install using Helm"},{"location":"dev/dev-mode/#option-2-install-using-rancher","text":"Tip You can create a testing Kubernetes environment in Rancher using the Digital Ocean cloud provider. For details, see this section . Add the Harvester repo https://github.com/harvester/harvester to your Rancher catalogs by clicking Global > Tools > Catalogs . Specify the URL and name. Set the branch to stable if you need a stable release version. Set the Helm version to be Helm v3 . Click Create . Navigate to your project-level Apps. Click Launch and choose the Harvester app. (Optional) You can modify the configurations if needed. Otherwise, use the default options. Click Launch and wait for the app's components to be ready. Click the /index.html link to navigate to the Harvester UI, as shown in the figure below:","title":"Option 2: Install using Rancher"},{"location":"dev/dev-mode/#digital-ocean-test-environment","text":"Digital Ocean supports nested virtualization by default. You can create a testing Kubernetes environment in Rancher using the Digital Ocean cloud provider. We recommend using the 8 core, 16 GB RAM node, which will have nested virtualization enabled by default. This screenshot shows how to create a Rancher node template that would allow Rancher to provision such a node in Digital Ocean: For more information on how to launch Digital Ocean nodes with Rancher, refer to the Rancher documentation.","title":"Digital Ocean Test Environment"},{"location":"install/harvester-configuration/","text":"Harvester Configuration \u00b6 Configuration Example \u00b6 Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: server_url : https://someserver:8443 token : TOKEN_VALUE os : ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files : - encoding : \"\" content : test content owner : root path : /etc/test.txt permissions : '0755' hostname : myhost modules : - kvm - nvme sysctl : kernel.printk : \"4 4 1 7\" kernel.kptr_restrict : \"1\" dns_nameservers : - 8.8.8.8 - 1.1.1.1 ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org password : rancher environment : http_proxy : http://myserver https_proxy : http://myserver install : mode : create networks : harvester-mgmt : interfaces : - name : ens5 default_route : true method : dhcp force_efi : true device : /dev/vda silent : true iso_url : http://myserver/test.iso poweroff : true no_format : true debug : true tty : ttyS0 vip : 10.10.0.19 vip_hw_addr : 52:54:00:ec:0e:0b vip_mode : dhcp Configuration Reference \u00b6 Below is a reference of all configuration keys. Warning Security Risks : The configuration file contains credentials which should be kept secretly. Please do not make the configuration file publicly accessible at the moment. server_url \u00b6 Definition \u00b6 The URL of the Harvester server to join as an agent. This configuration is mandatory when the installation is in JOIN mode. It tells the Harvester installer where the main server is. Example \u00b6 server_url : https://someserver:8443 install : mode : join token \u00b6 Definition \u00b6 The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match from what server has. Example \u00b6 token : myclustersecret Or a node token token : \"K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4\" os.ssh_authorized_keys \u00b6 Definition \u00b6 A list of SSH authorized keys that should be added to the default user rancher . SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME} . This is done by downloading the keys from https://github.com/${USERNAME}.keys . Example \u00b6 os : ssh_authorized_keys : - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D\" - \"github:ibuildthecloud\" os.write_files \u00b6 A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: \"\" : content data are written in plain text. In this case, the encoding field can be also omitted. b64 , base64 : content data are base64-encoded. gz , gzip : content data are gzip-compressed. gz+base64 , gzip+base64 , gz+b64 , gzip+b64 : content data are gzip-compressed first and then base64-encoded. Example os : write_files : - encoding : b64 content : CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner : root:root path : /etc/connman/main.conf permissions : '0644' - content : | # My new /etc/sysconfig/samba file SMDBOPTIONS=\"-D\" path : /etc/sysconfig/samba - content : !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path : /bin/arch permissions : '0555' - content : | 15 * * * * root ship_logs path : /etc/crontab os.hostname \u00b6 Definition \u00b6 Set the system hostname. This value will be overwritten by DHCP if DHCP supplies a hostname for the system. If DHCP doesn't offer a hostname and this value is empty, a random hostname will be generated. Example \u00b6 os : hostname : myhostname os.modules \u00b6 Definition \u00b6 A list of kernel modules to be loaded on start. Example \u00b6 os : modules : - kvm - nvme os.sysctls \u00b6 Definition \u00b6 Kernel sysctl to setup on start. These are the same configuration you'd typically find in /etc/sysctl.conf . Must be specified as string values. Example \u00b6 os : sysctl : kernel.printk : 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict : \"1\" # force the YAML parser to read as a string os.dns_nameservers \u00b6 Definition \u00b6 Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS. Example \u00b6 os : dns_nameservers : - 8.8.8.8 - 1.1.1.1 os.ntp_servers \u00b6 Definition \u00b6 Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Example \u00b6 os : ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org os.password \u00b6 Definition \u00b6 The password for the default user rancher . By default there is no password for the rancher user. If you set a password at runtime it will be reset on next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to just change your password on a Linux system and copy the value of the second field from /etc/shadow . You can also encrypt a password using openssl passwd -1 . Example \u00b6 os : password : \"$1$tYtghCfK$QHa51MS6MVAcfUKuOzNKt0\" Or clear text os : password : supersecure os.environment \u00b6 Definition \u00b6 Environment variables to be set on k3s and other processes like the boot process. Primary use of this field is to set the http proxy. Example \u00b6 os : environment : http_proxy : http://myserver https_proxy : http://myserver install.mode \u00b6 Definition \u00b6 Harvester installer mode: create : Creating a new Harvester installer join : Join an existing Harvester installer. Need to specify server_url . Example \u00b6 install : mode : create install.networks \u00b6 Definition \u00b6 Configure network interfaces for the host machine. Each key-value pair represents as a network interface. The key name becomes the network name, and the values are configurations for each network. Valid configuration fields are: method : Method to assign IP for this network. Support static and dhcp . ip : Static IP for this network. Required if static method is chosen. subnet_mask : Subnet mask for this network. Required if static method is chosen. gateway : Gateway for this network. Required if static method is chosen. interfaces : An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name : The name of slave interface for the bonded network. default_route : Set the network as the default route or not. bond_options : Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlb miimon: 100 Note A network harvester-mgmt is mandatory to establish a valid management network . Note Harvester uses systemd net naming scheme . Please make sure the interface name presents on target machine before installation. Example \u00b6 install : mode : create networks : harvester-mgmt : # The management bond name. This is mandatory. interfaces : - name : ens5 default_route : true method : dhcp bond_options : mode : balance-tlb miimon : 100 bond0 : interfaces : - name : ens8 method : static ip : 10.10.18.2 subnet_mask : 255.255.255.0 gateway : 192.168.11.1 install.force_efi \u00b6 Force EFI installation even when EFI is not detected. Default: false . install.device \u00b6 The device to install the OS. install.silent \u00b6 Reserved. install.iso_url \u00b6 ISO to download and install from if booting from kernel/vmlinuz and not ISO. install.poweroff \u00b6 Shutdown the machine after install instead of rebooting install.no_format \u00b6 Do not partition and format, assume layout exists already. install.debug \u00b6 Run installation with more logging and configure debug for installed system. install.tty \u00b6 Definition \u00b6 The tty device used for console. Example \u00b6 install : tty : ttyS0,115200n8 install.vip \u00b6 install.vip_mode \u00b6 install.vip_hw_addr \u00b6 Definition \u00b6 install.vip : The VIP of Harvester management endpoint. After installation, users can access Harvester GUI at URL https://<VIP> . install.vip_mode dhcp : Harvester will send DHCP requests to get VIP. install.vip_hw_addr field needs to be provided. static : Harvester uses a static VIP. install.vip_hw_addr : The hardware address corresponding to the VIP. Users have to configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp . Example \u00b6 Configure a static VIP. install : vip : 192.168.0.100 vip_mode : static Configure a DHCP VIP. install : vip : 10.10.0.19 vip_mode : dhcp vip_hw_addr : 52:54:00:ec:0e:0b","title":"Harvester Configuration"},{"location":"install/harvester-configuration/#harvester-configuration","text":"","title":"Harvester Configuration"},{"location":"install/harvester-configuration/#configuration-example","text":"Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: server_url : https://someserver:8443 token : TOKEN_VALUE os : ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files : - encoding : \"\" content : test content owner : root path : /etc/test.txt permissions : '0755' hostname : myhost modules : - kvm - nvme sysctl : kernel.printk : \"4 4 1 7\" kernel.kptr_restrict : \"1\" dns_nameservers : - 8.8.8.8 - 1.1.1.1 ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org password : rancher environment : http_proxy : http://myserver https_proxy : http://myserver install : mode : create networks : harvester-mgmt : interfaces : - name : ens5 default_route : true method : dhcp force_efi : true device : /dev/vda silent : true iso_url : http://myserver/test.iso poweroff : true no_format : true debug : true tty : ttyS0 vip : 10.10.0.19 vip_hw_addr : 52:54:00:ec:0e:0b vip_mode : dhcp","title":"Configuration Example"},{"location":"install/harvester-configuration/#configuration-reference","text":"Below is a reference of all configuration keys. Warning Security Risks : The configuration file contains credentials which should be kept secretly. Please do not make the configuration file publicly accessible at the moment.","title":"Configuration Reference"},{"location":"install/harvester-configuration/#server_url","text":"","title":"server_url"},{"location":"install/harvester-configuration/#definition","text":"The URL of the Harvester server to join as an agent. This configuration is mandatory when the installation is in JOIN mode. It tells the Harvester installer where the main server is.","title":"Definition"},{"location":"install/harvester-configuration/#example","text":"server_url : https://someserver:8443 install : mode : join","title":"Example"},{"location":"install/harvester-configuration/#token","text":"","title":"token"},{"location":"install/harvester-configuration/#definition_1","text":"The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match from what server has.","title":"Definition"},{"location":"install/harvester-configuration/#example_1","text":"token : myclustersecret Or a node token token : \"K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4\"","title":"Example"},{"location":"install/harvester-configuration/#osssh_authorized_keys","text":"","title":"os.ssh_authorized_keys"},{"location":"install/harvester-configuration/#definition_2","text":"A list of SSH authorized keys that should be added to the default user rancher . SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME} . This is done by downloading the keys from https://github.com/${USERNAME}.keys .","title":"Definition"},{"location":"install/harvester-configuration/#example_2","text":"os : ssh_authorized_keys : - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D\" - \"github:ibuildthecloud\"","title":"Example"},{"location":"install/harvester-configuration/#oswrite_files","text":"A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: \"\" : content data are written in plain text. In this case, the encoding field can be also omitted. b64 , base64 : content data are base64-encoded. gz , gzip : content data are gzip-compressed. gz+base64 , gzip+base64 , gz+b64 , gzip+b64 : content data are gzip-compressed first and then base64-encoded. Example os : write_files : - encoding : b64 content : CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner : root:root path : /etc/connman/main.conf permissions : '0644' - content : | # My new /etc/sysconfig/samba file SMDBOPTIONS=\"-D\" path : /etc/sysconfig/samba - content : !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path : /bin/arch permissions : '0555' - content : | 15 * * * * root ship_logs path : /etc/crontab","title":"os.write_files"},{"location":"install/harvester-configuration/#oshostname","text":"","title":"os.hostname"},{"location":"install/harvester-configuration/#definition_3","text":"Set the system hostname. This value will be overwritten by DHCP if DHCP supplies a hostname for the system. If DHCP doesn't offer a hostname and this value is empty, a random hostname will be generated.","title":"Definition"},{"location":"install/harvester-configuration/#example_3","text":"os : hostname : myhostname","title":"Example"},{"location":"install/harvester-configuration/#osmodules","text":"","title":"os.modules"},{"location":"install/harvester-configuration/#definition_4","text":"A list of kernel modules to be loaded on start.","title":"Definition"},{"location":"install/harvester-configuration/#example_4","text":"os : modules : - kvm - nvme","title":"Example"},{"location":"install/harvester-configuration/#ossysctls","text":"","title":"os.sysctls"},{"location":"install/harvester-configuration/#definition_5","text":"Kernel sysctl to setup on start. These are the same configuration you'd typically find in /etc/sysctl.conf . Must be specified as string values.","title":"Definition"},{"location":"install/harvester-configuration/#example_5","text":"os : sysctl : kernel.printk : 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict : \"1\" # force the YAML parser to read as a string","title":"Example"},{"location":"install/harvester-configuration/#osdns_nameservers","text":"","title":"os.dns_nameservers"},{"location":"install/harvester-configuration/#definition_6","text":"Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS.","title":"Definition"},{"location":"install/harvester-configuration/#example_6","text":"os : dns_nameservers : - 8.8.8.8 - 1.1.1.1","title":"Example"},{"location":"install/harvester-configuration/#osntp_servers","text":"","title":"os.ntp_servers"},{"location":"install/harvester-configuration/#definition_7","text":"Fallback ntp servers to use if NTP is not configured elsewhere in the OS.","title":"Definition"},{"location":"install/harvester-configuration/#example_7","text":"os : ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org","title":"Example"},{"location":"install/harvester-configuration/#ospassword","text":"","title":"os.password"},{"location":"install/harvester-configuration/#definition_8","text":"The password for the default user rancher . By default there is no password for the rancher user. If you set a password at runtime it will be reset on next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to just change your password on a Linux system and copy the value of the second field from /etc/shadow . You can also encrypt a password using openssl passwd -1 .","title":"Definition"},{"location":"install/harvester-configuration/#example_8","text":"os : password : \"$1$tYtghCfK$QHa51MS6MVAcfUKuOzNKt0\" Or clear text os : password : supersecure","title":"Example"},{"location":"install/harvester-configuration/#osenvironment","text":"","title":"os.environment"},{"location":"install/harvester-configuration/#definition_9","text":"Environment variables to be set on k3s and other processes like the boot process. Primary use of this field is to set the http proxy.","title":"Definition"},{"location":"install/harvester-configuration/#example_9","text":"os : environment : http_proxy : http://myserver https_proxy : http://myserver","title":"Example"},{"location":"install/harvester-configuration/#installmode","text":"","title":"install.mode"},{"location":"install/harvester-configuration/#definition_10","text":"Harvester installer mode: create : Creating a new Harvester installer join : Join an existing Harvester installer. Need to specify server_url .","title":"Definition"},{"location":"install/harvester-configuration/#example_10","text":"install : mode : create","title":"Example"},{"location":"install/harvester-configuration/#installnetworks","text":"","title":"install.networks"},{"location":"install/harvester-configuration/#definition_11","text":"Configure network interfaces for the host machine. Each key-value pair represents as a network interface. The key name becomes the network name, and the values are configurations for each network. Valid configuration fields are: method : Method to assign IP for this network. Support static and dhcp . ip : Static IP for this network. Required if static method is chosen. subnet_mask : Subnet mask for this network. Required if static method is chosen. gateway : Gateway for this network. Required if static method is chosen. interfaces : An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name : The name of slave interface for the bonded network. default_route : Set the network as the default route or not. bond_options : Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlb miimon: 100 Note A network harvester-mgmt is mandatory to establish a valid management network . Note Harvester uses systemd net naming scheme . Please make sure the interface name presents on target machine before installation.","title":"Definition"},{"location":"install/harvester-configuration/#example_11","text":"install : mode : create networks : harvester-mgmt : # The management bond name. This is mandatory. interfaces : - name : ens5 default_route : true method : dhcp bond_options : mode : balance-tlb miimon : 100 bond0 : interfaces : - name : ens8 method : static ip : 10.10.18.2 subnet_mask : 255.255.255.0 gateway : 192.168.11.1","title":"Example"},{"location":"install/harvester-configuration/#installforce_efi","text":"Force EFI installation even when EFI is not detected. Default: false .","title":"install.force_efi"},{"location":"install/harvester-configuration/#installdevice","text":"The device to install the OS.","title":"install.device"},{"location":"install/harvester-configuration/#installsilent","text":"Reserved.","title":"install.silent"},{"location":"install/harvester-configuration/#installiso_url","text":"ISO to download and install from if booting from kernel/vmlinuz and not ISO.","title":"install.iso_url"},{"location":"install/harvester-configuration/#installpoweroff","text":"Shutdown the machine after install instead of rebooting","title":"install.poweroff"},{"location":"install/harvester-configuration/#installno_format","text":"Do not partition and format, assume layout exists already.","title":"install.no_format"},{"location":"install/harvester-configuration/#installdebug","text":"Run installation with more logging and configure debug for installed system.","title":"install.debug"},{"location":"install/harvester-configuration/#installtty","text":"","title":"install.tty"},{"location":"install/harvester-configuration/#definition_12","text":"The tty device used for console.","title":"Definition"},{"location":"install/harvester-configuration/#example_12","text":"install : tty : ttyS0,115200n8","title":"Example"},{"location":"install/harvester-configuration/#installvip","text":"","title":"install.vip"},{"location":"install/harvester-configuration/#installvip_mode","text":"","title":"install.vip_mode"},{"location":"install/harvester-configuration/#installvip_hw_addr","text":"","title":"install.vip_hw_addr"},{"location":"install/harvester-configuration/#definition_13","text":"install.vip : The VIP of Harvester management endpoint. After installation, users can access Harvester GUI at URL https://<VIP> . install.vip_mode dhcp : Harvester will send DHCP requests to get VIP. install.vip_hw_addr field needs to be provided. static : Harvester uses a static VIP. install.vip_hw_addr : The hardware address corresponding to the VIP. Users have to configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp .","title":"Definition"},{"location":"install/harvester-configuration/#example_13","text":"Configure a static VIP. install : vip : 192.168.0.100 vip_mode : static Configure a DHCP VIP. install : vip : 10.10.0.19 vip_mode : dhcp vip_hw_addr : 52:54:00:ec:0e:0b","title":"Example"},{"location":"install/iso-install/","text":"ISO Installation \u00b6 To get the Harvester ISO, download it from the Github releases. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device that the Harvester will be formatted to. Configure the hostname and select the network interface for the management network, by default harvester will create a bond NIC named harvester-mgmt , and the IP address can either be configured via DHCP or static method. (Optional) Configure the DNS servers, use commas as a delimiter. Config the Virtual IP which you can use for accessing the cluster or joining the cluster by the other nodes. Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default ssh user is rancher . (Optional) Config the NTP Servers of the node if needed, default to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here, otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with cloud-init config, enter the HTTP URL here. Confirm the installation options and the Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete it will restart the host and a console UI with management URL and status will be displayed. (You can Use F12 to switch between Harvester console and the Shell) The default URL of the web interface is https://your-virtual-ip . User will be prompted to set the password for the default admin user on the first-time login.","title":"ISO Installation"},{"location":"install/iso-install/#iso-installation","text":"To get the Harvester ISO, download it from the Github releases. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device that the Harvester will be formatted to. Configure the hostname and select the network interface for the management network, by default harvester will create a bond NIC named harvester-mgmt , and the IP address can either be configured via DHCP or static method. (Optional) Configure the DNS servers, use commas as a delimiter. Config the Virtual IP which you can use for accessing the cluster or joining the cluster by the other nodes. Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default ssh user is rancher . (Optional) Config the NTP Servers of the node if needed, default to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here, otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with cloud-init config, enter the HTTP URL here. Confirm the installation options and the Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete it will restart the host and a console UI with management URL and status will be displayed. (You can Use F12 to switch between Harvester console and the Shell) The default URL of the web interface is https://your-virtual-ip . User will be prompted to set the password for the default admin user on the first-time login.","title":"ISO Installation"},{"location":"install/pxe-boot-install/","text":"PXE Boot Installation \u00b6 Starting from version 0.2.0 , Harvester can be installed automatically. This document provides an example to do the automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If NIC cards don't come with iPXE firmware, iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples . Prerequisite \u00b6 Important Nodes need to have at least 8G of RAM because the installer loads the full ISO file into tmpfs. Preparing HTTP Servers \u00b6 An HTTP server is required to serve boot files. Let's assume an NGINX HTTP server's IP is 10.100.0.10 , and it serves /usr/share/nginx/html/ directory with the path http://10.100.0.10/ . Preparing Boot Files \u00b6 Download the required files from Harvester Releases Page . The ISO: harvester-<version>-amd64.iso The kernel: harvester-<version>-vmlinuz-amd64 The initrd: harvester-<version>-initrd-amd64 The rootfs squashfs image: harvester-<version>-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. e.g., sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/ Preparing iPXE boot scripts \u00b6 When performing automatic installation, there are two modes: CREATE : we are installing a node to construct an initial Harvester cluster. JOIN : we are installing a node to join an existing Harvester cluster. CREATE mode \u00b6 Warning Security Risks : The configuration file below contains credentials which should be kept secretly. Please do not make the configuration file publicly accessible at the moment. Create a Harvester configuration file config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml token : token os : hostname : node1 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin password : rancher install : mode : create networks : harvester-mgmt : # The management bond name. This is mandatory. interfaces : - name : ens5 default_route : true method : dhcp device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso vip : 10.100.0.100 # The VIP to access the Harvester GUI. Make sure the IP is free to use. vip_mode : static # Or dhcp, check configuration file for more information. For machines that needs to be installed as CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot Let's assume the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create NOTE If there are multiple network interfaces on the installing machine, the user can use ip=<interface>:dhcp to specify the booting interface (e.g., ip=eth1:dhcp ). JOIN mode \u00b6 Warning Security Risks : The configuration file below contains credentials which should be kept secretly. Please do not make the configuration file publicly accessible at the moment. Create a Harvester configuration file config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml server_url : https://10.100.0.130:8443 token : token os : hostname : node2 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin dns_nameservers : - 1.1.1.1 - 8.8.8.8 password : rancher install : mode : join networks : harvester-mgmt : # The management bond name. This is mandatory. interfaces : - name : ens5 default_route : true method : dhcp device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-<version>-initrd boot Let's assume the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join . DHCP server configuration \u00b6 Here is an example to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16 ; subnet 10 .100.0.0 netmask 255 .255.255.0 { option routers 10 .100.0.10 ; option domain-name-servers 192 .168.2.1 ; range 10 .100.0.100 10 .100.0.253 ; } group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } group { # join group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-join-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-join\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node2 { hardware ethernet 52 :54:00:69:d5:92 ; } } The config file declares a subnet and two groups. The first group is for hosts to boot with CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client, it also offers the iPXE image according to client architecture. Please prepare those images and a tftp server first. Harvester configuration \u00b6 For more information about Harvester configuration, please refer to the Harvester configuration . Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, the user can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file. UEFI HTTP Boot support \u00b6 UEFI firmware supports loading a boot image from HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform the automatic installation. Serve the iPXE program \u00b6 Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make ipxe.efi can be downloaded from the HTTP server. e.g.: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally. DHCP server configuration \u00b6 If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provides the iPXE program URL when it sees such a request. Here is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } elsif substring ( option vendor-class-identifier, 0 , 10 ) = \"HTTPClient\" { # UEFI HTTP Boot option vendor-class-identifier \"HTTPClient\" ; filename \"http://10.100.0.10/harvester/ipxe.efi\" ; } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from URL http://10.100.0.10/harvester/ipxe-create-efi . The iPXE script for UEFI boot \u00b6 It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. Here is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-<version>-vmlinuz initrd=harvester-<version>-initrd ip=dhcp net.ifnames=1 rd.cos.disable console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot The parameter initrd=harvester-<version>-initrd is required.","title":"PXE Boot Installation"},{"location":"install/pxe-boot-install/#pxe-boot-installation","text":"Starting from version 0.2.0 , Harvester can be installed automatically. This document provides an example to do the automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If NIC cards don't come with iPXE firmware, iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples .","title":"PXE Boot Installation"},{"location":"install/pxe-boot-install/#prerequisite","text":"Important Nodes need to have at least 8G of RAM because the installer loads the full ISO file into tmpfs.","title":"Prerequisite"},{"location":"install/pxe-boot-install/#preparing-http-servers","text":"An HTTP server is required to serve boot files. Let's assume an NGINX HTTP server's IP is 10.100.0.10 , and it serves /usr/share/nginx/html/ directory with the path http://10.100.0.10/ .","title":"Preparing HTTP Servers"},{"location":"install/pxe-boot-install/#preparing-boot-files","text":"Download the required files from Harvester Releases Page . The ISO: harvester-<version>-amd64.iso The kernel: harvester-<version>-vmlinuz-amd64 The initrd: harvester-<version>-initrd-amd64 The rootfs squashfs image: harvester-<version>-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. e.g., sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/","title":"Preparing Boot Files"},{"location":"install/pxe-boot-install/#preparing-ipxe-boot-scripts","text":"When performing automatic installation, there are two modes: CREATE : we are installing a node to construct an initial Harvester cluster. JOIN : we are installing a node to join an existing Harvester cluster.","title":"Preparing iPXE boot scripts"},{"location":"install/pxe-boot-install/#create-mode","text":"Warning Security Risks : The configuration file below contains credentials which should be kept secretly. Please do not make the configuration file publicly accessible at the moment. Create a Harvester configuration file config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml token : token os : hostname : node1 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin password : rancher install : mode : create networks : harvester-mgmt : # The management bond name. This is mandatory. interfaces : - name : ens5 default_route : true method : dhcp device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso vip : 10.100.0.100 # The VIP to access the Harvester GUI. Make sure the IP is free to use. vip_mode : static # Or dhcp, check configuration file for more information. For machines that needs to be installed as CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot Let's assume the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create NOTE If there are multiple network interfaces on the installing machine, the user can use ip=<interface>:dhcp to specify the booting interface (e.g., ip=eth1:dhcp ).","title":"CREATE mode"},{"location":"install/pxe-boot-install/#join-mode","text":"Warning Security Risks : The configuration file below contains credentials which should be kept secretly. Please do not make the configuration file publicly accessible at the moment. Create a Harvester configuration file config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml server_url : https://10.100.0.130:8443 token : token os : hostname : node2 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin dns_nameservers : - 1.1.1.1 - 8.8.8.8 password : rancher install : mode : join networks : harvester-mgmt : # The management bond name. This is mandatory. interfaces : - name : ens5 default_route : true method : dhcp device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-<version>-initrd boot Let's assume the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join .","title":"JOIN mode"},{"location":"install/pxe-boot-install/#dhcp-server-configuration","text":"Here is an example to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16 ; subnet 10 .100.0.0 netmask 255 .255.255.0 { option routers 10 .100.0.10 ; option domain-name-servers 192 .168.2.1 ; range 10 .100.0.100 10 .100.0.253 ; } group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } group { # join group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-join-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-join\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node2 { hardware ethernet 52 :54:00:69:d5:92 ; } } The config file declares a subnet and two groups. The first group is for hosts to boot with CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client, it also offers the iPXE image according to client architecture. Please prepare those images and a tftp server first.","title":"DHCP server configuration"},{"location":"install/pxe-boot-install/#harvester-configuration","text":"For more information about Harvester configuration, please refer to the Harvester configuration . Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, the user can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file.","title":"Harvester configuration"},{"location":"install/pxe-boot-install/#uefi-http-boot-support","text":"UEFI firmware supports loading a boot image from HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform the automatic installation.","title":"UEFI HTTP Boot support"},{"location":"install/pxe-boot-install/#serve-the-ipxe-program","text":"Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make ipxe.efi can be downloaded from the HTTP server. e.g.: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally.","title":"Serve the iPXE program"},{"location":"install/pxe-boot-install/#dhcp-server-configuration_1","text":"If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provides the iPXE program URL when it sees such a request. Here is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } elsif substring ( option vendor-class-identifier, 0 , 10 ) = \"HTTPClient\" { # UEFI HTTP Boot option vendor-class-identifier \"HTTPClient\" ; filename \"http://10.100.0.10/harvester/ipxe.efi\" ; } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from URL http://10.100.0.10/harvester/ipxe-create-efi .","title":"DHCP server configuration"},{"location":"install/pxe-boot-install/#the-ipxe-script-for-uefi-boot","text":"It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. Here is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-<version>-vmlinuz initrd=harvester-<version>-initrd ip=dhcp net.ifnames=1 rd.cos.disable console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot The parameter initrd=harvester-<version>-initrd is required.","title":"The iPXE script for UEFI boot"},{"location":"monitoring/monitoring/","text":"Monitoring \u00b6 Available as of v0.3.0 Dashboard Metrics \u00b6 Harvester v0.3.0 has provided a built-in monitoring integration using the Prometheus , the monitoring will be installed automatically from the ISO installation. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboard on the Grafana UI. Note only the admin users are able to view the dashboard metrics VM Detail Metrics \u00b6 For each VM, users can view the VM metrics by clicking the VM details page.","title":"Monitoring"},{"location":"monitoring/monitoring/#monitoring","text":"Available as of v0.3.0","title":"Monitoring"},{"location":"monitoring/monitoring/#dashboard-metrics","text":"Harvester v0.3.0 has provided a built-in monitoring integration using the Prometheus , the monitoring will be installed automatically from the ISO installation. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboard on the Grafana UI. Note only the admin users are able to view the dashboard metrics","title":"Dashboard Metrics"},{"location":"monitoring/monitoring/#vm-detail-metrics","text":"For each VM, users can view the VM metrics by clicking the VM details page.","title":"VM Detail Metrics"},{"location":"networking/harvester-network/","text":"Harvester Network \u00b6 Summary \u00b6 Harvester is built on Kubernetes, which uses CNI as an interface between network providers and Kubernetes pod networking. Naturally, we implement the Harvester network based on CNI. Moreover, the Harvester UI integrates the Harvester network to provide a user-friendly way to configure networks for VMs. management network VLAN Implementation \u00b6 Management Network \u00b6 Harvester adopts flannel as the default CNI to implement the management network. It's an internal network, which means the user can only access the VM's management network within its cluster nodes or pods. VLAN \u00b6 Harvester network-controller leverages the multus and bridge CNI plugins to implement the VLAN. Below is a use case of the VLAN in Harvester. Harvester network-controller uses a bridge for a node and a pair of veth for a VM to implement the VLAN. The bridge acts as a switch to forward the network traffic from or to VMs and the veth pair is like the connected ports between VMs and switch. VMs within the same VLAN can communicate with each other, while the VMs within different VLANs can't. The external switch ports connected with the hosts or other devices(such as DHCP server) should be set as trunk or hybrid type and permit the specified VLANs. Users can use VLAN with PVID (default 1) to communicate with any normal untagged traffic. Enabling VLAN in the Harvester UI \u00b6 Enable VLAN via going to Setting > vlan to enable VLAN and input a valid default physical NIC name for the VLAN. It is recommended to choose a separate NIC for the VLAN other than the one used for the management network (the one selected during the Harvester installation) for better network performance and isolation. Note Modifying the default VLAN network setting will not change the existing configured host networks. Note Harvester supports configuring bond interfaces but it can only be created during PEX Boot installation. Here is an example . (optional) Users can customize each node's VLAN network configuration via going to the HOST > Network tab. A new VLAN network is created by going to the Advanced > Networks page and clicking the Create button. The network is configured when the VM is created. Only the first network interface card will be enabled by default. Users can either choose to use a management network or VLAN network. Note You will need to select the Install guest agent option in the Advanced Options tab to get the VLAN network IP address from the Harvester UI. Users can choose to add one or multiple network interface cards. Additional network interface card configurations can be set via cloud-init network data, for example: version : 1 config : - type : physical name : enp1s0 # name is varies upon OS image subnets : - type : dhcp - type : physical name : enp2s0 subnets : - type : DHCP","title":"Harvester Network"},{"location":"networking/harvester-network/#harvester-network","text":"","title":"Harvester Network"},{"location":"networking/harvester-network/#summary","text":"Harvester is built on Kubernetes, which uses CNI as an interface between network providers and Kubernetes pod networking. Naturally, we implement the Harvester network based on CNI. Moreover, the Harvester UI integrates the Harvester network to provide a user-friendly way to configure networks for VMs. management network VLAN","title":"Summary"},{"location":"networking/harvester-network/#implementation","text":"","title":"Implementation"},{"location":"networking/harvester-network/#management-network","text":"Harvester adopts flannel as the default CNI to implement the management network. It's an internal network, which means the user can only access the VM's management network within its cluster nodes or pods.","title":"Management Network"},{"location":"networking/harvester-network/#vlan","text":"Harvester network-controller leverages the multus and bridge CNI plugins to implement the VLAN. Below is a use case of the VLAN in Harvester. Harvester network-controller uses a bridge for a node and a pair of veth for a VM to implement the VLAN. The bridge acts as a switch to forward the network traffic from or to VMs and the veth pair is like the connected ports between VMs and switch. VMs within the same VLAN can communicate with each other, while the VMs within different VLANs can't. The external switch ports connected with the hosts or other devices(such as DHCP server) should be set as trunk or hybrid type and permit the specified VLANs. Users can use VLAN with PVID (default 1) to communicate with any normal untagged traffic.","title":"VLAN"},{"location":"networking/harvester-network/#enabling-vlan-in-the-harvester-ui","text":"Enable VLAN via going to Setting > vlan to enable VLAN and input a valid default physical NIC name for the VLAN. It is recommended to choose a separate NIC for the VLAN other than the one used for the management network (the one selected during the Harvester installation) for better network performance and isolation. Note Modifying the default VLAN network setting will not change the existing configured host networks. Note Harvester supports configuring bond interfaces but it can only be created during PEX Boot installation. Here is an example . (optional) Users can customize each node's VLAN network configuration via going to the HOST > Network tab. A new VLAN network is created by going to the Advanced > Networks page and clicking the Create button. The network is configured when the VM is created. Only the first network interface card will be enabled by default. Users can either choose to use a management network or VLAN network. Note You will need to select the Install guest agent option in the Advanced Options tab to get the VLAN network IP address from the Harvester UI. Users can choose to add one or multiple network interface cards. Additional network interface card configurations can be set via cloud-init network data, for example: version : 1 config : - type : physical name : enp1s0 # name is varies upon OS image subnets : - type : dhcp - type : physical name : enp2s0 subnets : - type : DHCP","title":"Enabling VLAN in the Harvester UI"},{"location":"networking/vip/","text":"Management Address \u00b6 Harvester provides a fixed virtual IP(VIP) as the management address. Users can see the management address on the console dashboard after installation. Usages \u00b6 The management address has two usages. Allow users to access the Harvester UI via https protocol. Used by the other nodes to join the cluster. Configure VIP \u00b6 Users can specify the VIP during installation. It can either be configured via DHCP or static method. Note: In PXE boot, Harvester does not support setting the VIP via DHCP. It will be addressed in the next release. Issue: https://github.com/harvester/harvester/issues/1410","title":"Management Address"},{"location":"networking/vip/#management-address","text":"Harvester provides a fixed virtual IP(VIP) as the management address. Users can see the management address on the console dashboard after installation.","title":"Management Address"},{"location":"networking/vip/#usages","text":"The management address has two usages. Allow users to access the Harvester UI via https protocol. Used by the other nodes to join the cluster.","title":"Usages"},{"location":"networking/vip/#configure-vip","text":"Users can specify the VIP during installation. It can either be configured via DHCP or static method. Note: In PXE boot, Harvester does not support setting the VIP via DHCP. It will be addressed in the next release. Issue: https://github.com/harvester/harvester/issues/1410","title":"Configure VIP"},{"location":"rancher/cloud-provider/","text":"Harvester Cloud Provider \u00b6 Available as of v0.3.0 The Harvester cloud provider used by the guest cluster in Harvester provides a CSI driver and cloud controller manager(CCM) which implements a built-in load balancer. In this section, you will learn about how to deploy the harvester cloud provider in RKE and RKE2 how to configure the load balancer with the annotation of services Deploying \u00b6 Prerequisites \u00b6 The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines running as guest Kubernetes nodes are in the same namespace. Deploying int the RKE1 with Harvester node driver \u00b6 When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider. Select the External cloud provider. Generate add-on configuration and add it to the rke YAML file. # depend on kubectl to operate the Harvester curl -sfL https://raw.githubusercontent.com/harvester/cloud-provider-harvester/master/deploy/generate_addon.sh | sh -s <serviceAccount name> <namespace> Deploying in the RKE2 with Harvester node driver \u00b6 When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider, then the node driver will help deploy both the CSI driver and CCM automatically. Load Balancer Request Parameters \u00b6 When setting up a Kubernetes service of load balancer type, you can configure the load balancer with the service annotations. IPAM \u00b6 The Harvester built-in load balancer supports both pool and dhcp mode to specify the load balancer IP by the annotation key cloudprovider.harvesterhci.io/ipam . The value defaults to pool . pool: You should configure an IP address pool in the Harvester in advance. The Harvester LoadBalancer controller will allocate an IP address from the IP address poll for the load balancer. Refer to the guideline about how to configure an IP address pool. dhcp: It requires a DHCP server. The Harvester LoadBalancer controller will request an IP address from the DHCP server. Health Checks \u00b6 The Harvester load balancer supports TCP health checks. The details of the related annotations are as following. cloudprovider.harvesterhci.io/healthcheck-port specifies the port. The prober will access the address composed of the backend server IP and the port. This option is required. cloudprovider.harvesterhci.io/healthcheck-success-threshold specifies the health check success threshold. The default value is 1. If the number of times that the prober continuously successfully detects an address reaches the success threshold, the backend server can start to forward traffic. cloudprovider.harvesterhci.io/healthcheck-failure-threshold specify the success and failure threshold. The default value is 3. The backend server will stop forward traffic if the number of health check failures reaches the failure threshold. cloudprovider.harvesterhci.io/healthcheck-periodseconds specifies the health check period. The default value is 5 seconds. cloudprovider.harvesterhci.io/healthcheck-timeoutseconds specifies the timeout of every health check. The default value is 3 seconds.","title":"Harvester Cloud Provider"},{"location":"rancher/cloud-provider/#harvester-cloud-provider","text":"Available as of v0.3.0 The Harvester cloud provider used by the guest cluster in Harvester provides a CSI driver and cloud controller manager(CCM) which implements a built-in load balancer. In this section, you will learn about how to deploy the harvester cloud provider in RKE and RKE2 how to configure the load balancer with the annotation of services","title":"Harvester Cloud Provider"},{"location":"rancher/cloud-provider/#deploying","text":"","title":"Deploying"},{"location":"rancher/cloud-provider/#prerequisites","text":"The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines running as guest Kubernetes nodes are in the same namespace.","title":"Prerequisites"},{"location":"rancher/cloud-provider/#deploying-int-the-rke1-with-harvester-node-driver","text":"When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider. Select the External cloud provider. Generate add-on configuration and add it to the rke YAML file. # depend on kubectl to operate the Harvester curl -sfL https://raw.githubusercontent.com/harvester/cloud-provider-harvester/master/deploy/generate_addon.sh | sh -s <serviceAccount name> <namespace>","title":"Deploying int the RKE1 with Harvester node driver"},{"location":"rancher/cloud-provider/#deploying-in-the-rke2-with-harvester-node-driver","text":"When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider, then the node driver will help deploy both the CSI driver and CCM automatically.","title":"Deploying in the RKE2 with Harvester node driver"},{"location":"rancher/cloud-provider/#load-balancer-request-parameters","text":"When setting up a Kubernetes service of load balancer type, you can configure the load balancer with the service annotations.","title":"Load Balancer Request Parameters"},{"location":"rancher/cloud-provider/#ipam","text":"The Harvester built-in load balancer supports both pool and dhcp mode to specify the load balancer IP by the annotation key cloudprovider.harvesterhci.io/ipam . The value defaults to pool . pool: You should configure an IP address pool in the Harvester in advance. The Harvester LoadBalancer controller will allocate an IP address from the IP address poll for the load balancer. Refer to the guideline about how to configure an IP address pool. dhcp: It requires a DHCP server. The Harvester LoadBalancer controller will request an IP address from the DHCP server.","title":"IPAM"},{"location":"rancher/cloud-provider/#health-checks","text":"The Harvester load balancer supports TCP health checks. The details of the related annotations are as following. cloudprovider.harvesterhci.io/healthcheck-port specifies the port. The prober will access the address composed of the backend server IP and the port. This option is required. cloudprovider.harvesterhci.io/healthcheck-success-threshold specifies the health check success threshold. The default value is 1. If the number of times that the prober continuously successfully detects an address reaches the success threshold, the backend server can start to forward traffic. cloudprovider.harvesterhci.io/healthcheck-failure-threshold specify the success and failure threshold. The default value is 3. The backend server will stop forward traffic if the number of health check failures reaches the failure threshold. cloudprovider.harvesterhci.io/healthcheck-periodseconds specifies the health check period. The default value is 5 seconds. cloudprovider.harvesterhci.io/healthcheck-timeoutseconds specifies the timeout of every health check. The default value is 3 seconds.","title":"Health Checks"},{"location":"rancher/csi-driver/","text":"Harvester CSI Driver \u00b6 The Harvester Container Storage Interface (CSI) Driver provides a CSI interface used by guest kubernetes clusters in Harvester. It connects to the host cluster and hot-plug host volumes to the VMs to provide native storage performance. Deploying \u00b6 Prerequisites \u00b6 The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines running as guest Kubernetes nodes are in the same namespace. Deploying with Harvester RKE2 node driver \u00b6 When spin up a kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed when harvester cloud provider is selected. Deploying with Harvester RKE1 node driver \u00b6 Select the external cloud provider option. Generate addon configuration and add it in the rke config yaml. # depend on kubectl to operate the Harvester ./deploy/generate_addon.sh <serviceaccount name> <namespace>","title":"Harvester CSI Driver"},{"location":"rancher/csi-driver/#harvester-csi-driver","text":"The Harvester Container Storage Interface (CSI) Driver provides a CSI interface used by guest kubernetes clusters in Harvester. It connects to the host cluster and hot-plug host volumes to the VMs to provide native storage performance.","title":"Harvester CSI Driver"},{"location":"rancher/csi-driver/#deploying","text":"","title":"Deploying"},{"location":"rancher/csi-driver/#prerequisites","text":"The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines running as guest Kubernetes nodes are in the same namespace.","title":"Prerequisites"},{"location":"rancher/csi-driver/#deploying-with-harvester-rke2-node-driver","text":"When spin up a kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed when harvester cloud provider is selected.","title":"Deploying with Harvester RKE2 node driver"},{"location":"rancher/csi-driver/#deploying-with-harvester-rke1-node-driver","text":"Select the external cloud provider option. Generate addon configuration and add it in the rke config yaml. # depend on kubectl to operate the Harvester ./deploy/generate_addon.sh <serviceaccount name> <namespace>","title":"Deploying with Harvester RKE1 node driver"},{"location":"rancher/node-driver/","text":"Harvester Node Driver \u00b6 The Harvester node driver is used to provision VMs in the Harvester cluster. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver , and the project repo is available at harvester/docker-machine-driver-harvester . Users can now provision RKE1/RKE2 Kubernetes clusters in Rancher v2.6.1 using the built-in Harvester Node Driver. Additionally, Harvester now can provide built-in Load Balancer support as well as raw cluster persistent storage support to the guest Kubernetes cluster. Note Currently Rancher v2.6.1 is the only version that is compatible with Harvester v0.3.0. Enable Harvester Node Driver \u00b6 The Harvester node driver is not enabled by default from the Rancher UI, click the Cluster Management tab to enable the Harvester node driver. click the Drivers page and click the Node Drivers tab select the Harvester node driver and click Active to enable the Harvester node driver. Now users can spin up Kubernetes clusters on top of the Harvester cluster, and manage them there. RKE1 Kubernetes Cluster \u00b6 Click to view how to creat RKE1 Kubernetes Cluster RKE2 Kubernetes Cluster \u00b6 Click to view how to creat RKE2 Kubernetes Cluster","title":"Harvester Node Driver"},{"location":"rancher/node-driver/#harvester-node-driver","text":"The Harvester node driver is used to provision VMs in the Harvester cluster. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver , and the project repo is available at harvester/docker-machine-driver-harvester . Users can now provision RKE1/RKE2 Kubernetes clusters in Rancher v2.6.1 using the built-in Harvester Node Driver. Additionally, Harvester now can provide built-in Load Balancer support as well as raw cluster persistent storage support to the guest Kubernetes cluster. Note Currently Rancher v2.6.1 is the only version that is compatible with Harvester v0.3.0.","title":"Harvester Node Driver"},{"location":"rancher/node-driver/#enable-harvester-node-driver","text":"The Harvester node driver is not enabled by default from the Rancher UI, click the Cluster Management tab to enable the Harvester node driver. click the Drivers page and click the Node Drivers tab select the Harvester node driver and click Active to enable the Harvester node driver. Now users can spin up Kubernetes clusters on top of the Harvester cluster, and manage them there.","title":"Enable Harvester Node Driver"},{"location":"rancher/node-driver/#rke1-kubernetes-cluster","text":"Click to view how to creat RKE1 Kubernetes Cluster","title":"RKE1 Kubernetes Cluster"},{"location":"rancher/node-driver/#rke2-kubernetes-cluster","text":"Click to view how to creat RKE2 Kubernetes Cluster","title":"RKE2 Kubernetes Cluster"},{"location":"rancher/rancher-integration/","text":"Rancher Integration \u00b6 Available as of v0.3.0 Rancher is an open-source multi-cluster management platform. Harvester has made an integration with Rancher(start from v2.6.1) on its HCI mode. Users can now import and manage multiple Harvester clusters using Rancher v2.6.1 Virtualization Management feature and leverage Rancher authentication mechanism and RBAC control for multi-tenancy support. Deploy Rancher \u00b6 Previously in Harvester v0.2.0, we've provided the option to enable the embedded Rancher server. This option has been removed from Harvester v0.3.0 . To use Rancher with Harvester, please install Rancher v2.6.1 separately from Harvester. You can spin up a VM with Rancher v2.6.1 to try out the integration features. Quick Start Guide \u00b6 Begin creation of a custom cluster by provisioning a Linux host. Your host can be: A cloud-host virtual machine (VM) An on-prem VM A bare-metal server Log in to your Linux host using your preferred shell, such as PuTTy or a remote Terminal connection. From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart = unless-stopped -p 80 :80 -p 443 :443 --privileged rancher/rancher:v2.6.1 Note For more detail about how to deploy the Rancher server, please refer to the Rancher doc Virtualization Management \u00b6 With Rancher's Virtualization Management, users can now import and manage the Harvester clusters, by click one of the cluster, user was able to view manage the downstream Harvester resources e.g like VMs, images, volumes and so on. Additionally, it has leveraged the Rancher existing features like authentication with various auth providers and multi-tenant support. For more details please reference the virtualization management page. Note Virtualization Management is in Tech Preview. Creating K8s Clusters using the Harvester Node Driver \u00b6 Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage Kubernetes clusters. Starting from Rancher v2.6.1 , the Harvester node driver has been added by default. Users can reference this doc for more details. Note Harvester Node Driver is in Tech Preview.","title":"Rancher integration"},{"location":"rancher/rancher-integration/#rancher-integration","text":"Available as of v0.3.0 Rancher is an open-source multi-cluster management platform. Harvester has made an integration with Rancher(start from v2.6.1) on its HCI mode. Users can now import and manage multiple Harvester clusters using Rancher v2.6.1 Virtualization Management feature and leverage Rancher authentication mechanism and RBAC control for multi-tenancy support.","title":"Rancher Integration"},{"location":"rancher/rancher-integration/#deploy-rancher","text":"Previously in Harvester v0.2.0, we've provided the option to enable the embedded Rancher server. This option has been removed from Harvester v0.3.0 . To use Rancher with Harvester, please install Rancher v2.6.1 separately from Harvester. You can spin up a VM with Rancher v2.6.1 to try out the integration features.","title":"Deploy Rancher"},{"location":"rancher/rancher-integration/#quick-start-guide","text":"Begin creation of a custom cluster by provisioning a Linux host. Your host can be: A cloud-host virtual machine (VM) An on-prem VM A bare-metal server Log in to your Linux host using your preferred shell, such as PuTTy or a remote Terminal connection. From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart = unless-stopped -p 80 :80 -p 443 :443 --privileged rancher/rancher:v2.6.1 Note For more detail about how to deploy the Rancher server, please refer to the Rancher doc","title":"Quick Start Guide"},{"location":"rancher/rancher-integration/#virtualization-management","text":"With Rancher's Virtualization Management, users can now import and manage the Harvester clusters, by click one of the cluster, user was able to view manage the downstream Harvester resources e.g like VMs, images, volumes and so on. Additionally, it has leveraged the Rancher existing features like authentication with various auth providers and multi-tenant support. For more details please reference the virtualization management page. Note Virtualization Management is in Tech Preview.","title":"Virtualization Management"},{"location":"rancher/rancher-integration/#creating-k8s-clusters-using-the-harvester-node-driver","text":"Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage Kubernetes clusters. Starting from Rancher v2.6.1 , the Harvester node driver has been added by default. Users can reference this doc for more details. Note Harvester Node Driver is in Tech Preview.","title":"Creating K8s Clusters using the Harvester Node Driver"},{"location":"rancher/rke1-cluster/","text":"RKE1 Kubernetes Cluster \u00b6 Users can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1 using the built-in Harvester Node Driver. Note Harvester RKE1 node driver is in tech preview. VLAN network is required for Harvester node driver. Create RKE1 Kubernetes Cluster \u00b6 Create Your Cloud Credentials \u00b6 Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name Select \"Imported Harvester\" or \"External Harvester\" Click Create . Create Node Template \u00b6 You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials Configure Instance Options Configure the CPU, memory, and disk Select an OS image that is compatible with the cloud-init config. Select a network that the node driver is able to connect to, currently only VLAN is supported. Enter the SSH User, the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu . Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more info. Create RKE1 Kubernetes Cluster \u00b6 Users can create a RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE1 . Select Harvester node driver. Cluster Name is required. Name Prefix is required. Template is required. etcd and Control Plane is required.","title":"RKE1 Kubernetes Cluster"},{"location":"rancher/rke1-cluster/#rke1-kubernetes-cluster","text":"Users can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1 using the built-in Harvester Node Driver. Note Harvester RKE1 node driver is in tech preview. VLAN network is required for Harvester node driver.","title":"RKE1 Kubernetes Cluster"},{"location":"rancher/rke1-cluster/#create-rke1-kubernetes-cluster","text":"","title":"Create RKE1 Kubernetes Cluster"},{"location":"rancher/rke1-cluster/#create-your-cloud-credentials","text":"Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name Select \"Imported Harvester\" or \"External Harvester\" Click Create .","title":"Create Your Cloud Credentials"},{"location":"rancher/rke1-cluster/#create-node-template","text":"You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials Configure Instance Options Configure the CPU, memory, and disk Select an OS image that is compatible with the cloud-init config. Select a network that the node driver is able to connect to, currently only VLAN is supported. Enter the SSH User, the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu . Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more info.","title":"Create Node Template"},{"location":"rancher/rke1-cluster/#create-rke1-kubernetes-cluster_1","text":"Users can create a RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE1 . Select Harvester node driver. Cluster Name is required. Name Prefix is required. Template is required. etcd and Control Plane is required.","title":"Create RKE1 Kubernetes Cluster"},{"location":"rancher/rke2-cluster/","text":"RKE2 Kubernetes Cluster \u00b6 Users can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1 using the built-in Harvester Node Driver. Note Harvester RKE2 node driver is in tech preview. VLAN network is required for Harvester node driver. Create Your Cloud Credentials \u00b6 Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name Select \"Imported Harvester\" or \"External Harvester\" Click Create . Create RKE2 Kubernetes Cluster \u00b6 Users can create a RKE2 Kubernetes cluster from the Cluster Management page via RKE2 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE2/K3s . Select Harvester node driver. Select a Cloud Credential . Cluster Name is required. Namespace is required. Image is required. Network Name is required. SSH User is required. Note With RKE2 v1.21.5+rke2r2 or above, it provides a built-in Harvester Cloud Provider and Guest CSI driver integration currently only imported Harvester cluster is supported automatically","title":"RKE2 Kubernetes Cluster"},{"location":"rancher/rke2-cluster/#rke2-kubernetes-cluster","text":"Users can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1 using the built-in Harvester Node Driver. Note Harvester RKE2 node driver is in tech preview. VLAN network is required for Harvester node driver.","title":"RKE2 Kubernetes Cluster"},{"location":"rancher/rke2-cluster/#create-your-cloud-credentials","text":"Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name Select \"Imported Harvester\" or \"External Harvester\" Click Create .","title":"Create Your Cloud Credentials"},{"location":"rancher/rke2-cluster/#create-rke2-kubernetes-cluster","text":"Users can create a RKE2 Kubernetes cluster from the Cluster Management page via RKE2 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE2/K3s . Select Harvester node driver. Select a Cloud Credential . Cluster Name is required. Namespace is required. Image is required. Network Name is required. SSH User is required. Note With RKE2 v1.21.5+rke2r2 or above, it provides a built-in Harvester Cloud Provider and Guest CSI driver integration currently only imported Harvester cluster is supported automatically","title":"Create RKE2 Kubernetes Cluster"},{"location":"rancher/virtualization-management/","text":"Virtualization Management \u00b6 From Harvester v0.3.0, virtualization management with multi-cluster feature will be supported using the Rancher 2.6.x version Firstly, you will need to install the Rancher v2.6.1 server or above, for testing purpose you can spin up a rancher server using the docker run command, e.g., $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.1 Note For production environment setup please refer to the official rancher docs Once the Rancher server is up and running, login and click the hamburger menu, choose the Virtualization Management tab, and select Import Existing to import the downstream Harvester cluster into the Rancher server Specify the cluster name and click create, then u will get the registration commands, copy the command and ssh to one of the Harvester management node and run this command accordingly. Once the agent node is ready you should be able to view and access the imported Harvester cluster from the Rancher server and mange your VMs accordingly. From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster page. Multi-tenancy \u00b6 In Harvester, we have leveraged the existing Rancher RBAC authorization , users are able to view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants you access to Rancher. As mentioned in Authentication , users can either be local or external. Once the user logs in to Rancher, their authorization, or their access rights within the system, is determined by global permissions, and cluster and project roles. Global Permissions : Define user authorization outside the scope of any particular cluster. Cluster and Project Roles : Define user authorization inside the specific cluster or project where they are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC . Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner have full control over the cluster and all resources inside it, e.g. like hosts, VMs, volumes, image, networks, backups, and settings A project user can be assigned to a specific project with permission to manage the resources inside the project Example \u00b6 The following example provides a good explanation of how the multi-tenant feature works: Firstly, add new users via the Rancher Users & Authentication page, click Create to add two new separated users e.g. the project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project e.g, the default project. A project-readonly is a user with read-only permission of a particular project e.g, the default project. Click one of the imported Harvester cluster and visiting the Harvester UI click the Projects/Namespaces tab select a project e.g, default and click the Edit Config menu to assign the users to this project with appropriate permissions, e.g. the project-owner user will be assigned with project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click save. Open an incognito Browser and login to different accounts e.g, the project-owner After login with the project-owner user, Click the Virtualization Management menu, you should be able to view the cluster you have been assigned to. Click the Images tab, you should be able to view a list of images uploaded to the harvester-public namespace previously, or you can upload your own image if needed. Create a VM with one of the images that you have uploaded Login with another user e.g, project-readonly , and this user will only have the read permission of this project. Note A know issue of Read-only user was able to manage API actions","title":"Virtualization management"},{"location":"rancher/virtualization-management/#virtualization-management","text":"From Harvester v0.3.0, virtualization management with multi-cluster feature will be supported using the Rancher 2.6.x version Firstly, you will need to install the Rancher v2.6.1 server or above, for testing purpose you can spin up a rancher server using the docker run command, e.g., $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.1 Note For production environment setup please refer to the official rancher docs Once the Rancher server is up and running, login and click the hamburger menu, choose the Virtualization Management tab, and select Import Existing to import the downstream Harvester cluster into the Rancher server Specify the cluster name and click create, then u will get the registration commands, copy the command and ssh to one of the Harvester management node and run this command accordingly. Once the agent node is ready you should be able to view and access the imported Harvester cluster from the Rancher server and mange your VMs accordingly. From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster page.","title":"Virtualization Management"},{"location":"rancher/virtualization-management/#multi-tenancy","text":"In Harvester, we have leveraged the existing Rancher RBAC authorization , users are able to view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants you access to Rancher. As mentioned in Authentication , users can either be local or external. Once the user logs in to Rancher, their authorization, or their access rights within the system, is determined by global permissions, and cluster and project roles. Global Permissions : Define user authorization outside the scope of any particular cluster. Cluster and Project Roles : Define user authorization inside the specific cluster or project where they are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC . Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner have full control over the cluster and all resources inside it, e.g. like hosts, VMs, volumes, image, networks, backups, and settings A project user can be assigned to a specific project with permission to manage the resources inside the project","title":"Multi-tenancy"},{"location":"rancher/virtualization-management/#example","text":"The following example provides a good explanation of how the multi-tenant feature works: Firstly, add new users via the Rancher Users & Authentication page, click Create to add two new separated users e.g. the project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project e.g, the default project. A project-readonly is a user with read-only permission of a particular project e.g, the default project. Click one of the imported Harvester cluster and visiting the Harvester UI click the Projects/Namespaces tab select a project e.g, default and click the Edit Config menu to assign the users to this project with appropriate permissions, e.g. the project-owner user will be assigned with project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click save. Open an incognito Browser and login to different accounts e.g, the project-owner After login with the project-owner user, Click the Virtualization Management menu, you should be able to view the cluster you have been assigned to. Click the Images tab, you should be able to view a list of images uploaded to the harvester-public namespace previously, or you can upload your own image if needed. Create a VM with one of the images that you have uploaded Login with another user e.g, project-readonly , and this user will only have the read permission of this project. Note A know issue of Read-only user was able to manage API actions","title":"Example"},{"location":"reference/api/","text":"API Reference \u00b6 const ui = SwaggerUIBundle({ url: 'swagger.json', dom_id: '#swagger-ui', })","title":"API Reference"},{"location":"reference/api/#api-reference","text":"const ui = SwaggerUIBundle({ url: 'swagger.json', dom_id: '#swagger-ui', })","title":"API Reference"},{"location":"terraform/terraform/","text":"Harvester Terraform Provider \u00b6 Requirements \u00b6 Terraform >= 0.13.x Go 1.16 to build the provider plugin Install The Provider \u00b6 Option 1: Download and Install The Provider From Releases \u00b6 tar -zxvf terraform-provider-harvester-amd64.tar.gz ./install-terraform-provider-harvester.sh Option 2: Build and Install The Provider Manually \u00b6 1. Build the provider \u00b6 Clone repository git clone git@github.com:harvester/terraform-provider-harvester Enter the provider directory and build the provider This will build the provider and put the provider binary in ./bin . cd terraform-provider-harvester make 2. Install the provider \u00b6 The expected location for the Harvester provider for that target platform within one of the local search directories would be like the following: registry.terraform.io/harvester/harvester/0.2.6/linux_amd64/terraform-provider-harvester_v0.2.6 The default location for locally-installed providers is one of the following, depending on which operating system you are running Terraform under: Windows: %APPDATA%\\terraform.d\\plugins All other systems: ~/.terraform.d/plugins Place the provider into the plugin directory, for example: version = 0 .2.6 arch = linux_amd64 terraform_harvester_provider_bin = ./bin/terraform-provider-harvester terraform_harvester_provider_dir = \" ${ HOME } /.terraform.d/plugins/registry.terraform.io/harvester/harvester/ ${ version } / ${ arch } /\" mkdir -p \" ${ terraform_harvester_provider_dir } \" cp ${ terraform_harvester_provider_bin } \" ${ terraform_harvester_provider_dir } /terraform-provider-harvester_v ${ version } \" } Using the provider \u00b6 After placing it into your plugins directory, run terraform init to initialize it. Documentation about the provider specific configuration options can be found on the docs directory .","title":"Harvester Terraform Provider"},{"location":"terraform/terraform/#harvester-terraform-provider","text":"","title":"Harvester Terraform Provider"},{"location":"terraform/terraform/#requirements","text":"Terraform >= 0.13.x Go 1.16 to build the provider plugin","title":"Requirements"},{"location":"terraform/terraform/#install-the-provider","text":"","title":"Install The Provider"},{"location":"terraform/terraform/#option-1-download-and-install-the-provider-from-releases","text":"tar -zxvf terraform-provider-harvester-amd64.tar.gz ./install-terraform-provider-harvester.sh","title":"Option 1: Download and Install The Provider From Releases"},{"location":"terraform/terraform/#option-2-build-and-install-the-provider-manually","text":"","title":"Option 2: Build and Install The Provider Manually"},{"location":"terraform/terraform/#1-build-the-provider","text":"Clone repository git clone git@github.com:harvester/terraform-provider-harvester Enter the provider directory and build the provider This will build the provider and put the provider binary in ./bin . cd terraform-provider-harvester make","title":"1. Build the provider"},{"location":"terraform/terraform/#2-install-the-provider","text":"The expected location for the Harvester provider for that target platform within one of the local search directories would be like the following: registry.terraform.io/harvester/harvester/0.2.6/linux_amd64/terraform-provider-harvester_v0.2.6 The default location for locally-installed providers is one of the following, depending on which operating system you are running Terraform under: Windows: %APPDATA%\\terraform.d\\plugins All other systems: ~/.terraform.d/plugins Place the provider into the plugin directory, for example: version = 0 .2.6 arch = linux_amd64 terraform_harvester_provider_bin = ./bin/terraform-provider-harvester terraform_harvester_provider_dir = \" ${ HOME } /.terraform.d/plugins/registry.terraform.io/harvester/harvester/ ${ version } / ${ arch } /\" mkdir -p \" ${ terraform_harvester_provider_dir } \" cp ${ terraform_harvester_provider_bin } \" ${ terraform_harvester_provider_dir } /terraform-provider-harvester_v ${ version } \" }","title":"2. Install the provider"},{"location":"terraform/terraform/#using-the-provider","text":"After placing it into your plugins directory, run terraform init to initialize it. Documentation about the provider specific configuration options can be found on the docs directory .","title":"Using the provider"},{"location":"vm/access-to-the-vm/","text":"Access to the VM \u00b6 Once the VM is up and running, it can be accessed using either VNC or the serial console from the Harvester UI. Optionally, connect directly from your computer's SSH client. Access with the UI \u00b6 VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM (e.g., when using the Ubuntu minimal cloud image), the VM can be accessed with the serial console. Access using SSH \u00b6 Use the address in a terminal emulation client (such as Putty) or use the following command line to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@<ip-address-or-hostname>","title":"Access to the VM"},{"location":"vm/access-to-the-vm/#access-to-the-vm","text":"Once the VM is up and running, it can be accessed using either VNC or the serial console from the Harvester UI. Optionally, connect directly from your computer's SSH client.","title":"Access to the VM"},{"location":"vm/access-to-the-vm/#access-with-the-ui","text":"VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM (e.g., when using the Ubuntu minimal cloud image), the VM can be accessed with the serial console.","title":"Access with the UI"},{"location":"vm/access-to-the-vm/#access-using-ssh","text":"Use the address in a terminal emulation client (such as Putty) or use the following command line to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@<ip-address-or-hostname>","title":"Access using SSH"},{"location":"vm/backup-restore/","text":"VM Backup & Restore \u00b6 Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server) and they can be used to either restore a new VM or replace an existing VM. Note A backup target must be set up. For more information, see Backup Target Setup . If the BackupTarget has not been set, you\u2019ll be presented with a prompt message. Configure Backup Target \u00b6 A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings > backup-target . Parameter Type Description Type string Choose S3 or NFS Endpoint string Endpoint is a hostname or an IP address. It can be left empty for AWS S3. BucketName string Name of the bucket BucketRegion string Region of the bucket AccessKeyID string AccessKeyID is like a user-id that uniquely identifies your account. SecretAccessKey string SecretAccessKey is the password to your account. Certificate string Paste the certificate if you want to use a self-signed SSL certificate of your s3 server VirtualHostedStyle bool Use virtual-hosted-style access only, e.g., Alibaba Cloud(Aliyun) OSS Create a VM backup \u00b6 Once the backup target is set, go to the Virtual Machines page. Click Take Backup of the VM actions to create a new VM backup. Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. A notification message will be promoted, and users can go to the Advanced > Backups page to view all VM backups. The ReadyToUse status will be set to true once the Backup is complete. Users can either choose to restore a new VM, or replace an existing VM using this backup. Restore a new VM using a backup \u00b6 To restore a new VM from a backup, follow these steps: Go to the Backups page. Specify the new VM name and click Create . A new VM will be restored using the backup volumes and metadata, and users can access it from the Virtual Machines page. Replace an Existing VM using a backup \u00b6 You can replace an existing VM using the backup with the same VM backup target . You can choose to either delete the previous volumes or retain them. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the Backups page. Click Create . The restore process can be viewed from the Virtual Machines page.","title":"VM Backup & Restore"},{"location":"vm/backup-restore/#vm-backup-restore","text":"Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server) and they can be used to either restore a new VM or replace an existing VM. Note A backup target must be set up. For more information, see Backup Target Setup . If the BackupTarget has not been set, you\u2019ll be presented with a prompt message.","title":"VM Backup &amp; Restore"},{"location":"vm/backup-restore/#configure-backup-target","text":"A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings > backup-target . Parameter Type Description Type string Choose S3 or NFS Endpoint string Endpoint is a hostname or an IP address. It can be left empty for AWS S3. BucketName string Name of the bucket BucketRegion string Region of the bucket AccessKeyID string AccessKeyID is like a user-id that uniquely identifies your account. SecretAccessKey string SecretAccessKey is the password to your account. Certificate string Paste the certificate if you want to use a self-signed SSL certificate of your s3 server VirtualHostedStyle bool Use virtual-hosted-style access only, e.g., Alibaba Cloud(Aliyun) OSS","title":"Configure Backup Target"},{"location":"vm/backup-restore/#create-a-vm-backup","text":"Once the backup target is set, go to the Virtual Machines page. Click Take Backup of the VM actions to create a new VM backup. Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. A notification message will be promoted, and users can go to the Advanced > Backups page to view all VM backups. The ReadyToUse status will be set to true once the Backup is complete. Users can either choose to restore a new VM, or replace an existing VM using this backup.","title":"Create a VM backup"},{"location":"vm/backup-restore/#restore-a-new-vm-using-a-backup","text":"To restore a new VM from a backup, follow these steps: Go to the Backups page. Specify the new VM name and click Create . A new VM will be restored using the backup volumes and metadata, and users can access it from the Virtual Machines page.","title":"Restore a new VM using a backup"},{"location":"vm/backup-restore/#replace-an-existing-vm-using-a-backup","text":"You can replace an existing VM using the backup with the same VM backup target . You can choose to either delete the previous volumes or retain them. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the Backups page. Click Create . The restore process can be viewed from the Virtual Machines page.","title":"Replace an Existing VM using a backup"},{"location":"vm/create-vm/","text":"Create a VM \u00b6 How to Create a VM \u00b6 Create one or more virtual machines from the Virtual Machines page. Choose the option to create either one or multiple VM instances. The VM name is required. (Optional) You can select to use the VM template. By default we have added ISO, raw, and Windows image templates. Configure the CPU and Memory of the VM. Select SSH keys or upload a new one. Go to the Volumes tab, Select a custom VM image. Also you can add more disks to the VM, The default disk will be the root disk. To configure networks, go to the Networks tab. The Management Network is added by default. It is also possible to add secondary networks to the VMs using vlan networks (configured on Advanced > Networks ). (Optional) Configure advanced options like hostname and cloud-init data in the Advanced Options section. Cloud config examples \u00b6 Config for the password of the default user: #cloud-config password : password chpasswd : { expire : False } ssh_pwauth : True Network-data configuration using DHCP: version : 1 config : - type : physical name : eth0 subnets : - type : dhcp - type : physical name : eth1 subnets : - type : dhcp You can also use the Cloud Config Template feature to include a pre-defined cloud-init config for the VM. Networks \u00b6 Management Network \u00b6 A management network represents the default vm eth0 interface configured by the cluster network solution that is present in each VM. By default, a VM can be accessed via the management network. Secondary Network \u00b6 It is also possible to connect VMs using additional networks with Harvester's built-in vlan networks .","title":"Create a VM"},{"location":"vm/create-vm/#create-a-vm","text":"","title":"Create a VM"},{"location":"vm/create-vm/#how-to-create-a-vm","text":"Create one or more virtual machines from the Virtual Machines page. Choose the option to create either one or multiple VM instances. The VM name is required. (Optional) You can select to use the VM template. By default we have added ISO, raw, and Windows image templates. Configure the CPU and Memory of the VM. Select SSH keys or upload a new one. Go to the Volumes tab, Select a custom VM image. Also you can add more disks to the VM, The default disk will be the root disk. To configure networks, go to the Networks tab. The Management Network is added by default. It is also possible to add secondary networks to the VMs using vlan networks (configured on Advanced > Networks ). (Optional) Configure advanced options like hostname and cloud-init data in the Advanced Options section.","title":"How to Create a VM"},{"location":"vm/create-vm/#cloud-config-examples","text":"Config for the password of the default user: #cloud-config password : password chpasswd : { expire : False } ssh_pwauth : True Network-data configuration using DHCP: version : 1 config : - type : physical name : eth0 subnets : - type : dhcp - type : physical name : eth1 subnets : - type : dhcp You can also use the Cloud Config Template feature to include a pre-defined cloud-init config for the VM.","title":"Cloud config examples"},{"location":"vm/create-vm/#networks","text":"","title":"Networks"},{"location":"vm/create-vm/#management-network","text":"A management network represents the default vm eth0 interface configured by the cluster network solution that is present in each VM. By default, a VM can be accessed via the management network.","title":"Management Network"},{"location":"vm/create-vm/#secondary-network","text":"It is also possible to connect VMs using additional networks with Harvester's built-in vlan networks .","title":"Secondary Network"},{"location":"vm/hotplug-volume/","text":"sidebar_position: 5 keywords: - Harvester - Hot-plug - Volume Description: Adding hot-plug volumes to a running VM. Hot-Plug Volumes \u00b6 Harvester supports adding hot-plug volumes to a running VM. Adding hot-plug volumes to a running VM \u00b6 The following steps assume that you have a running VM and a ready volume. Go to the Virtual Machines page. Find the VM that you want to add a volume to and select Vertical \u22ee (\u2026 ) > Add Volume . Enter the Name and select the Volume . Click Apply .","title":"Hotplug volume"},{"location":"vm/hotplug-volume/#hot-plug-volumes","text":"Harvester supports adding hot-plug volumes to a running VM.","title":"Hot-Plug Volumes"},{"location":"vm/hotplug-volume/#adding-hot-plug-volumes-to-a-running-vm","text":"The following steps assume that you have a running VM and a ready volume. Go to the Virtual Machines page. Find the VM that you want to add a volume to and select Vertical \u22ee (\u2026 ) > Add Volume . Enter the Name and select the Volume . Click Apply .","title":"Adding hot-plug volumes to a running VM"},{"location":"vm/live-migration/","text":"Live Migration \u00b6 Live migration means moving a virtual machine to a different host without downtime. Note Live migration is not allowed when the virtual machine is using a management network of bridge interface type. To support live migration, 3 or more hosts in the Harvester cluster are required due to a known issue . Starting a migration \u00b6 Go to the Virtual Machines page. Find the virtual machine that you want to migrate and select Vertical \u22ee (... ) > Migrate . Choose the node that you want to migrate the virtual machine to. Click Apply . Aborting a migration \u00b6 Go to the Virtual Machines page. Find the virtual machine that is in migrating status and you want to abort the migration. Select Vertical \u22ee (... ) > Abort Migration . Migration timeouts \u00b6 Completion timeout \u00b6 The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied, which will prevent the migration process from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout which is 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds. Progress timeout \u00b6 Live migration will also be aborted when it is noticed that copying memory doesn't make any progress in 150s.","title":"Live Migration"},{"location":"vm/live-migration/#live-migration","text":"Live migration means moving a virtual machine to a different host without downtime. Note Live migration is not allowed when the virtual machine is using a management network of bridge interface type. To support live migration, 3 or more hosts in the Harvester cluster are required due to a known issue .","title":"Live Migration"},{"location":"vm/live-migration/#starting-a-migration","text":"Go to the Virtual Machines page. Find the virtual machine that you want to migrate and select Vertical \u22ee (... ) > Migrate . Choose the node that you want to migrate the virtual machine to. Click Apply .","title":"Starting a migration"},{"location":"vm/live-migration/#aborting-a-migration","text":"Go to the Virtual Machines page. Find the virtual machine that is in migrating status and you want to abort the migration. Select Vertical \u22ee (... ) > Abort Migration .","title":"Aborting a migration"},{"location":"vm/live-migration/#migration-timeouts","text":"","title":"Migration timeouts"},{"location":"vm/live-migration/#completion-timeout","text":"The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied, which will prevent the migration process from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout which is 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds.","title":"Completion timeout"},{"location":"vm/live-migration/#progress-timeout","text":"Live migration will also be aborted when it is noticed that copying memory doesn't make any progress in 150s.","title":"Progress timeout"}]}