---
sidebar_position: 2
sidebar_label: Upgrade from v1.4.0 to v1.4.1
title: "Upgrade from v1.4.0 to v1.4.1"
---

<head>
  <link rel="canonical" href="https://docs.harvesterhci.io/v1.4/upgrade/v1-4-0-to-v1-4-1"/>
</head>

## General information

An **Upgrade** button appears on the **Dashboard** screen whenever a new Harvester version that you can upgrade to becomes available. For more information, see [Start an upgrade](./automatic.md#start-an-upgrade).

For air-gapped environments, see [Prepare an air-gapped upgrade](./automatic.md#prepare-an-air-gapped-upgrade).

:::info important

Before upgrading from v1.4.0 to v1.4.1, check the current disk usage of the OS images on each node. To do this, log in via `ssh` and run the following command:

```
# du -sh /run/initramfs/cos-state/cOS/*
1.7G    /run/initramfs/cos-state/cOS/active.img
3.1G    /run/initramfs/cos-state/cOS/passive.img
```
If `passive.img` is using 3.1G of disk space as in the above example, become root and run the following commands:
```
# mount -o remount,rw /run/initramfs/cos-state
# fallocate --dig-holes /run/initramfs/cos-state/cOS/passive.img
# mount -o remount,ro /run/initramfs/cos-state
```
This will turn `passive.img` into a sparse file, which should then only be using 1.7G of disk space, the same as `active.img`. Doing this ensures that there will be enough free space on the COS_STATE partition to avoid hitting the [Upgrade is stuck in the "Waiting Reboot" state](#3-upgrade-is-stuck-in-the-waiting-reboot-state) problem.
:::


### Update Harvester UI Extension on Rancher v2.10.1

To import Harvester v1.4.1 clusters on Rancher v2.10.1, you must use **v1.0.3** of the Rancher UI extension for Harvester.


1. On the Rancher UI, go to **local > Apps > Repositories**.

1. Locate the repository named **harvester**, and then select **⋮ > Refresh**.
    This repository has the following properties:
    
    - URL: **https://github.com/harvester/harvester-ui-extension**
    - Branch: **gh-pages**
    ![](/img/v1.4/upgrade/rancher-2.10.1-repository-page.png)

1. Go to the **Extensions** screen.

1. Locate the extension named **Harvester**, and then click **Update**.

1. Select version **1.0.3**, and then click **Update**.
    ![](/img/v1.4/upgrade/update-harvester-ui-extension-modal.png)

1. Allow some time for the extension to be updated, and then refresh the screen.

:::info important
The Rancher UI displays an error message after the extension is updated. The error message disappears when you refresh the screen.
This issue, which exists in Rancher v2.10.0 and v2.10.1, will be fixed in v2.10.2. 
:::

Related issues:
 - [Issue #7234](https://github.com/harvester/harvester/issues/7234) 
 - [Issue #107](https://github.com/rancher/capi-ui-extension/issues/107) 


## Known issues

---

### 1. Upgrade is stuck in the "Pre-drained" state

The upgrade process may become stuck in the "Pre-drained" state. Kubernetes is supposed to drain the workload on the node, but some factors may cause the process to stall.

![](/img/v1.2/upgrade/known_issues/3730-stuck.png)

A possible cause is processes related to orphan engines of the Longhorn Instance Manager. To determine if this applies to your situation, perform the following steps:

1. Check the name of the `instance-manager` pod on the stuck node.

    Example:

    The stuck node is `harvester-node-1`, and the name of the Instance Manager pod is `instance-manager-d80e13f520e7b952f4b7593fc1883e2a`.

    ```
    $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager
    instance-manager-d80e13f520e7b952f4b7593fc1883e2a          1/1     Running   0              3d8h
    ```

1. Check the Longhorn Manager logs for informational messages.

    Example:

    ```
    $ kubectl -n longhorn-system logs daemonsets/longhorn-manager
    ...
    time="2025-01-14T00:00:01Z" level=info msg="Node instance-manager-d80e13f520e7b952f4b7593fc1883e2a is marked unschedulable but removing harvester-node-1 PDB is blocked: some volumes are still attached InstanceEngines count 1 pvc-9ae0e9a5-a630-4f0c-98cc-b14893c74f9e-e-0" func="controller.(*InstanceManagerController).syncInstanceManagerPDB" file="instance_manager_controller.go:823" controller=longhorn-instance-manager node=harvester-node-1
    ```

    The `instance-manager` pod cannot be drained because of the engine `pvc-9ae0e9a5-a630-4f0c-98cc-b14893c74f9e-e-0`.

1. Check if the engine is still running on the stuck node.

    Example:

    ```
    $ kubectl -n longhorn-system get engines.longhorn.io pvc-9ae0e9a5-a630-4f0c-98cc-b14893c74f9e-e-0 -o jsonpath='{"Current state: "}{.status.currentState}{"\nNode ID: "}{.spec.nodeID}{"\n"}'
    Current state: stopped
    Node ID:
    ```

    The issue likely exists if the output shows that the engine is not running or even the engine is not found.

1. Check if all volumes are healthy.

    ```
    kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == "attached")| .status.robustness'
    ```

    All volumes must be marked `healthy`. If this is not the case, please help to report the issue.

1. Remove the `instance-manager` pod's PodDisruptionBudget (PDB) .

    Example:

    ```
    kubectl delete pdb instance-manager-d80e13f520e7b952f4b7593fc1883e2a -n longhorn-system
    ```

Related issues:
  - [[BUG] v1.4.0 -> v1.4.1-rc1 upgrade stuck in Pre-drained and the node stay in Cordoned](https://github.com/harvester/harvester/issues/7366)
  - [[IMPROVEMENT] Cleanup orphaned volume runtime resources if the resources already deleted](https://github.com/longhorn/longhorn/issues/6764)

### 2. Upgrade with default StorageClass that is not harvester-longhorn

Harvester adds the annotation `storageclass.kubernetes.io/is-default-class: "true"` to `harvester-longhorn`, which is the original default StorageClass. When you replace `harvester-longhorn` with another StorageClass, the following occur:

- The Harvester ManagedChart shows the error message `cannot patch "harvester-longhorn" with kind StorageClass: admission webhook "validator.harvesterhci.io" denied the request: default storage class %!s(MISSING) already exists, please reset it first`.

- The webhook denies the upgrade request.

    ![Upgrade with another default storage class](/img/v1.4/upgrade/upgrade-with-another-default-storage-class.png)

You can perform any of the following workarounds:

- Set `harvester-longhorn` as the default StorageClass.

- Add `spec.values.storageClass.defaultStorageClass: false` to the `harvester` ManagedChart.

    ```
    kubectl edit managedchart harvester -n fleet-local
    ```

- Add `timeoutSeconds: 600` to the Harvester ManagedChart spec.

    ```
    kubectl edit managedchart harvester -n fleet-local
    ```

    ![Upgrade with another default storage class workaround](/img/v1.4/upgrade/upgrade-with-another-default-storage-class-workaround.png)

For more information, see [Issue #7375](https://github.com/harvester/harvester/issues/7375).

### 3. Upgrade is stuck in the "Waiting Reboot" state

The upgrade process may become stuck in the "Waiting Reboot" state. At this point, a new Harvester v1.4.1 OS image has been installed on a node, a reboot has been initiated, and the upgrade controller is waiting to observe that specific OS version running.

If the new Harvester v1.4.1 OS image (hereafter referred to as `active.img`) fails to boot for any reason, the node will automatically restart in fallback mode, i.e. it will then be running the previous Harvester v1.4.0 OS image (`passive.img`). It will remain that way, and thus the upgrade will be stuck "Waiting Reboot", until the administrator investigates and fixes the problem with `active.img`.

If you encounter this issue, the most likely cause is that there was insufficient disk space on the COS_STATE parition during upgrade, which has resulted in a corrupt (and thus unbootable) `active.img`. This will occur if the system was originally installed with Harvester v1.4.0, and was configured to use a separate data disk. If the system only has a single disk shared between OS and data, or if the system was originally installed with an earlier verison of Harvester then later upgraded to v1.4.0 before upgrading to v1.4.1, the problem will not occur.

To verify if you have this problem, log in to the affected node via `ssh`, become root, and run the following commands:

```
# cat /proc/cmdline
BOOT_IMAGE=(loop0)/boot/vmlinuz console=tty1 root=LABEL=COS_STATE cos-img/filename=/cOS/passive.img panic=0 net.ifnames=1 rd.cos.oemlabel=COS_OEM rd.cos.mount=LABEL=COS_OEM:/oem rd.cos.mount=LABEL=COS_PERSISTENT:/usr/local rd.cos.oemtimeout=120 audit=1 audit_backlog_limit=8192 intel_iommu=on amd_iommu=on iommu=pt multipath=off upgrade_failure

# head -n1 /etc/harvester-release.yaml
harvester: v1.4.0
```

The presence of `cos-img/filename=/cOS/passive.img` and `upgrade_failure` in the above ouptut indicates that the system has booted into fallback mode. The Harvester version in `/etc/harvester-release.yaml` confirms that the system is currently using the Harvester v1.4.0 OS image.

Next, check if `active.img` is corrupt:

```
# fsck.ext2 -nf /run/initramfs/cos-state/cOS/active.img
e2fsck 1.46.4 (18-Aug-2021)
Pass 1: Checking inodes, blocks, and sizes
Pass 2: Checking directory structure

  [...a list of various different errors may appear here...]

e2fsck: aborted

COS_ACTIVE: ********** WARNING: Filesystem still has errors **********
```

Finally, check partition sizes:

```
# lsblk -o NAME,LABEL,SIZE
NAME   LABEL             SIZE
loop0  COS_ACTIVE          3G
sr0                     1024M
vda                      250G
├─vda1 COS_GRUB           64M
├─vda2 COS_OEM            64M
├─vda3 COS_RECOVERY        4G
├─vda4 COS_STATE           8G
└─vda5 COS_PERSISTENT  237.9G
vdb    HARV_LH_DEFAULT   128G
```

In the above output, the COS_STATE partition is 8G in size. In this specific case (a system upgraded to v1.4.1, which now has a corrupt `active.img`) this a good indicator that there initially wasn't enough free space on that parition for the upgrade to succeed.

To fix this problem:

1. If you have two or more nodes, on each of the remaining nodes that are yet to be upgraded, log in via `ssh` and check the disk usage of `active.img` and `passive.img`:
   ```
   # du -sh /run/initramfs/cos-state/cOS/*
   1.7G    /run/initramfs/cos-state/cOS/active.img
   3.1G    /run/initramfs/cos-state/cOS/passive.img
   ```
   If `passive.img` is using 3.1G of disk space, become root and run the following commands:
   ```
   # mount -o remount,rw /run/initramfs/cos-state
   # fallocate --dig-holes /run/initramfs/cos-state/cOS/passive.img 
   # mount -o remount,ro /run/initramfs/cos-state
   ```
   This will turn `passive.img` into a sparse file, which should then only be using 1.7G of disk space, the same as `active.img`. Doing this ensures that there will be enough free space on those other nodes to not get stuck again as the upgrade proceeds.

2. On the stuck node, log in via `ssh`, become root and run the following commands:
   ```
   # mount -o remount,rw /run/initramfs/cos-state
   # cp /run/initramfs/cos-state/cOS/passive.img \
        /run/initramfs/cos-state/cOS/active.img
   # tune2fs -L COS_ACTIVE /run/initramfs/cos-state/cOS/active.img
   # mount -o remount,ro /run/initramfs/cos-state
   ```
   This will copy your existing (clean) `passive.img` over the corrupt `active.img`, and set the label correctly so that it can be used.

3. Reboot the stuck node. When you get to the GRUB boot screen, it will still default to "Harvester v1.4.1 (fallback)". Select the first entry instead ("Harvester v1.4.1"). Despite the version shown on the GRUB boot screen, this will actually now boot the system into Harvester v1.4.0.

4. Copy `rootfs.squashfs` from the Harvester v1.4.1 ISO image to somewhere convenient on the stuck node. You can either mount the ISO directly on the stuck node, or you can mount it on another system and copy the file over via `scp`.

5. Log in to the stuck node via `ssh`, become root, and run the following commands (replace `rootfs.squashfs` on the fourth line with whatever path you copied that file to):
   ```
   # mkdir /tmp/manual-os-upgrade    
   # mkdir /tmp/manual-os-upgrade/config
   # mkdir /tmp/manual-os-upgrade/rootfs
   # mount -o loop rootfs.squashfs /tmp/manual-os-upgrade/rootfs
   # cat > /tmp/manual-os-upgrade/config/config.yaml <<EOF
   upgrade:
     system:
       size: 3072
   EOF
   # elemental upgrade \
           --logfile /tmp/manual-os-upgrade/upgrade.log \
           --directory /tmp/manual-os-upgrade/rootfs \
           --config-dir /tmp/manual-os-upgrade/config \
           --debug
   ```
   This will generate a new, clean, `active.img` based on the root image from the Harvester v1.4.1 ISO. If there are any errors, save a copy of `/tmp/manual-os-upgrade/upgrade.log` for further analysis. Otherwise, run the following commands:
   ```
   # umount /tmp/manual-os-upgrade/rootfs
   # reboot
   ```
   The node should then boot successfully into Harvester v1.4.1, and the upgrade should proceed as expected.


Related issues:
- [[BUG] Stuck upgrade from 1.4.0 to 1.4.1](https://github.com/harvester/harvester/issues/7457)
- [[BUG] discrepancy in default OS partition sizes when using separate data disk](https://github.com/harvester/harvester/issues/7493)
- [[BUG] after initial installation, passive.img uses 3.1G of disk space, vs. active.img which only uses 1.7G](https://github.com/harvester/harvester/issues/7518)

