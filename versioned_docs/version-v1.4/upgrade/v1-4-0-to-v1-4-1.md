---
sidebar_position: 2
sidebar_label: Upgrade from v1.4.0 to v1.4.1
title: "Upgrade from v1.4.0 to v1.4.1"
---

<head>
  <link rel="canonical" href="https://docs.harvesterhci.io/v1.4/upgrade/v1-4-0-to-v1-4-1"/>
</head>

## General information

An **Upgrade** button appears on the **Dashboard** screen whenever a new Harvester version that you can upgrade to becomes available. For more information, see [Start an upgrade](./automatic.md#start-an-upgrade).

For air-gapped environments, see [Prepare an air-gapped upgrade](./automatic.md#prepare-an-air-gapped-upgrade).


### 1. Upgrade Harvester UI Extension to v1.0.3 on Rancher v2.10.1

To import Harvester v1.4.1 on Rancher v2.10.1, harvester-ui-extension requires upgrade to v1.0.3.

Follow below steps to upgrade harvester-ui-extension
1. Go to Rancher UI local -> Apps -> Repositories page, find harvester repository with URL **https://github.com/harvester/harvester-ui-extension** and branch **gh-pages** then click refresh action

![](/img/v1.4/upgrade/rancher-2.10.1-repository-page.png)


2. Go to extension page, click **update** and choose version 1.0.3

![](/img/v1.4/upgrade/update-harvester-ui-extension-modal.png)


It will display error message after installing new version. The error message will auto dismiss after waiting a few moment and refresh the page. 

This is a known issue on Rancher v2.10.0 and v2.10.1 and will be fixed in v2.10.2. 

Related issues:
 - [Issue #7234](https://github.com/harvester/harvester/issues/7234) 
 - [Issue #107](https://github.com/rancher/capi-ui-extension/issues/107) 


## Known issues

---

### 1. Upgrade is stuck in the "Pre-drained" state

The upgrade process may become stuck in the "Pre-drained" state. Kubernetes is supposed to drain the workload on the node, but some factors may cause the process to stall.

![](/img/v1.2/upgrade/known_issues/3730-stuck.png)

A possible cause is processes related to orphan engines of the Longhorn Instance Manager. To determine if this applies to your situation, perform the following steps:

1. Check the name of the `instance-manager` pod on the stuck node.

    Example:

    The stuck node is `harvester-node-1`, and the name of the Instance Manager pod is `instance-manager-d80e13f520e7b952f4b7593fc1883e2a`.

    ```
    $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager
    instance-manager-d80e13f520e7b952f4b7593fc1883e2a          1/1     Running   0              3d8h
    ```

1. Check the Longhorn Manager logs for informational messages.

    Example:

    ```
    $ kubectl -n longhorn-system logs daemonsets/longhorn-manager
    ...
    time="2025-01-14T00:00:01Z" level=info msg="Node instance-manager-d80e13f520e7b952f4b7593fc1883e2a is marked unschedulable but removing harvester-node-1 PDB is blocked: some volumes are still attached InstanceEngines count 1 pvc-9ae0e9a5-a630-4f0c-98cc-b14893c74f9e-e-0" func="controller.(*InstanceManagerController).syncInstanceManagerPDB" file="instance_manager_controller.go:823" controller=longhorn-instance-manager node=harvester-node-1
    ```

    The `instance-manager` pod cannot be drained because of the engine `pvc-9ae0e9a5-a630-4f0c-98cc-b14893c74f9e-e-0`.

1. Check if the engine is still running on the stuck node.

    Example:

    ```
    $ kubectl -n longhorn-system get engines.longhorn.io pvc-9ae0e9a5-a630-4f0c-98cc-b14893c74f9e-e-0 -o jsonpath='{"Current state: "}{.status.currentState}{"\nNode ID: "}{.spec.nodeID}{"\n"}'
    Current state: stopped
    Node ID:
    ```

    The issue likely exists if the output shows that the engine is not running or even the engine is not found.

1. Check if all volumes are healthy.

    ```
    kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == "attached")| .status.robustness'
    ```

    All volumes must be marked `healthy`. If this is not the case, please help to report the issue.

1. Remove the `instance-manager` pod's PodDisruptionBudget (PDB) .

    Example:

    ```
    kubectl delete pdb instance-manager-d80e13f520e7b952f4b7593fc1883e2a -n longhorn-system
    ```

Related issues:
  - [[BUG] v1.4.0 -> v1.4.1-rc1 upgrade stuck in Pre-drained and the node stay in Cordoned](https://github.com/harvester/harvester/issues/7366)
  - [[IMPROVEMENT] Cleanup orphaned volume runtime resources if the resources already deleted](https://github.com/longhorn/longhorn/issues/6764)

### 2. Upgrade with default StorageClass that is not harvester-longhorn

Harvester adds the annotation `storageclass.kubernetes.io/is-default-class: "true"` to `harvester-longhorn`, which is the original default StorageClass. When you replace `harvester-longhorn` with another StorageClass, the following occur:

- The Harvester ManagedChart shows the error message `cannot patch "harvester-longhorn" with kind StorageClass: admission webhook "validator.harvesterhci.io" denied the request: default storage class %!s(MISSING) already exists, please reset it first`.

- The webhook denies the upgrade request.

    ![Upgrade with another default storage class](/img/v1.4/upgrade/upgrade-with-another-default-storage-class.png)

You can perform any of the following workarounds:

- Set `harvester-longhorn` as the default StorageClass.

- Add `spec.values.storageClass.defaultStorageClass: false` to the `harvester` ManagedChart.

    ```
    kubectl edit managedchart harvester -n fleet-local
    ```

- Add `timeoutSeconds: 600` to the Harvester ManagedChart spec.

    ```
    kubectl edit managedchart harvester -n fleet-local
    ```

    ![Upgrade with another default storage class workaround](/img/v1.4/upgrade/upgrade-with-another-default-storage-class-workaround.png)

For more information, see [Issue #7375](https://github.com/harvester/harvester/issues/7375).
