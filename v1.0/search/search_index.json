{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Harvester Overview \u00b6 Harvester is an open-source hyper-converged infrastructure (HCI) software built on Kubernetes. It is an open alternative to using a proprietary HCI stack that incorporates the design and ethos of Cloud Native Computing . Harvester Features \u00b6 Harvester implements HCI on bare metal servers. Harvester is designed to use local, direct attached storage instead of complex external SANs. It ships as an integrated bootable appliance image that can be deployed directly to servers through an ISO or PXE boot artifact. Some notable features of Harvester include the following: VM lifecycle management including SSH-Key injection, cloud-init, and graphic and serial port console VM live migration support Supported VM backup and restore Distributed block storage Multiple network interface controllers (NICs) in the VM connecting to the management network or VLANs Virtual Machine and cloud-init templates Rancher integration with multi-cluster management and the Harvester node driver PXE/iPXE boot support Virtual IP and bond NIC support Monitoring integration Harvester Architecture \u00b6 The following diagram outlines a high-level architecture of Harvester: Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes. KubeVirt is a virtual machine management add-on for Kubernetes. Elemental for SLE-Micro 5.2 (based on openSUSE Leap 15.3 before v1.0.3) is an immutable Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster. Hardware Requirements \u00b6 To get the Harvester server up and running, the following minimum hardware is required: Type Requirements CPU x86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum; 16-core or above preferred Memory 32 GB minimum; 64 GB or above preferred Disk Capacity 140 GB minimum for testing; 500 GB or above preferred for production Disk Performance 5,000+ random IOPS per disk (SSD/NVMe). Management nodes (first three nodes) must be fast enough for etcd Network Card 1 Gbps Ethernet minimum for testing; 10Gbps Ethernet recommended for production Network Switch Trunking of ports required for VLAN support Quick start \u00b6 You can install Harvester via the ISO installation or the PXE boot installation. Instructions are provided in the sections below. ISO Installation \u00b6 You can use the ISO to install Harvester directly on the bare metal server to form a Harvester cluster. Users can add one or many compute nodes to join the existing cluster. To get the Harvester ISO, download it from the Github releases . During the installation, you can either choose to form a new cluster or join the node to an existing cluster. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster or by joining an existing one. Choose the installation device on which the Harvester cluster will be installed Note: By default, Harvester uses GPT partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select MBR . Configure the hostname and select the network interface for the management network. By default, Harvester will create a bonded NIC named harvester-mgmt , and the IP address can be configured via DHCP or a statically assigned one (Note: The Node IP can not change at the lifecycle of a Harvester cluster, in case the DHCP is used, the user must make sure the DHCP server always offers the same IP for the same Node. Due to a changed Node IP the related Node can not join the cluster, or even break the cluster) . Optional: Configure the DNS servers; use commas as delimiters. Configure the Virtual IP which you can use to access the cluster or join other nodes to the cluster (Note: If your IP address is configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP, VIP must be different than any Node IP) . Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default SSH user is rancher . Recommended configuring the NTP server to make sure all nodes' times are synchronized. This defaults to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote server URL. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. Confirm the installation options and Harvester will be installed to your host. The installation may take a few minutes to complete. Once the installation is complete, the host will restart, and a console UI with management URL and status will be displayed. (You can Use F12 to switch between the Harvester console and the Shell). The default URL of the web interface is https://your-virtual-ip . Users will be prompted to set the password for the default admin user at first login. PXE/iPXE Installation \u00b6 Harvester can also be installed automatically. Please refer to PXE Boot Install for detailed instructions and additional guidance. Note More iPXE usage examples are available at harvester/ipxe-examples .","title":"Harvester Overview"},{"location":"#harvester-overview","text":"Harvester is an open-source hyper-converged infrastructure (HCI) software built on Kubernetes. It is an open alternative to using a proprietary HCI stack that incorporates the design and ethos of Cloud Native Computing .","title":"Harvester Overview"},{"location":"#harvester-features","text":"Harvester implements HCI on bare metal servers. Harvester is designed to use local, direct attached storage instead of complex external SANs. It ships as an integrated bootable appliance image that can be deployed directly to servers through an ISO or PXE boot artifact. Some notable features of Harvester include the following: VM lifecycle management including SSH-Key injection, cloud-init, and graphic and serial port console VM live migration support Supported VM backup and restore Distributed block storage Multiple network interface controllers (NICs) in the VM connecting to the management network or VLANs Virtual Machine and cloud-init templates Rancher integration with multi-cluster management and the Harvester node driver PXE/iPXE boot support Virtual IP and bond NIC support Monitoring integration","title":"Harvester Features"},{"location":"#harvester-architecture","text":"The following diagram outlines a high-level architecture of Harvester: Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes. KubeVirt is a virtual machine management add-on for Kubernetes. Elemental for SLE-Micro 5.2 (based on openSUSE Leap 15.3 before v1.0.3) is an immutable Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster.","title":"Harvester Architecture"},{"location":"#hardware-requirements","text":"To get the Harvester server up and running, the following minimum hardware is required: Type Requirements CPU x86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum; 16-core or above preferred Memory 32 GB minimum; 64 GB or above preferred Disk Capacity 140 GB minimum for testing; 500 GB or above preferred for production Disk Performance 5,000+ random IOPS per disk (SSD/NVMe). Management nodes (first three nodes) must be fast enough for etcd Network Card 1 Gbps Ethernet minimum for testing; 10Gbps Ethernet recommended for production Network Switch Trunking of ports required for VLAN support","title":"Hardware Requirements"},{"location":"#quick-start","text":"You can install Harvester via the ISO installation or the PXE boot installation. Instructions are provided in the sections below.","title":"Quick start"},{"location":"#iso-installation","text":"You can use the ISO to install Harvester directly on the bare metal server to form a Harvester cluster. Users can add one or many compute nodes to join the existing cluster. To get the Harvester ISO, download it from the Github releases . During the installation, you can either choose to form a new cluster or join the node to an existing cluster. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster or by joining an existing one. Choose the installation device on which the Harvester cluster will be installed Note: By default, Harvester uses GPT partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select MBR . Configure the hostname and select the network interface for the management network. By default, Harvester will create a bonded NIC named harvester-mgmt , and the IP address can be configured via DHCP or a statically assigned one (Note: The Node IP can not change at the lifecycle of a Harvester cluster, in case the DHCP is used, the user must make sure the DHCP server always offers the same IP for the same Node. Due to a changed Node IP the related Node can not join the cluster, or even break the cluster) . Optional: Configure the DNS servers; use commas as delimiters. Configure the Virtual IP which you can use to access the cluster or join other nodes to the cluster (Note: If your IP address is configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP, VIP must be different than any Node IP) . Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default SSH user is rancher . Recommended configuring the NTP server to make sure all nodes' times are synchronized. This defaults to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote server URL. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. Confirm the installation options and Harvester will be installed to your host. The installation may take a few minutes to complete. Once the installation is complete, the host will restart, and a console UI with management URL and status will be displayed. (You can Use F12 to switch between the Harvester console and the Shell). The default URL of the web interface is https://your-virtual-ip . Users will be prompted to set the password for the default admin user at first login.","title":"ISO Installation"},{"location":"#pxeipxe-installation","text":"Harvester can also be installed automatically. Please refer to PXE Boot Install for detailed instructions and additional guidance. Note More iPXE usage examples are available at harvester/ipxe-examples .","title":"PXE/iPXE Installation"},{"location":"airgap/","text":"Air Gapped Environment \u00b6 This section describes how to use Harvester in an air gapped environment. Some use cases could be where Harvester will be installed offline, behind a firewall, or behind a proxy. The Harvester ISO image contains all the packages to make it work in an air gapped environment. Working Behind an HTTP Proxy \u00b6 In some environments, the connection to external services, from the servers or VMs, requires an HTTP(S) proxy. Configure an HTTP Proxy During Installation \u00b6 You can configure the HTTP(S) proxy during the ISO installation as shown in picture below: Configure an HTTP Proxy in Harvester Settings \u00b6 You can configure the HTTP(S) proxy in the settings page of the Harvester dashboard: Go to the settings page of the Harvester UI. Find the http-proxy setting, click \u22ee > Edit setting Enter the value(s) for http-proxy , https-proxy and no-proxy . Note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,.svc,.cluster.local","title":"Air Gapped Environment"},{"location":"airgap/#air-gapped-environment","text":"This section describes how to use Harvester in an air gapped environment. Some use cases could be where Harvester will be installed offline, behind a firewall, or behind a proxy. The Harvester ISO image contains all the packages to make it work in an air gapped environment.","title":"Air Gapped Environment"},{"location":"airgap/#working-behind-an-http-proxy","text":"In some environments, the connection to external services, from the servers or VMs, requires an HTTP(S) proxy.","title":"Working Behind an HTTP Proxy"},{"location":"airgap/#configure-an-http-proxy-during-installation","text":"You can configure the HTTP(S) proxy during the ISO installation as shown in picture below:","title":"Configure an HTTP Proxy During Installation"},{"location":"airgap/#configure-an-http-proxy-in-harvester-settings","text":"You can configure the HTTP(S) proxy in the settings page of the Harvester dashboard: Go to the settings page of the Harvester UI. Find the http-proxy setting, click \u22ee > Edit setting Enter the value(s) for http-proxy , https-proxy and no-proxy . Note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,.svc,.cluster.local","title":"Configure an HTTP Proxy in Harvester Settings"},{"location":"authentication/","text":"Authentication \u00b6 After installation, user will be prompted to set the password for the default admin user on the first-time login. Note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","title":"Authentication"},{"location":"authentication/#authentication","text":"After installation, user will be prompted to set the password for the default admin user on the first-time login. Note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","title":"Authentication"},{"location":"faq/","text":"FAQ \u00b6 This FAQ is a work in progress designed to answer the questions our users most frequently ask about Harvester. How can I ssh login to the Harvester node? \u00b6 $ ssh rancher@node-ip What is the default login username and password of the Harvester dashboard? \u00b6 username: admin password: # you will be promoted to set the default password when logging in for the first time How can I access the kubeconfig file of the Harvester cluster? \u00b6 Option 1. You can download the kubeconfig file from the support page of the Harvester dashboard. Option 2. You can get the kubeconfig file from one of the Harvester management nodes. E.g., $ sudo su $ cat /etc/rancher/rke2/rke2.yaml How to install the qemu-guest-agent of a running VM? \u00b6 # cloud-init will only be executed once, reboot it after add the cloud-init config with the following command. $ cloud-init clean --logs --reboot https://cloudinit.readthedocs.io/en/latest/topics/cli.html#clean How can I reset the administrator password? \u00b6 In case you forget the administrator password, you can reset it via the command line. SSH to one of the management node and run the following command: # switch to root and run $ kubectl -n cattle-system exec $( kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app = rancher --no-headers | head -1 | awk '{ print $1 }' ) -c rancher -- reset-password New password for default administrator ( user-xxxxx ) : <new_password>","title":"FAQ"},{"location":"faq/#faq","text":"This FAQ is a work in progress designed to answer the questions our users most frequently ask about Harvester.","title":"FAQ"},{"location":"faq/#how-can-i-ssh-login-to-the-harvester-node","text":"$ ssh rancher@node-ip","title":"How can I ssh login to the Harvester node?"},{"location":"faq/#what-is-the-default-login-username-and-password-of-the-harvester-dashboard","text":"username: admin password: # you will be promoted to set the default password when logging in for the first time","title":"What is the default login username and password of the Harvester dashboard?"},{"location":"faq/#how-can-i-access-the-kubeconfig-file-of-the-harvester-cluster","text":"Option 1. You can download the kubeconfig file from the support page of the Harvester dashboard. Option 2. You can get the kubeconfig file from one of the Harvester management nodes. E.g., $ sudo su $ cat /etc/rancher/rke2/rke2.yaml","title":"How can I access the kubeconfig file of the Harvester cluster?"},{"location":"faq/#how-to-install-the-qemu-guest-agent-of-a-running-vm","text":"# cloud-init will only be executed once, reboot it after add the cloud-init config with the following command. $ cloud-init clean --logs --reboot https://cloudinit.readthedocs.io/en/latest/topics/cli.html#clean","title":"How to install the qemu-guest-agent of a running VM?"},{"location":"faq/#how-can-i-reset-the-administrator-password","text":"In case you forget the administrator password, you can reset it via the command line. SSH to one of the management node and run the following command: # switch to root and run $ kubectl -n cattle-system exec $( kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app = rancher --no-headers | head -1 | awk '{ print $1 }' ) -c rancher -- reset-password New password for default administrator ( user-xxxxx ) : <new_password>","title":"How can I reset the administrator password?"},{"location":"upload-image/","text":"Upload Images \u00b6 Currently, there are three ways that are supported to create an image: uploading images via URL, uploading images via local files, and creating images via volumes. Upload Images via URL \u00b6 To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. Note The image name will be auto-filled using the URL address's filename. You can customize the image name at any time. Upload Images via Local File \u00b6 Currently, qcow2, raw, and ISO images are supported. Note Please do not refresh the page until the file upload is finished. Create Images via Volumes \u00b6 On the Volumes page, click Export Image . Enter image name to create image. Image labels \u00b6 You can add labels to the image, which will help identify the OS type more accurately. Additionally, you can also add any custom labels when needed. If you create an image from a URL, the UI will automatically recognize the OS type and image category based on the image name. However, if you created the image by uploading a local file, you will need to manually select the corresponding labels.","title":"Upload Images"},{"location":"upload-image/#upload-images","text":"Currently, there are three ways that are supported to create an image: uploading images via URL, uploading images via local files, and creating images via volumes.","title":"Upload Images"},{"location":"upload-image/#upload-images-via-url","text":"To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. Note The image name will be auto-filled using the URL address's filename. You can customize the image name at any time.","title":"Upload Images via URL"},{"location":"upload-image/#upload-images-via-local-file","text":"Currently, qcow2, raw, and ISO images are supported. Note Please do not refresh the page until the file upload is finished.","title":"Upload Images via Local File"},{"location":"upload-image/#create-images-via-volumes","text":"On the Volumes page, click Export Image . Enter image name to create image.","title":"Create Images via Volumes"},{"location":"upload-image/#image-labels","text":"You can add labels to the image, which will help identify the OS type more accurately. Additionally, you can also add any custom labels when needed. If you create an image from a URL, the UI will automatically recognize the OS type and image category based on the image name. However, if you created the image by uploading a local file, you will need to manually select the corresponding labels.","title":"Image labels"},{"location":"dev/dev-mode/","text":"Developer Mode Installation \u00b6 Attention Developer mode (dev mode) is intended to be used for local testing and development purposes. Requirements \u00b6 The Kubernetes node must pass the host-check If the Kubelet's RootDir is not /var/lib/kubelet , you must create a bind mount to /var/lib/kubelet as follows: KUBELET_ROOT_DIR = \"path to your kubelet root dir\" echo \" ${ KUBELET_ROOT_DIR } /var/lib/kubelet none bind 0 0\" >> /etc/fstab mkdir -p /var/lib/kubelet && mount -a Multus is installed across your cluster and a corresponding NetworkAttachmentDefinition CRD is created. The Harvester Chart already contains the Kubevirt and Longhorn Installation \u00b6 For development purpose, Harvester can be installed on a Kubernetes cluster by using the Helm CLI. Please refer to the Harvester Helm chart for more details on installing and configuring the Helm chart. Create the cattle-system namespace kubectl create ns cattle-system Add the rancher-latest helm repo helm repo add rancher-latest https://releases.rancher.com/server-charts/latest Install Rancher chart helm install rancher rancher-latest/rancher \\ --namespace cattle-system \\ --set tls = external \\ --set rancherImagePullPolicy = IfNotPresent \\ --set rancherImage = rancher/rancher \\ --set rancherImageTag = v2.6.3-harvester1 \\ --set noDefaultAdmin = false \\ --set features = \"multi-cluster-management=false\\,multi-cluster-management-agent=false\" \\ --set useBundledSystemChart = true \\ --set bootstrapPassword = admin Change the 'status.provider' of the local cluster object to \"harvester\" kubectl edit clusters.management.cattle.io local Clone the GitHub repository git clone https://github.com/harvester/harvester.git --depth = 1 Go to the Helm chart cd harvester/deploy/charts Create the harvester-system namespace kubectl create ns harvester-system Install the Harvester crd chart helm install harvester-crd ./harvester-crd --namespace harvester-system Install the Harvester chart ## In order to use the service type LoadBalancer and create a vip in control-plane nodes, we need to enable kubevip. VIP_IP = \"replace with your vip ip, such as 192.168.5.10\" VIP_NIC = \"replace with your vip interface name, such as eth0\" helm install harvester ./harvester --namespace harvester-system \\ --set harvester-node-disk-manager.enabled = true \\ --set harvester-network-controller.enabled = true \\ --set harvester-load-balancer.enabled = true \\ --set kube-vip.enabled = true \\ --set kube-vip.config.vip_interface = ${ VIP_NIC } \\ --set kube-vip.config.vip_address = ${ VIP_IP } \\ --set service.vip.enabled = true \\ --set service.vip.ip = ${ VIP_IP } ## In some Kubernetes distributions (such as kubeadm), we need to modify the kube-vip nodeSelector to match the control-plane nodes. --set kube-vip.nodeSelector. \"node-role\\.kubernetes\\.io/master\" = \"\" Expose Harvester UI ## Refer to https://kube-vip.chipzoller.dev/docs/usage/cloud-provider/. Add `cidr-cattle-system: ${VIP_IP}/32` to kubevip configMap. kubectl -n kube-system edit cm kubevip ## Change the rancher service type from ClusterIP to LoadBalancer, and then you can access Harvester UI via https://${VIP_IP}. kubectl -n cattle-system edit svc rancher DigitalOcean Test Environment \u00b6 You can create a test Kubernetes environment in Rancher using DigitalOcean as a cloud provider, which supports nested virtualization. We recommend using a 8 core, 16 GB RAM droplet, which will have nested virtualization enabled by default. This screenshot shows how to create a Rancher node template that would allow Rancher to provision such a node in DigitalOcean: For more information on how to launch DigitalOcean nodes with Rancher, refer to the Rancher documentation.","title":"Developer Mode Installation"},{"location":"dev/dev-mode/#developer-mode-installation","text":"Attention Developer mode (dev mode) is intended to be used for local testing and development purposes.","title":"Developer Mode Installation"},{"location":"dev/dev-mode/#requirements","text":"The Kubernetes node must pass the host-check If the Kubelet's RootDir is not /var/lib/kubelet , you must create a bind mount to /var/lib/kubelet as follows: KUBELET_ROOT_DIR = \"path to your kubelet root dir\" echo \" ${ KUBELET_ROOT_DIR } /var/lib/kubelet none bind 0 0\" >> /etc/fstab mkdir -p /var/lib/kubelet && mount -a Multus is installed across your cluster and a corresponding NetworkAttachmentDefinition CRD is created. The Harvester Chart already contains the Kubevirt and Longhorn","title":"Requirements"},{"location":"dev/dev-mode/#installation","text":"For development purpose, Harvester can be installed on a Kubernetes cluster by using the Helm CLI. Please refer to the Harvester Helm chart for more details on installing and configuring the Helm chart. Create the cattle-system namespace kubectl create ns cattle-system Add the rancher-latest helm repo helm repo add rancher-latest https://releases.rancher.com/server-charts/latest Install Rancher chart helm install rancher rancher-latest/rancher \\ --namespace cattle-system \\ --set tls = external \\ --set rancherImagePullPolicy = IfNotPresent \\ --set rancherImage = rancher/rancher \\ --set rancherImageTag = v2.6.3-harvester1 \\ --set noDefaultAdmin = false \\ --set features = \"multi-cluster-management=false\\,multi-cluster-management-agent=false\" \\ --set useBundledSystemChart = true \\ --set bootstrapPassword = admin Change the 'status.provider' of the local cluster object to \"harvester\" kubectl edit clusters.management.cattle.io local Clone the GitHub repository git clone https://github.com/harvester/harvester.git --depth = 1 Go to the Helm chart cd harvester/deploy/charts Create the harvester-system namespace kubectl create ns harvester-system Install the Harvester crd chart helm install harvester-crd ./harvester-crd --namespace harvester-system Install the Harvester chart ## In order to use the service type LoadBalancer and create a vip in control-plane nodes, we need to enable kubevip. VIP_IP = \"replace with your vip ip, such as 192.168.5.10\" VIP_NIC = \"replace with your vip interface name, such as eth0\" helm install harvester ./harvester --namespace harvester-system \\ --set harvester-node-disk-manager.enabled = true \\ --set harvester-network-controller.enabled = true \\ --set harvester-load-balancer.enabled = true \\ --set kube-vip.enabled = true \\ --set kube-vip.config.vip_interface = ${ VIP_NIC } \\ --set kube-vip.config.vip_address = ${ VIP_IP } \\ --set service.vip.enabled = true \\ --set service.vip.ip = ${ VIP_IP } ## In some Kubernetes distributions (such as kubeadm), we need to modify the kube-vip nodeSelector to match the control-plane nodes. --set kube-vip.nodeSelector. \"node-role\\.kubernetes\\.io/master\" = \"\" Expose Harvester UI ## Refer to https://kube-vip.chipzoller.dev/docs/usage/cloud-provider/. Add `cidr-cattle-system: ${VIP_IP}/32` to kubevip configMap. kubectl -n kube-system edit cm kubevip ## Change the rancher service type from ClusterIP to LoadBalancer, and then you can access Harvester UI via https://${VIP_IP}. kubectl -n cattle-system edit svc rancher","title":"Installation"},{"location":"dev/dev-mode/#digitalocean-test-environment","text":"You can create a test Kubernetes environment in Rancher using DigitalOcean as a cloud provider, which supports nested virtualization. We recommend using a 8 core, 16 GB RAM droplet, which will have nested virtualization enabled by default. This screenshot shows how to create a Rancher node template that would allow Rancher to provision such a node in DigitalOcean: For more information on how to launch DigitalOcean nodes with Rancher, refer to the Rancher documentation.","title":"DigitalOcean Test Environment"},{"location":"host/host/","text":"Host Management \u00b6 Users can view and manage Harvester nodes from the host page. The first node always defaults to be a management node of the cluster. When there are three or more nodes, the two other nodes that first joined are automatically promoted to management nodes to form a HA cluster. Note Because Harvester is built on top of Kubernetes and uses etcd as its database, the maximum node fault toleration is one when there are three management nodes. Node Maintenance \u00b6 For admin users, you can click Enable Maintenance Mode to evict all VMs from a node automatically. It will leverage the VM live migration feature to migrate all VMs to other nodes automatically. Note that at least two active nodes are required to use this feature. Cordoning a Node \u00b6 Cordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions. When you\u2019re done, power back on and make the node schedulable again by uncordoning it. Deleting a Node \u00b6 Deleting a node is done in two phases: Delete the node from Harvester Go to the Hosts page On the node you want to modify, click \u22ee > Delete Uninstall RKE2 from the node Login to the node as root Run rke2-uninstall.sh to delete the whole RKE2 service. Warning You will lose all data of the control plane node after deleing the RKE2 service. Note There's a known issue about node hard delete. Once resolved, the last step can be skipped. Multi-disk Management - Tech Preview \u00b6 Users can view and add multiple disks as additional data volumes from the host detail page. Go to the Hosts page. On the node you want to modify, click \u22ee > Edit Config . Select the Disks tab and click Add Disks . Select an additional raw block device to add as an additional data volume. The Force Formatted option is required if the block device has never been force-formatted. Note In order for Harvester to identify the disks, each disk needs to have a unique WWN . Otherwise, Harvester will refuse to add the disk. If your disk does not have a WWN, you can format it with the EXT4 filesystem to help Harvester recognize the disk. Note If you are testing Harvester in a QEMU environment, you'll need to use QEMU v6.0 or later. Previous versions of QEMU will always generate the same WWN for NVMe disks emulation. This will cause Harvester to not add the additional disks, as explained above.","title":"Host Management"},{"location":"host/host/#host-management","text":"Users can view and manage Harvester nodes from the host page. The first node always defaults to be a management node of the cluster. When there are three or more nodes, the two other nodes that first joined are automatically promoted to management nodes to form a HA cluster. Note Because Harvester is built on top of Kubernetes and uses etcd as its database, the maximum node fault toleration is one when there are three management nodes.","title":"Host Management"},{"location":"host/host/#node-maintenance","text":"For admin users, you can click Enable Maintenance Mode to evict all VMs from a node automatically. It will leverage the VM live migration feature to migrate all VMs to other nodes automatically. Note that at least two active nodes are required to use this feature.","title":"Node Maintenance"},{"location":"host/host/#cordoning-a-node","text":"Cordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions. When you\u2019re done, power back on and make the node schedulable again by uncordoning it.","title":"Cordoning a Node"},{"location":"host/host/#deleting-a-node","text":"Deleting a node is done in two phases: Delete the node from Harvester Go to the Hosts page On the node you want to modify, click \u22ee > Delete Uninstall RKE2 from the node Login to the node as root Run rke2-uninstall.sh to delete the whole RKE2 service. Warning You will lose all data of the control plane node after deleing the RKE2 service. Note There's a known issue about node hard delete. Once resolved, the last step can be skipped.","title":"Deleting a Node"},{"location":"host/host/#multi-disk-management-tech-preview","text":"Users can view and add multiple disks as additional data volumes from the host detail page. Go to the Hosts page. On the node you want to modify, click \u22ee > Edit Config . Select the Disks tab and click Add Disks . Select an additional raw block device to add as an additional data volume. The Force Formatted option is required if the block device has never been force-formatted. Note In order for Harvester to identify the disks, each disk needs to have a unique WWN . Otherwise, Harvester will refuse to add the disk. If your disk does not have a WWN, you can format it with the EXT4 filesystem to help Harvester recognize the disk. Note If you are testing Harvester in a QEMU environment, you'll need to use QEMU v6.0 or later. Previous versions of QEMU will always generate the same WWN for NVMe disks emulation. This will cause Harvester to not add the additional disks, as explained above.","title":"Multi-disk Management - Tech Preview"},{"location":"install/harvester-configuration/","text":"Harvester Configuration \u00b6 Configuration Example \u00b6 Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: server_url : https://someserver:443 token : TOKEN_VALUE os : ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files : - encoding : \"\" content : test content owner : root path : /etc/test.txt permissions : '0755' hostname : myhost modules : - kvm - nvme sysctls : kernel.printk : \"4 4 1 7\" kernel.kptr_restrict : \"1\" dns_nameservers : - 8.8.8.8 - 1.1.1.1 ntp_servers : - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org password : rancher environment : http_proxy : http://myserver https_proxy : http://myserver labels : foo : bar mylabel : myvalue install : mode : create networks : harvester-mgmt : interfaces : - name : ens5 hwAddr : \"B8:CA:3A:6A:64:7C\" method : dhcp force_efi : true device : /dev/vda silent : true iso_url : http://myserver/test.iso poweroff : true no_format : true debug : true tty : ttyS0 vip : 10.10.0.19 vip_hw_addr : 52:54:00:ec:0e:0b vip_mode : dhcp force_mbr : false system_settings : auto-disk-provision-paths : \"\" Configuration Reference \u00b6 Below is a reference of all configuration keys. Warning Security Risks : The configuration file contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Note Configuration Priority : When you provide a remote Harvester Configuration file during the install of Harvester, the Harvester Configuration file will not overwrite the values for the inputs you had previously filled out and selected. Priority is given to the values that you input during the guided install. For instance, if you have in your Harvester Configuration file specified os.hostname and during install you fill in the field of hostname when prompted, the value that you filled in will take priority over your Harvester Configuration's os.hostname . server_url \u00b6 Definition \u00b6 The URL of the Harvester server to join as an agent. This configuration is mandatory when the installation is in JOIN mode. It tells the Harvester installer where the main server is. Note To ensure a high availability (HA) Harvester cluster, either use the Harvester main server VIP or a domain name in server_url . Example \u00b6 server_url : https://someserver:443 install : mode : join token \u00b6 Definition \u00b6 The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match what the server has. Example \u00b6 token : myclustersecret Or a node token token : \"K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4\" os.ssh_authorized_keys \u00b6 Definition \u00b6 A list of SSH authorized keys that should be added to the default user, rancher . SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME} . This is done by downloading the keys from https://github.com/${USERNAME}.keys . Example \u00b6 os : ssh_authorized_keys : - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D\" - \"github:ibuildthecloud\" os.write_files \u00b6 A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: \"\" : content data are written in plain text. In this case, the encoding field can be also omitted. b64 , base64 : content data are base64-encoded. gz , gzip : content data are gzip-compressed. gz+base64 , gzip+base64 , gz+b64 , gzip+b64 : content data are gzip-compressed first and then base64-encoded. Example os : write_files : - encoding : b64 content : CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner : root:root path : /etc/connman/main.conf permissions : '0644' - content : | # My new /etc/sysconfig/samba file SMDBOPTIONS=\"-D\" path : /etc/sysconfig/samba - content : !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path : /bin/arch permissions : '0555' - content : | 15 * * * * root ship_logs path : /etc/crontab os.hostname \u00b6 Definition \u00b6 Set the system hostname. The installer will generate a random hostname if the user doesn't provide a value. Example \u00b6 os : hostname : myhostname os.modules \u00b6 Definition \u00b6 A list of kernel modules to be loaded on start. Example \u00b6 os : modules : - kvm - nvme os.sysctls \u00b6 Definition \u00b6 Kernel sysctl to set up on start. These are the typical configurations found in /etc/sysctl.conf . Values must be specified as strings. Example \u00b6 os : sysctls : kernel.printk : 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict : \"1\" # force the YAML parser to read as a string os.dns_nameservers \u00b6 Definition \u00b6 Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS. Example \u00b6 os : dns_nameservers : - 8.8.8.8 - 1.1.1.1 os.ntp_servers \u00b6 Definition \u00b6 Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Highly recommend to configure os.ntp_servers to avoid time synchronization issue between machines. Example \u00b6 os : ntp_servers : - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org os.password \u00b6 Definition \u00b6 The password for the default user, rancher . By default, there is no password for the rancher user. If you set a password at runtime it will be reset on the next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to change your password on a Linux system and copy the value of the second field from /etc/shadow . You can also encrypt a password using openssl passwd -1 . Example \u00b6 Encrypted: os : password : \"$1$tYtghCfK$QHa51MS6MVAcfUKuOzNKt0\" Or clear text: os : password : supersecure os.environment \u00b6 Definition \u00b6 Environment variables to be set on K3s and other processes like the boot process. Primary use of this field is to set the HTTP proxy. Example \u00b6 os : environment : http_proxy : http://myserver https_proxy : http://myserver Note This example sets the HTTP(S) proxy for foundational OS components . To set up an HTTP(S) proxy for Harvester components such as fetching external images and backup to S3 services, see Settings/http-proxy . os.labels \u00b6 Definition \u00b6 Labels to be added to this Node. install.mode \u00b6 Definition \u00b6 Harvester installation mode: create : Creating a new Harvester installation. join : Join an existing Harvester installation. Need to specify server_url . Example \u00b6 install : mode : create install.networks \u00b6 Definition \u00b6 Configure network interfaces for the host machine. Each key-value pair represents a network interface. The key name becomes the network name, and the values are configurations for each network. Valid configuration fields are: method : Method to assign an IP to this network. The following are supported: static : Manually assign an IP and gateway. dhcp : Request an IP from the DHCP server. none : Do nothing. Useful when the interface does not need an IP, such as when creating VLAN network NIC in Harvester. ip : Static IP for this network. Required if static method is chosen. subnet_mask : Subnet mask for this network. Required if static method is chosen. gateway : Gateway for this network. Required if static method is chosen. interfaces : An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name : The name of the slave interface for the bonded network. interfaces.hwAddr : The hardware MAC address of the interface. bond_options : Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlb miimon: 100 mtu : The MTU for the interface. Note A network called harvester-mgmt is mandatory to establish a valid management network . Note Harvester uses the systemd net naming scheme . Please make sure the interface name is present on the target machine before installation. Example \u00b6 install : mode : create networks : harvester-mgmt : # The management bond name. This is mandatory. interfaces : - name : ens5 hwAddr : \"B8:CA:3A:6A:64:7D\" # The hwAddr is optional method : dhcp bond_options : mode : balance-tlb miimon : 100 mtu : 1492 harvester-vlan : # The VLAN network bond name. User can then input `harvester-vlan` in the VLAN NIC setting in the GUI. interfaces : - name : ens6 hwAddr : \"B8:CA:3A:6A:64:7E\" # The hwAddr is optional method : none bond_options : mode : balance-tlb miimon : 100 bond0 : interfaces : - name : ens8 hwAddr : \"B8:CA:3A:6A:64:7F\" # The hwAddr is optional method : static ip : 10.10.18.2 subnet_mask : 255.255.255.0 gateway : 192.168.11.1 mtu : 9000 install.force_efi \u00b6 Force EFI installation even when EFI is not detected. Default: false . install.device \u00b6 The device to install the OS. install.silent \u00b6 Reserved. install.iso_url \u00b6 ISO to download and install from if booting from kernel/vmlinuz and not ISO. install.poweroff \u00b6 Shutdown the machine after installation instead of rebooting install.no_format \u00b6 Do not partition and format, assume layout exists already. install.debug \u00b6 Run the installation with additional logging and debugging enabled for the installed system. install.tty \u00b6 Definition \u00b6 The tty device used for the console. Example \u00b6 install : tty : ttyS0,115200n8 install.vip \u00b6 install.vip_mode \u00b6 install.vip_hw_addr \u00b6 Definition \u00b6 install.vip : The VIP of the Harvester management endpoint. After installation, users can access the Harvester GUI at the URL https://<VIP> . install.vip_mode dhcp : Harvester will send DHCP requests to get the VIP. The install.vip_hw_addr field needs to be provided. static : Harvester uses a static VIP. install.vip_hw_addr : The hardware address corresponding to the VIP. Users must configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp . See Management Address for more information. Example \u00b6 Configure a static VIP. install : vip : 192.168.0.100 vip_mode : static Configure a DHCP VIP. install : vip : 10.10.0.19 vip_mode : dhcp vip_hw_addr : 52:54:00:ec:0e:0b install.force_mbr \u00b6 Definition \u00b6 By default, Harvester uses GPT partitioning scheme on both UEFI and BIOS systems. However, if you face compatibility issues, the MBR partitioning scheme can be forced on BIOS systems. Note Harvester creates an additional partition for storing VM data if install.data_disk is configured to use the same storage device as the one set for install.device . When force using MBR, no additional partition will be created and VM data will be stored in a partition shared with the OS data. Example \u00b6 install : force_mbr : true install.data_disk \u00b6 Available as of v1.0.1 Definition \u00b6 Sets the default storage device to store the VM data. Default: Same storage device as the one set for install.device Example \u00b6 install : data_disk : /dev/sdb system_settings \u00b6 Definition \u00b6 You can overwrite the default Harvester system settings by configuring system_settings . See the Settings page for additional information and the list of all the options. Note Overwriting system settings only works when Harvester is installed in \"create\" mode. If you install Harvester in \"join\" mode, this setting is ignored. Installing in \"join\" mode will adopt the system settings from the existing Harvester system. Example \u00b6 The example below overwrites http-proxy and ui-source settings. The values must be a string . system_settings : http-proxy : '{\"httpProxy\": \"http://my.proxy\", \"httpsProxy\": \"https://my.proxy\", \"noProxy\": \"some.internal.svc\"}' ui-source : auto cluster_networks \u00b6 Available as of v1.0.1 Definition \u00b6 You can setup the default network in Harvester by configuring cluster_networks . Network configuration reference: vlan : Setup for VLAN network. The following fields are supported: enable : enable VLAN network settings or not. Default value: false . description : Additional information for ClusterNetworks . Default value: \"\". config : ClusterNetworks configuration to be used. Valid configuration fields are: defaultPhysicalNIC (string, required): assign a physical NIC to be external entry of VLAN network. Note To configure the cluster_networks , Harvester needs to be installed in \"create\" mode. If you install Harvester in \"join\" mode, this setting is ignored. Installing in \"join\" mode will apply the cluster_networks configuration from the existing Harvester system. Example \u00b6 The following example sets the default physical NIC name of the VLAN network: cluster_networks : vlan : enable : true description : \"some description about this cluster network\" config : defaultPhysicalNIC : ens3","title":"Harvester Configuration"},{"location":"install/harvester-configuration/#harvester-configuration","text":"","title":"Harvester Configuration"},{"location":"install/harvester-configuration/#configuration-example","text":"Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: server_url : https://someserver:443 token : TOKEN_VALUE os : ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files : - encoding : \"\" content : test content owner : root path : /etc/test.txt permissions : '0755' hostname : myhost modules : - kvm - nvme sysctls : kernel.printk : \"4 4 1 7\" kernel.kptr_restrict : \"1\" dns_nameservers : - 8.8.8.8 - 1.1.1.1 ntp_servers : - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org password : rancher environment : http_proxy : http://myserver https_proxy : http://myserver labels : foo : bar mylabel : myvalue install : mode : create networks : harvester-mgmt : interfaces : - name : ens5 hwAddr : \"B8:CA:3A:6A:64:7C\" method : dhcp force_efi : true device : /dev/vda silent : true iso_url : http://myserver/test.iso poweroff : true no_format : true debug : true tty : ttyS0 vip : 10.10.0.19 vip_hw_addr : 52:54:00:ec:0e:0b vip_mode : dhcp force_mbr : false system_settings : auto-disk-provision-paths : \"\"","title":"Configuration Example"},{"location":"install/harvester-configuration/#configuration-reference","text":"Below is a reference of all configuration keys. Warning Security Risks : The configuration file contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Note Configuration Priority : When you provide a remote Harvester Configuration file during the install of Harvester, the Harvester Configuration file will not overwrite the values for the inputs you had previously filled out and selected. Priority is given to the values that you input during the guided install. For instance, if you have in your Harvester Configuration file specified os.hostname and during install you fill in the field of hostname when prompted, the value that you filled in will take priority over your Harvester Configuration's os.hostname .","title":"Configuration Reference"},{"location":"install/harvester-configuration/#server_url","text":"","title":"server_url"},{"location":"install/harvester-configuration/#definition","text":"The URL of the Harvester server to join as an agent. This configuration is mandatory when the installation is in JOIN mode. It tells the Harvester installer where the main server is. Note To ensure a high availability (HA) Harvester cluster, either use the Harvester main server VIP or a domain name in server_url .","title":"Definition"},{"location":"install/harvester-configuration/#example","text":"server_url : https://someserver:443 install : mode : join","title":"Example"},{"location":"install/harvester-configuration/#token","text":"","title":"token"},{"location":"install/harvester-configuration/#definition_1","text":"The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match what the server has.","title":"Definition"},{"location":"install/harvester-configuration/#example_1","text":"token : myclustersecret Or a node token token : \"K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4\"","title":"Example"},{"location":"install/harvester-configuration/#osssh_authorized_keys","text":"","title":"os.ssh_authorized_keys"},{"location":"install/harvester-configuration/#definition_2","text":"A list of SSH authorized keys that should be added to the default user, rancher . SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME} . This is done by downloading the keys from https://github.com/${USERNAME}.keys .","title":"Definition"},{"location":"install/harvester-configuration/#example_2","text":"os : ssh_authorized_keys : - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D\" - \"github:ibuildthecloud\"","title":"Example"},{"location":"install/harvester-configuration/#oswrite_files","text":"A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: \"\" : content data are written in plain text. In this case, the encoding field can be also omitted. b64 , base64 : content data are base64-encoded. gz , gzip : content data are gzip-compressed. gz+base64 , gzip+base64 , gz+b64 , gzip+b64 : content data are gzip-compressed first and then base64-encoded. Example os : write_files : - encoding : b64 content : CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner : root:root path : /etc/connman/main.conf permissions : '0644' - content : | # My new /etc/sysconfig/samba file SMDBOPTIONS=\"-D\" path : /etc/sysconfig/samba - content : !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path : /bin/arch permissions : '0555' - content : | 15 * * * * root ship_logs path : /etc/crontab","title":"os.write_files"},{"location":"install/harvester-configuration/#oshostname","text":"","title":"os.hostname"},{"location":"install/harvester-configuration/#definition_3","text":"Set the system hostname. The installer will generate a random hostname if the user doesn't provide a value.","title":"Definition"},{"location":"install/harvester-configuration/#example_3","text":"os : hostname : myhostname","title":"Example"},{"location":"install/harvester-configuration/#osmodules","text":"","title":"os.modules"},{"location":"install/harvester-configuration/#definition_4","text":"A list of kernel modules to be loaded on start.","title":"Definition"},{"location":"install/harvester-configuration/#example_4","text":"os : modules : - kvm - nvme","title":"Example"},{"location":"install/harvester-configuration/#ossysctls","text":"","title":"os.sysctls"},{"location":"install/harvester-configuration/#definition_5","text":"Kernel sysctl to set up on start. These are the typical configurations found in /etc/sysctl.conf . Values must be specified as strings.","title":"Definition"},{"location":"install/harvester-configuration/#example_5","text":"os : sysctls : kernel.printk : 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict : \"1\" # force the YAML parser to read as a string","title":"Example"},{"location":"install/harvester-configuration/#osdns_nameservers","text":"","title":"os.dns_nameservers"},{"location":"install/harvester-configuration/#definition_6","text":"Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS.","title":"Definition"},{"location":"install/harvester-configuration/#example_6","text":"os : dns_nameservers : - 8.8.8.8 - 1.1.1.1","title":"Example"},{"location":"install/harvester-configuration/#osntp_servers","text":"","title":"os.ntp_servers"},{"location":"install/harvester-configuration/#definition_7","text":"Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Highly recommend to configure os.ntp_servers to avoid time synchronization issue between machines.","title":"Definition"},{"location":"install/harvester-configuration/#example_7","text":"os : ntp_servers : - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org","title":"Example"},{"location":"install/harvester-configuration/#ospassword","text":"","title":"os.password"},{"location":"install/harvester-configuration/#definition_8","text":"The password for the default user, rancher . By default, there is no password for the rancher user. If you set a password at runtime it will be reset on the next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to change your password on a Linux system and copy the value of the second field from /etc/shadow . You can also encrypt a password using openssl passwd -1 .","title":"Definition"},{"location":"install/harvester-configuration/#example_8","text":"Encrypted: os : password : \"$1$tYtghCfK$QHa51MS6MVAcfUKuOzNKt0\" Or clear text: os : password : supersecure","title":"Example"},{"location":"install/harvester-configuration/#osenvironment","text":"","title":"os.environment"},{"location":"install/harvester-configuration/#definition_9","text":"Environment variables to be set on K3s and other processes like the boot process. Primary use of this field is to set the HTTP proxy.","title":"Definition"},{"location":"install/harvester-configuration/#example_9","text":"os : environment : http_proxy : http://myserver https_proxy : http://myserver Note This example sets the HTTP(S) proxy for foundational OS components . To set up an HTTP(S) proxy for Harvester components such as fetching external images and backup to S3 services, see Settings/http-proxy .","title":"Example"},{"location":"install/harvester-configuration/#oslabels","text":"","title":"os.labels"},{"location":"install/harvester-configuration/#definition_10","text":"Labels to be added to this Node.","title":"Definition"},{"location":"install/harvester-configuration/#installmode","text":"","title":"install.mode"},{"location":"install/harvester-configuration/#definition_11","text":"Harvester installation mode: create : Creating a new Harvester installation. join : Join an existing Harvester installation. Need to specify server_url .","title":"Definition"},{"location":"install/harvester-configuration/#example_10","text":"install : mode : create","title":"Example"},{"location":"install/harvester-configuration/#installnetworks","text":"","title":"install.networks"},{"location":"install/harvester-configuration/#definition_12","text":"Configure network interfaces for the host machine. Each key-value pair represents a network interface. The key name becomes the network name, and the values are configurations for each network. Valid configuration fields are: method : Method to assign an IP to this network. The following are supported: static : Manually assign an IP and gateway. dhcp : Request an IP from the DHCP server. none : Do nothing. Useful when the interface does not need an IP, such as when creating VLAN network NIC in Harvester. ip : Static IP for this network. Required if static method is chosen. subnet_mask : Subnet mask for this network. Required if static method is chosen. gateway : Gateway for this network. Required if static method is chosen. interfaces : An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name : The name of the slave interface for the bonded network. interfaces.hwAddr : The hardware MAC address of the interface. bond_options : Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlb miimon: 100 mtu : The MTU for the interface. Note A network called harvester-mgmt is mandatory to establish a valid management network . Note Harvester uses the systemd net naming scheme . Please make sure the interface name is present on the target machine before installation.","title":"Definition"},{"location":"install/harvester-configuration/#example_11","text":"install : mode : create networks : harvester-mgmt : # The management bond name. This is mandatory. interfaces : - name : ens5 hwAddr : \"B8:CA:3A:6A:64:7D\" # The hwAddr is optional method : dhcp bond_options : mode : balance-tlb miimon : 100 mtu : 1492 harvester-vlan : # The VLAN network bond name. User can then input `harvester-vlan` in the VLAN NIC setting in the GUI. interfaces : - name : ens6 hwAddr : \"B8:CA:3A:6A:64:7E\" # The hwAddr is optional method : none bond_options : mode : balance-tlb miimon : 100 bond0 : interfaces : - name : ens8 hwAddr : \"B8:CA:3A:6A:64:7F\" # The hwAddr is optional method : static ip : 10.10.18.2 subnet_mask : 255.255.255.0 gateway : 192.168.11.1 mtu : 9000","title":"Example"},{"location":"install/harvester-configuration/#installforce_efi","text":"Force EFI installation even when EFI is not detected. Default: false .","title":"install.force_efi"},{"location":"install/harvester-configuration/#installdevice","text":"The device to install the OS.","title":"install.device"},{"location":"install/harvester-configuration/#installsilent","text":"Reserved.","title":"install.silent"},{"location":"install/harvester-configuration/#installiso_url","text":"ISO to download and install from if booting from kernel/vmlinuz and not ISO.","title":"install.iso_url"},{"location":"install/harvester-configuration/#installpoweroff","text":"Shutdown the machine after installation instead of rebooting","title":"install.poweroff"},{"location":"install/harvester-configuration/#installno_format","text":"Do not partition and format, assume layout exists already.","title":"install.no_format"},{"location":"install/harvester-configuration/#installdebug","text":"Run the installation with additional logging and debugging enabled for the installed system.","title":"install.debug"},{"location":"install/harvester-configuration/#installtty","text":"","title":"install.tty"},{"location":"install/harvester-configuration/#definition_13","text":"The tty device used for the console.","title":"Definition"},{"location":"install/harvester-configuration/#example_12","text":"install : tty : ttyS0,115200n8","title":"Example"},{"location":"install/harvester-configuration/#installvip","text":"","title":"install.vip"},{"location":"install/harvester-configuration/#installvip_mode","text":"","title":"install.vip_mode"},{"location":"install/harvester-configuration/#installvip_hw_addr","text":"","title":"install.vip_hw_addr"},{"location":"install/harvester-configuration/#definition_14","text":"install.vip : The VIP of the Harvester management endpoint. After installation, users can access the Harvester GUI at the URL https://<VIP> . install.vip_mode dhcp : Harvester will send DHCP requests to get the VIP. The install.vip_hw_addr field needs to be provided. static : Harvester uses a static VIP. install.vip_hw_addr : The hardware address corresponding to the VIP. Users must configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp . See Management Address for more information.","title":"Definition"},{"location":"install/harvester-configuration/#example_13","text":"Configure a static VIP. install : vip : 192.168.0.100 vip_mode : static Configure a DHCP VIP. install : vip : 10.10.0.19 vip_mode : dhcp vip_hw_addr : 52:54:00:ec:0e:0b","title":"Example"},{"location":"install/harvester-configuration/#installforce_mbr","text":"","title":"install.force_mbr"},{"location":"install/harvester-configuration/#definition_15","text":"By default, Harvester uses GPT partitioning scheme on both UEFI and BIOS systems. However, if you face compatibility issues, the MBR partitioning scheme can be forced on BIOS systems. Note Harvester creates an additional partition for storing VM data if install.data_disk is configured to use the same storage device as the one set for install.device . When force using MBR, no additional partition will be created and VM data will be stored in a partition shared with the OS data.","title":"Definition"},{"location":"install/harvester-configuration/#example_14","text":"install : force_mbr : true","title":"Example"},{"location":"install/harvester-configuration/#installdata_disk","text":"Available as of v1.0.1","title":"install.data_disk"},{"location":"install/harvester-configuration/#definition_16","text":"Sets the default storage device to store the VM data. Default: Same storage device as the one set for install.device","title":"Definition"},{"location":"install/harvester-configuration/#example_15","text":"install : data_disk : /dev/sdb","title":"Example"},{"location":"install/harvester-configuration/#system_settings","text":"","title":"system_settings"},{"location":"install/harvester-configuration/#definition_17","text":"You can overwrite the default Harvester system settings by configuring system_settings . See the Settings page for additional information and the list of all the options. Note Overwriting system settings only works when Harvester is installed in \"create\" mode. If you install Harvester in \"join\" mode, this setting is ignored. Installing in \"join\" mode will adopt the system settings from the existing Harvester system.","title":"Definition"},{"location":"install/harvester-configuration/#example_16","text":"The example below overwrites http-proxy and ui-source settings. The values must be a string . system_settings : http-proxy : '{\"httpProxy\": \"http://my.proxy\", \"httpsProxy\": \"https://my.proxy\", \"noProxy\": \"some.internal.svc\"}' ui-source : auto","title":"Example"},{"location":"install/harvester-configuration/#cluster_networks","text":"Available as of v1.0.1","title":"cluster_networks"},{"location":"install/harvester-configuration/#definition_18","text":"You can setup the default network in Harvester by configuring cluster_networks . Network configuration reference: vlan : Setup for VLAN network. The following fields are supported: enable : enable VLAN network settings or not. Default value: false . description : Additional information for ClusterNetworks . Default value: \"\". config : ClusterNetworks configuration to be used. Valid configuration fields are: defaultPhysicalNIC (string, required): assign a physical NIC to be external entry of VLAN network. Note To configure the cluster_networks , Harvester needs to be installed in \"create\" mode. If you install Harvester in \"join\" mode, this setting is ignored. Installing in \"join\" mode will apply the cluster_networks configuration from the existing Harvester system.","title":"Definition"},{"location":"install/harvester-configuration/#example_17","text":"The following example sets the default physical NIC name of the VLAN network: cluster_networks : vlan : enable : true description : \"some description about this cluster network\" config : defaultPhysicalNIC : ens3","title":"Example"},{"location":"install/iso-install/","text":"ISO Installation \u00b6 To get the Harvester ISO image, download it from the Github releases page. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer option. Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device on which the Harvester cluster will be installed Note: By default, Harvester uses GPT partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select MBR . Configure the hostname and select the network interface for the management network. By default, Harvester will create a bonded NIC named harvester-mgmt , and the IP address can be configured via DHCP or a statically assigned one (Note: The Node IP can not change at the lifecycle of a Harvester cluster, in case the DHCP is used, the user must make sure the DHCP server always offers the same IP for the same Node. Due to a changed Node IP the related Node can not join the cluster, or even break the cluster) . (Optional) Configure the DNS servers. Use commas as a delimiter. Configure the Virtual IP which you can use to access the cluster or join other nodes to the cluster (Note: If your IP address is configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP, VIP must be different than any Node IP) . Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default SSH user is rancher . Recommended configuring the NTP server to make sure all nodes' times are synchronized. This defaults to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote server URL. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. After confirming the installation options, Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete, it will restart the host. After the restart, the Harvester console containing the management URL and status will be displayed. You can Use F12 to switch between the Harvester console and the Shell. The default URL of the web interface is https://your-virtual-ip . You will be prompted to set the password for the default admin user when logging in for the first time.","title":"ISO Installation"},{"location":"install/iso-install/#iso-installation","text":"To get the Harvester ISO image, download it from the Github releases page. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer option. Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device on which the Harvester cluster will be installed Note: By default, Harvester uses GPT partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select MBR . Configure the hostname and select the network interface for the management network. By default, Harvester will create a bonded NIC named harvester-mgmt , and the IP address can be configured via DHCP or a statically assigned one (Note: The Node IP can not change at the lifecycle of a Harvester cluster, in case the DHCP is used, the user must make sure the DHCP server always offers the same IP for the same Node. Due to a changed Node IP the related Node can not join the cluster, or even break the cluster) . (Optional) Configure the DNS servers. Use commas as a delimiter. Configure the Virtual IP which you can use to access the cluster or join other nodes to the cluster (Note: If your IP address is configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP, VIP must be different than any Node IP) . Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default SSH user is rancher . Recommended configuring the NTP server to make sure all nodes' times are synchronized. This defaults to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote server URL. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. After confirming the installation options, Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete, it will restart the host. After the restart, the Harvester console containing the management URL and status will be displayed. You can Use F12 to switch between the Harvester console and the Shell. The default URL of the web interface is https://your-virtual-ip . You will be prompted to set the password for the default admin user when logging in for the first time.","title":"ISO Installation"},{"location":"install/management-address/","text":"Management Address \u00b6 Harvester provides a fixed virtual IP (VIP) as the management address, VIP must be different from any Node IP. You can find the management address on the console dashboard after the installation. Note If you selected the IP address to be configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP How to get the VIP MAC address \u00b6 To get the VIP MAC address, you can run the following command on the management node: $ kubectl get svc -n kube-system ingress-expose -ojsonpath = '{.metadata.annotations}' Example of output: { \"kube-vip.io/hwaddr\" : \"02:00:00:09:7f:3f\" , \"kube-vip.io/requestedIP\" : \"10.84.102.31\" } Usages \u00b6 The management address has two usages. Allows the access to the Harvester API/UI via HTTPS protocol. Is the address the other nodes use to join the cluster.","title":"Management Address"},{"location":"install/management-address/#management-address","text":"Harvester provides a fixed virtual IP (VIP) as the management address, VIP must be different from any Node IP. You can find the management address on the console dashboard after the installation. Note If you selected the IP address to be configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP","title":"Management Address"},{"location":"install/management-address/#how-to-get-the-vip-mac-address","text":"To get the VIP MAC address, you can run the following command on the management node: $ kubectl get svc -n kube-system ingress-expose -ojsonpath = '{.metadata.annotations}' Example of output: { \"kube-vip.io/hwaddr\" : \"02:00:00:09:7f:3f\" , \"kube-vip.io/requestedIP\" : \"10.84.102.31\" }","title":"How to get the VIP MAC address"},{"location":"install/management-address/#usages","text":"The management address has two usages. Allows the access to the Harvester API/UI via HTTPS protocol. Is the address the other nodes use to join the cluster.","title":"Usages"},{"location":"install/pxe-boot-install/","text":"PXE Boot Installation \u00b6 Starting from version 0.2.0 , Harvester can be installed automatically. This document provides an example to do an automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If the iPXE firmware is not available for your NIC card, the iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples . Prerequisite \u00b6 Important Nodes need to have at least 8G of RAM because the installer loads the full ISO file into tmpfs. Preparing HTTP Servers \u00b6 An HTTP server is required to serve boot files. Let's assume the NGINX HTTP server's IP is 10.100.0.10 , and it serves the /usr/share/nginx/html/ directory with the path http://10.100.0.10/ . Preparing Boot Files \u00b6 Download the required files from the Harvester releases page . The ISO: harvester-<version>-amd64.iso The kernel: harvester-<version>-vmlinuz-amd64 The initrd: harvester-<version>-initrd-amd64 The rootfs squashfs image: harvester-<version>-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. For example: sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/ Preparing iPXE Boot Scripts \u00b6 When performing an automatic installation, there are two modes: CREATE : we are installing a node to construct an initial Harvester cluster. JOIN : we are installing a node to join an existing Harvester cluster. CREATE Mode \u00b6 Warning Security Risks : The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml token : token os : hostname : node1 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin password : rancher ntp_servers : - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org install : mode : create networks : harvester-mgmt : # (Mandatory) The management bond name. interfaces : - name : ens5 method : dhcp bond_options : mode : balance-tlb miimon : 100 harvester-vlan : # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces : # host to host, consider creating a bonding device. Users can then select - name : ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method : none bond_options : mode : balance-tlb miimon : 100 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso vip : 10.100.0.99 # The VIP to access the Harvester GUI. Make sure the IP is free to use. vip_mode : static # Or dhcp, check configuration file for more information. For machines that needs to be installed using CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create . Note If you have multiple network interfaces, you can leverage dracut's ip= parameter to specify the booting interface and any other network configurations that dracut supports (e.g., ip=eth1:dhcp ) See man dracut.cmdline for more information. Use ip= parameter to designate the booting interface only, as we only support one single ip= parameter . JOIN Mode \u00b6 Warning Security Risks : The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml server_url : https://10.100.0.99:443 # Should be the VIP set up in \"CREATE\" config token : token os : hostname : node2 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin dns_nameservers : - 1.1.1.1 - 8.8.8.8 password : rancher install : mode : join networks : harvester-mgmt : # (Mandatory) The management bond name. interfaces : - name : ens5 method : dhcp bond_options : mode : balance-tlb miimon : 10 harvester-vlan : # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces : # host to host, consider creating a bonding device. Users can then select - name : ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method : none bond_options : mode : balance-tlb miimon : 100 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-<version>-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join . DHCP Server Configuration \u00b6 The following is an example of how to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16 ; subnet 10 .100.0.0 netmask 255 .255.255.0 { option routers 10 .100.0.10 ; option domain-name-servers 192 .168.2.1 ; range 10 .100.0.100 10 .100.0.253 ; } group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } group { # join group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-join-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-join\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node2 { hardware ethernet 52 :54:00:69:d5:92 ; } } The config file declares a subnet and two groups. The first group is for hosts to boot using CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client it offers the iPXE image according to the client architecture. Please prepare those images and a TFTP server first. Harvester Configuration \u00b6 For more information about Harvester configuration, please refer to the Harvester configuration page. Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, users can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file. UEFI HTTP Boot support \u00b6 UEFI firmware supports loading a boot image from an HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform an automatic installation. Serve the iPXE Program \u00b6 Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make sure ipxe.efi can be downloaded from the HTTP server. For example: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally. DHCP Server Configuration \u00b6 If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provide the iPXE program URL when it sees such a request. The following is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } elsif substring ( option vendor-class-identifier, 0 , 10 ) = \"HTTPClient\" { # UEFI HTTP Boot option vendor-class-identifier \"HTTPClient\" ; filename \"http://10.100.0.10/harvester/ipxe.efi\" ; } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from the URL http://10.100.0.10/harvester/ipxe-create-efi . The iPXE Script for UEFI Boot \u00b6 It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. The following is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-<version>-vmlinuz initrd=harvester-<version>-initrd ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot The parameter initrd=harvester-<version>-initrd is required. Useful Kernel Parameters \u00b6 Besides the Harvester configuration, you can also specify other kernel parameters that are useful in different scenarios. See also dracut.cmdline(7) . ip=dhcp \u00b6 If you have multiple network interfaces, you could add the ip=dhcp parameter to get IP from the DHCP server from all interfaces. rd.net.dhcp.retry=<cnt> \u00b6 Failing to get IP from the DHCP server would cause iPXE booting to fail. You can add parameter rd.net.dhcp.retry=<cnt> to retry DHCP request for <cnt> times.","title":"PXE Boot Installation"},{"location":"install/pxe-boot-install/#pxe-boot-installation","text":"Starting from version 0.2.0 , Harvester can be installed automatically. This document provides an example to do an automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If the iPXE firmware is not available for your NIC card, the iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples .","title":"PXE Boot Installation"},{"location":"install/pxe-boot-install/#prerequisite","text":"Important Nodes need to have at least 8G of RAM because the installer loads the full ISO file into tmpfs.","title":"Prerequisite"},{"location":"install/pxe-boot-install/#preparing-http-servers","text":"An HTTP server is required to serve boot files. Let's assume the NGINX HTTP server's IP is 10.100.0.10 , and it serves the /usr/share/nginx/html/ directory with the path http://10.100.0.10/ .","title":"Preparing HTTP Servers"},{"location":"install/pxe-boot-install/#preparing-boot-files","text":"Download the required files from the Harvester releases page . The ISO: harvester-<version>-amd64.iso The kernel: harvester-<version>-vmlinuz-amd64 The initrd: harvester-<version>-initrd-amd64 The rootfs squashfs image: harvester-<version>-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. For example: sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/","title":"Preparing Boot Files"},{"location":"install/pxe-boot-install/#preparing-ipxe-boot-scripts","text":"When performing an automatic installation, there are two modes: CREATE : we are installing a node to construct an initial Harvester cluster. JOIN : we are installing a node to join an existing Harvester cluster.","title":"Preparing iPXE Boot Scripts"},{"location":"install/pxe-boot-install/#create-mode","text":"Warning Security Risks : The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml token : token os : hostname : node1 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin password : rancher ntp_servers : - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org install : mode : create networks : harvester-mgmt : # (Mandatory) The management bond name. interfaces : - name : ens5 method : dhcp bond_options : mode : balance-tlb miimon : 100 harvester-vlan : # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces : # host to host, consider creating a bonding device. Users can then select - name : ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method : none bond_options : mode : balance-tlb miimon : 100 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso vip : 10.100.0.99 # The VIP to access the Harvester GUI. Make sure the IP is free to use. vip_mode : static # Or dhcp, check configuration file for more information. For machines that needs to be installed using CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create . Note If you have multiple network interfaces, you can leverage dracut's ip= parameter to specify the booting interface and any other network configurations that dracut supports (e.g., ip=eth1:dhcp ) See man dracut.cmdline for more information. Use ip= parameter to designate the booting interface only, as we only support one single ip= parameter .","title":"CREATE Mode"},{"location":"install/pxe-boot-install/#join-mode","text":"Warning Security Risks : The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml server_url : https://10.100.0.99:443 # Should be the VIP set up in \"CREATE\" config token : token os : hostname : node2 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin dns_nameservers : - 1.1.1.1 - 8.8.8.8 password : rancher install : mode : join networks : harvester-mgmt : # (Mandatory) The management bond name. interfaces : - name : ens5 method : dhcp bond_options : mode : balance-tlb miimon : 10 harvester-vlan : # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces : # host to host, consider creating a bonding device. Users can then select - name : ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method : none bond_options : mode : balance-tlb miimon : 100 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-<version>-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join .","title":"JOIN Mode"},{"location":"install/pxe-boot-install/#dhcp-server-configuration","text":"The following is an example of how to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16 ; subnet 10 .100.0.0 netmask 255 .255.255.0 { option routers 10 .100.0.10 ; option domain-name-servers 192 .168.2.1 ; range 10 .100.0.100 10 .100.0.253 ; } group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } group { # join group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-join-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-join\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node2 { hardware ethernet 52 :54:00:69:d5:92 ; } } The config file declares a subnet and two groups. The first group is for hosts to boot using CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client it offers the iPXE image according to the client architecture. Please prepare those images and a TFTP server first.","title":"DHCP Server Configuration"},{"location":"install/pxe-boot-install/#harvester-configuration","text":"For more information about Harvester configuration, please refer to the Harvester configuration page. Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, users can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file.","title":"Harvester Configuration"},{"location":"install/pxe-boot-install/#uefi-http-boot-support","text":"UEFI firmware supports loading a boot image from an HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform an automatic installation.","title":"UEFI HTTP Boot support"},{"location":"install/pxe-boot-install/#serve-the-ipxe-program","text":"Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make sure ipxe.efi can be downloaded from the HTTP server. For example: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally.","title":"Serve the iPXE Program"},{"location":"install/pxe-boot-install/#dhcp-server-configuration_1","text":"If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provide the iPXE program URL when it sees such a request. The following is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } elsif substring ( option vendor-class-identifier, 0 , 10 ) = \"HTTPClient\" { # UEFI HTTP Boot option vendor-class-identifier \"HTTPClient\" ; filename \"http://10.100.0.10/harvester/ipxe.efi\" ; } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from the URL http://10.100.0.10/harvester/ipxe-create-efi .","title":"DHCP Server Configuration"},{"location":"install/pxe-boot-install/#the-ipxe-script-for-uefi-boot","text":"It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. The following is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-<version>-vmlinuz initrd=harvester-<version>-initrd ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot The parameter initrd=harvester-<version>-initrd is required.","title":"The iPXE Script for UEFI Boot"},{"location":"install/pxe-boot-install/#useful-kernel-parameters","text":"Besides the Harvester configuration, you can also specify other kernel parameters that are useful in different scenarios. See also dracut.cmdline(7) .","title":"Useful Kernel Parameters"},{"location":"install/pxe-boot-install/#ipdhcp","text":"If you have multiple network interfaces, you could add the ip=dhcp parameter to get IP from the DHCP server from all interfaces.","title":"ip=dhcp"},{"location":"install/pxe-boot-install/#rdnetdhcpretrycnt","text":"Failing to get IP from the DHCP server would cause iPXE booting to fail. You can add parameter rd.net.dhcp.retry=<cnt> to retry DHCP request for <cnt> times.","title":"rd.net.dhcp.retry=&lt;cnt&gt;"},{"location":"install/requirements/","text":"Requirements \u00b6 As an HCI solution on bare metal servers, Harvester has some minimum requirements as outlined below. Hardware Requirements \u00b6 To get the Harvester server up and running the following minimum hardware is required: Type Requirements CPU x86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum; 16-core or above preferred Memory 32 GB minimum, 64 GB or above preferred Disk Capacity 140 GB minimum for testing, 500 GB or above preferred for production Disk Performance 5,000+ random IOPS per disk(SSD/NVMe). Management nodes (first 3 nodes) must be fast enough for Etcd Network Card 1 Gbps Ethernet minimum for testing, 10Gbps Ethernet recommended for production Network Switch Trunking of ports required for VLAN support We recommend server-class hardware for best results. Laptops and nested virtualization are not officially supported. Networking \u00b6 Harvester Hosts Inbound Rules \u00b6 Protocol Port Source Description TCP 2379 Harvester management nodes Etcd client port TCP 2381 Harvester management nodes Etcd health checks TCP 2380 Harvester management nodes Etcd peer port TCP 10010 Harvester management and compute nodes Containerd TCP 6443 Harvester management nodes Kubernetes API TCP 9345 Harvester management nodes Kubernetes API TCP 10252 Harvester management nodes Kube-controller-manager health checks TCP 10257 Harvester management nodes Kube-controller-manager secure port TCP 10251 Harvester management nodes Kube-scheduler health checks TCP 10259 Harvester management nodes Kube-scheduler secure port TCP 10250 Harvester management and compute nodes Kubelet TCP 10256 Harvester management and compute nodes Kube-proxy health checks TCP 10258 Harvester management nodes Cloud-controller-manager TCP 9091 Harvester management and compute nodes Canal calico-node felix TCP 9099 Harvester management and compute nodes Canal CNI health checks UDP 8472 Harvester management and compute nodes Canal CNI with VxLAN TCP 2112 Harvester management nodes Kube-vip TCP 6444 Harvester management and compute nodes RKE2 agent TCP 6060 Harvester management and compute nodes Node-disk-manager TCP 10246/10247/10248/10249 Harvester management and compute nodes Nginx worker process TCP 8181 Harvester management and compute nodes Nginx-ingress-controller TCP 8444 Harvester management and compute nodes Nginx-ingress-controller TCP 10245 Harvester management and compute nodes Nginx-ingress-controller TCP 80 Harvester management and compute nodes Nginx TCP 9796 Harvester management and compute nodes Node-exporter TCP 30000-32767 Harvester management and compute nodes NodePort port range TCP 22 Harvester management and compute nodes sshd UDP 68 Harvester management and compute nodes Wicked TCP 3260 Harvester management and compute nodes iscsid Typically, all outbound traffic will be allowed. Integrating Harvester with Rancher \u00b6 If you want to integrate Harvester with Rancher , you need to make sure, that all Harvester nodes can connect to TCP port 443 of the Rancher load balancer. The VMs of Kubernetes clusters, that are provisioned from Rancher into Harvester, also need to be able to connect to TCP port 443 of the Rancher load balancer. Otherwise the cluster won't be manageable by Rancher. For more information see also Rancher Architecture . Guest clusters \u00b6 As for the port requirements for the guest clusters deployed inside Harvester virtual machines, refer to the following links. K3s: https://rancher.com/docs/k3s/latest/en/installation/installation-requirements/#networking RKE: https://rancher.com/docs/rke/latest/en/os/#ports RKE2: https://docs.rke2.io/install/requirements/#networking","title":"Requirements"},{"location":"install/requirements/#requirements","text":"As an HCI solution on bare metal servers, Harvester has some minimum requirements as outlined below.","title":"Requirements"},{"location":"install/requirements/#hardware-requirements","text":"To get the Harvester server up and running the following minimum hardware is required: Type Requirements CPU x86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum; 16-core or above preferred Memory 32 GB minimum, 64 GB or above preferred Disk Capacity 140 GB minimum for testing, 500 GB or above preferred for production Disk Performance 5,000+ random IOPS per disk(SSD/NVMe). Management nodes (first 3 nodes) must be fast enough for Etcd Network Card 1 Gbps Ethernet minimum for testing, 10Gbps Ethernet recommended for production Network Switch Trunking of ports required for VLAN support We recommend server-class hardware for best results. Laptops and nested virtualization are not officially supported.","title":"Hardware Requirements"},{"location":"install/requirements/#networking","text":"","title":"Networking"},{"location":"install/requirements/#harvester-hosts-inbound-rules","text":"Protocol Port Source Description TCP 2379 Harvester management nodes Etcd client port TCP 2381 Harvester management nodes Etcd health checks TCP 2380 Harvester management nodes Etcd peer port TCP 10010 Harvester management and compute nodes Containerd TCP 6443 Harvester management nodes Kubernetes API TCP 9345 Harvester management nodes Kubernetes API TCP 10252 Harvester management nodes Kube-controller-manager health checks TCP 10257 Harvester management nodes Kube-controller-manager secure port TCP 10251 Harvester management nodes Kube-scheduler health checks TCP 10259 Harvester management nodes Kube-scheduler secure port TCP 10250 Harvester management and compute nodes Kubelet TCP 10256 Harvester management and compute nodes Kube-proxy health checks TCP 10258 Harvester management nodes Cloud-controller-manager TCP 9091 Harvester management and compute nodes Canal calico-node felix TCP 9099 Harvester management and compute nodes Canal CNI health checks UDP 8472 Harvester management and compute nodes Canal CNI with VxLAN TCP 2112 Harvester management nodes Kube-vip TCP 6444 Harvester management and compute nodes RKE2 agent TCP 6060 Harvester management and compute nodes Node-disk-manager TCP 10246/10247/10248/10249 Harvester management and compute nodes Nginx worker process TCP 8181 Harvester management and compute nodes Nginx-ingress-controller TCP 8444 Harvester management and compute nodes Nginx-ingress-controller TCP 10245 Harvester management and compute nodes Nginx-ingress-controller TCP 80 Harvester management and compute nodes Nginx TCP 9796 Harvester management and compute nodes Node-exporter TCP 30000-32767 Harvester management and compute nodes NodePort port range TCP 22 Harvester management and compute nodes sshd UDP 68 Harvester management and compute nodes Wicked TCP 3260 Harvester management and compute nodes iscsid Typically, all outbound traffic will be allowed.","title":"Harvester Hosts Inbound Rules"},{"location":"install/requirements/#integrating-harvester-with-rancher","text":"If you want to integrate Harvester with Rancher , you need to make sure, that all Harvester nodes can connect to TCP port 443 of the Rancher load balancer. The VMs of Kubernetes clusters, that are provisioned from Rancher into Harvester, also need to be able to connect to TCP port 443 of the Rancher load balancer. Otherwise the cluster won't be manageable by Rancher. For more information see also Rancher Architecture .","title":"Integrating Harvester with Rancher"},{"location":"install/requirements/#guest-clusters","text":"As for the port requirements for the guest clusters deployed inside Harvester virtual machines, refer to the following links. K3s: https://rancher.com/docs/k3s/latest/en/installation/installation-requirements/#networking RKE: https://rancher.com/docs/rke/latest/en/os/#ports RKE2: https://docs.rke2.io/install/requirements/#networking","title":"Guest clusters"},{"location":"install/usb-install/","text":"USB Installation \u00b6 Create a bootable USB flash drive \u00b6 There are a couple of ways to create a USB installation flash drive. balenaEtcher \u00b6 balenaEtcher supports writing images to USB flash drives. It has a GUI and is easy to use. Select the Harvester installation ISO and the target USB device to create a USB installation flash drive. dd command \u00b6 On Linux or other platforms that have the dd command, users can use dd to create a USB installation flash drive. Warning Make sure you choose the correct device. The process erases data on the selected device. # sudo dd if=<path_to_iso> of=<path_to_usb_device> bs=64k Common issues \u00b6 When booting from a USB installation flash drive, a GRUB _ text is displayed, but nothing happens \u00b6 If you are using the UEFI mode, try to boot from the UEFI boot partition on the USB device rather than the USB device itself. e.g., Select the UEFI: USB disk 3.0 PMAP, Partition 1 to boot. Note the representation varies from system to system. Graphics issue \u00b6 Firmwares of some graphic cards are not shipped in v0.3.0 . You can press e to edit the GRUB menu entry and append nomodeset to the boot parameters. Press Ctrl + x to boot. Other issues \u00b6 Harvester installer is not displayed If a USB flash driver boots, but you can't see the harvester installer. You may try out the following workarounds: Plug the USB flash drive into a USB 2.0 slot. For version v0.3.0 or above, try to remove the console=ttyS0 parameter when booting. You can press e to edit the GRUB menu entry and remove the console=ttyS0 parameter.","title":"USB Installation"},{"location":"install/usb-install/#usb-installation","text":"","title":"USB Installation"},{"location":"install/usb-install/#create-a-bootable-usb-flash-drive","text":"There are a couple of ways to create a USB installation flash drive.","title":"Create a bootable USB flash drive"},{"location":"install/usb-install/#balenaetcher","text":"balenaEtcher supports writing images to USB flash drives. It has a GUI and is easy to use. Select the Harvester installation ISO and the target USB device to create a USB installation flash drive.","title":"balenaEtcher"},{"location":"install/usb-install/#dd-command","text":"On Linux or other platforms that have the dd command, users can use dd to create a USB installation flash drive. Warning Make sure you choose the correct device. The process erases data on the selected device. # sudo dd if=<path_to_iso> of=<path_to_usb_device> bs=64k","title":"dd command"},{"location":"install/usb-install/#common-issues","text":"","title":"Common issues"},{"location":"install/usb-install/#when-booting-from-a-usb-installation-flash-drive-a-grub-_-text-is-displayed-but-nothing-happens","text":"If you are using the UEFI mode, try to boot from the UEFI boot partition on the USB device rather than the USB device itself. e.g., Select the UEFI: USB disk 3.0 PMAP, Partition 1 to boot. Note the representation varies from system to system.","title":"When booting from a USB installation flash drive, a GRUB _ text is displayed, but nothing happens"},{"location":"install/usb-install/#graphics-issue","text":"Firmwares of some graphic cards are not shipped in v0.3.0 . You can press e to edit the GRUB menu entry and append nomodeset to the boot parameters. Press Ctrl + x to boot.","title":"Graphics issue"},{"location":"install/usb-install/#other-issues","text":"Harvester installer is not displayed If a USB flash driver boots, but you can't see the harvester installer. You may try out the following workarounds: Plug the USB flash drive into a USB 2.0 slot. For version v0.3.0 or above, try to remove the console=ttyS0 parameter when booting. You can press e to edit the GRUB menu entry and remove the console=ttyS0 parameter.","title":"Other issues"},{"location":"monitoring/monitoring/","text":"Monitoring \u00b6 Available as of v0.3.0 Dashboard Metrics \u00b6 Harvester has provided a built-in monitoring integration using Prometheus . Monitoring is automatically enabled during the Harvester installations. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboards on the Grafana UI. Note Only admin users are able to view the cluster dashboard metrics. VM Detail Metrics \u00b6 For VMs, you can view VM metrics by clicking on the VM details page > VM Metrics . Note The current Memory Usage is calculated based on (1 - free/total) * 100% , not (used/total) * 100% . For example, in a Linux OS, the free -h command outputs the current memory statistics as follows $ free -h total used free shared buff/cache available Mem: 7.7Gi 166Mi 4.6Gi 1.0Mi 2.9Gi 7.2Gi Swap: 0B 0B 0B The corresponding Memory Usage is (1 - 4.6/7.7) * 100% , roughly 40% . How to Configure Monitoring Settings \u00b6 Available as of v1.0.1 Monitoring has several components that help to collect and aggregate metric data from all Nodes/Pods/VMs. The resources required for monitoring depend on your workloads and hardware resources. Harvester sets defaults based on general use cases, and you can change them accordingly. Currently, Resources Settings can be configured for the following components: Prometheus Prometheus Node Exporter( UI configurable as of v1.0.2 ) From WebUI \u00b6 In the Advanced Settings page, you can view and change the resources settings as follows: Navigate to settings page, find harvester-monitoring . Click Show harvester-monitoring to view the current values. Click \u22ee > Edit Setting to set a new value. Click Save and the Monitoring resource will be restarted within a few seconds. Please be aware that the reboot can take some time to reload previous data. The most frequently used option is the memory setting: The Requested Memory is the minimum memory required by the Monitoring resource. The recommended value is about 5% to 10% of the system memory of one single management node. A value less than 500Mi will be denied. The Memory Limit is the maximum memory that can be allocated to a Monitoring resource. The recommended value is about 30% of the system's memory for one single management node. When the Monitoring reaches this threshold, it will automatically restart. Depending on the available hardware resources and system loads, you may change the above settings accordingly. Note If you have multiple management nodes with different hardware resources, please set the value of Prometheus based on the smaller one. Attention When an increasing number of VMs get deployed on one node, the prometheus-node-exporter pod might get killed due to OOM(out of memory). In that case, you should increase the value of limits.memory . From CLI \u00b6 To update those values, you can also use the CLI command with: $kubectl edit managedchart rancher-monitoring -n fleet-local . For Harvester version >= v1.0.1 , the related path and default value are: # Prometheus configs spec.values.prometheus.prometheusSpec.resources.limits.cpu : 1000m spec.values.prometheus.prometheusSpec.resources.limits.memory : 2500Mi spec.values.prometheus.prometheusSpec.resources.requests.cpu : 750m spec.values.prometheus.prometheusSpec.resources.requests.memory : 1750Mi --- # node exporter configs spec.values.prometheus-node-exporter.resources.limits.cpu : 200m spec.values.prometheus-node-exporter.resources.limits.memory : 180Mi spec.values.prometheus-node-exporter.resources.requests.cpu : 100m spec.values.prometheus-node-exporter.resources.requests.memory : 30Mi For versions <= v1.0.0 , the related path and default value are not specified in the managedchart rancher-monitoring , you need to add them accordingly. Troubleshooting \u00b6 For Monitoring support and troubleshooting, please refer to the troubleshooting page .","title":"Monitoring"},{"location":"monitoring/monitoring/#monitoring","text":"Available as of v0.3.0","title":"Monitoring"},{"location":"monitoring/monitoring/#dashboard-metrics","text":"Harvester has provided a built-in monitoring integration using Prometheus . Monitoring is automatically enabled during the Harvester installations. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboards on the Grafana UI. Note Only admin users are able to view the cluster dashboard metrics.","title":"Dashboard Metrics"},{"location":"monitoring/monitoring/#vm-detail-metrics","text":"For VMs, you can view VM metrics by clicking on the VM details page > VM Metrics . Note The current Memory Usage is calculated based on (1 - free/total) * 100% , not (used/total) * 100% . For example, in a Linux OS, the free -h command outputs the current memory statistics as follows $ free -h total used free shared buff/cache available Mem: 7.7Gi 166Mi 4.6Gi 1.0Mi 2.9Gi 7.2Gi Swap: 0B 0B 0B The corresponding Memory Usage is (1 - 4.6/7.7) * 100% , roughly 40% .","title":"VM Detail Metrics"},{"location":"monitoring/monitoring/#how-to-configure-monitoring-settings","text":"Available as of v1.0.1 Monitoring has several components that help to collect and aggregate metric data from all Nodes/Pods/VMs. The resources required for monitoring depend on your workloads and hardware resources. Harvester sets defaults based on general use cases, and you can change them accordingly. Currently, Resources Settings can be configured for the following components: Prometheus Prometheus Node Exporter( UI configurable as of v1.0.2 )","title":"How to Configure Monitoring Settings"},{"location":"monitoring/monitoring/#from-webui","text":"In the Advanced Settings page, you can view and change the resources settings as follows: Navigate to settings page, find harvester-monitoring . Click Show harvester-monitoring to view the current values. Click \u22ee > Edit Setting to set a new value. Click Save and the Monitoring resource will be restarted within a few seconds. Please be aware that the reboot can take some time to reload previous data. The most frequently used option is the memory setting: The Requested Memory is the minimum memory required by the Monitoring resource. The recommended value is about 5% to 10% of the system memory of one single management node. A value less than 500Mi will be denied. The Memory Limit is the maximum memory that can be allocated to a Monitoring resource. The recommended value is about 30% of the system's memory for one single management node. When the Monitoring reaches this threshold, it will automatically restart. Depending on the available hardware resources and system loads, you may change the above settings accordingly. Note If you have multiple management nodes with different hardware resources, please set the value of Prometheus based on the smaller one. Attention When an increasing number of VMs get deployed on one node, the prometheus-node-exporter pod might get killed due to OOM(out of memory). In that case, you should increase the value of limits.memory .","title":"From WebUI"},{"location":"monitoring/monitoring/#from-cli","text":"To update those values, you can also use the CLI command with: $kubectl edit managedchart rancher-monitoring -n fleet-local . For Harvester version >= v1.0.1 , the related path and default value are: # Prometheus configs spec.values.prometheus.prometheusSpec.resources.limits.cpu : 1000m spec.values.prometheus.prometheusSpec.resources.limits.memory : 2500Mi spec.values.prometheus.prometheusSpec.resources.requests.cpu : 750m spec.values.prometheus.prometheusSpec.resources.requests.memory : 1750Mi --- # node exporter configs spec.values.prometheus-node-exporter.resources.limits.cpu : 200m spec.values.prometheus-node-exporter.resources.limits.memory : 180Mi spec.values.prometheus-node-exporter.resources.requests.cpu : 100m spec.values.prometheus-node-exporter.resources.requests.memory : 30Mi For versions <= v1.0.0 , the related path and default value are not specified in the managedchart rancher-monitoring , you need to add them accordingly.","title":"From CLI"},{"location":"monitoring/monitoring/#troubleshooting","text":"For Monitoring support and troubleshooting, please refer to the troubleshooting page .","title":"Troubleshooting"},{"location":"networking/harvester-network/","text":"Harvester Network \u00b6 Harvester is built on top of Kubernetes and leverages its built-in CNI mechanism to provide the interface between network providers and its VM networks. We have implemented the Harvester VLAN network based on the bridge CNI to provide a pure L2-mode network, that would bridge your VMs to the host network interface and can be connected using the physical switch for both internal and external network communication. Moreover, the Harvester UI integrates the harvester-network-controller to provide user-friendly VLAN network configurations, e.g., to create and manage VLAN networks or to add a VLAN network to the VM. Currently, Harvester supports two types of networks: Management Network VLAN Network Management Network \u00b6 Harvester uses canal as its default management network. It is a built-in network that can be used directly from the cluster. By default, the management network IP of a VM can only be accessed within the cluster nodes, and the management network IP will change after the VM reboot. This is non-typical behaviour that needs to be taken note of since VM IPs are expected to remain unchanged after a reboot. However, users can leverage the Kubernetes service object to create a stable IP for your VMs with the management network. VLAN Network \u00b6 The Harvester network-controller leverages the multus and bridge CNI plugins to implement its customized L2 bridge VLAN network. It helps to connect your VMs to the host network interface and can be accessed from internal and external networks using the physical switch. The below diagram illustrates how the VLAN network works in Harvester. The Harvester network-controller creates a bridge for each node and a pair of veth for each VM to implement its VLAN network. The bridge acts as a switch to forward the network traffic from or to VMs and the veth pair is like the connected ports between VMs and the switch. VMs within the same VLAN can communicate with each other, while the VMs from different VLANs can't. The external switch ports connected to the hosts or other devices (such as the DHCP server) should be set as trunk or hybrid type and permit the specified VLANs. Users can use VLAN with PVID (default 1) to communicate with any normal untagged traffic. Enabling Default VLAN Network \u00b6 You can enable VLAN network via Settings > vlan . Select enabled and you will be able to select one network interface from the nodes as the default VLAN NIC config. For better network performances and isolation, we recommend to choose different network interfaces for the VLAN and the one used for the management network (i.e., harvester-mgmt ). Note When selecting the network interface, the value in parentheses represents the distribution percentage of the network interface on all hosts. If a network interface with a value less than 100% is selected, the network interface needs to be manually specified on the host where the VLAN network configuration fails. Modifying the default VLAN network setting will not update the existing configured host network. Harvester VLAN network supports bond interfaces. Currently it can only be created automatically via PXE Boot Configuration . You may also login to the node and create it manually. You can also customize each node's VLAN network via the Hosts > Network tab. Create a VLAN Network \u00b6 A new VLAN network can be created via the Advanced > Networks page and clicking the Create button. Specify the name and VLAN ID that you want to create for the VLAN network (You can specify the same vlan ID on different namespaces of Rancher multi-tenancy support) . Configure a route in order to allow the hosts to connect to the VLAN network using IPv4 addresses. The CIDR and gateway of the VLAN network are mandatory parameters for the route configuration. You can configure the route by choosing one of the following options: auto(DHCP) mode: the Harvester network controller will get the CIDR and gateway values from the DHCP server using the DHCP protocol. Optionally, you can specify the DHCP server address. manual mode: You need to specify the CIDR and gateway values manually. Create a VM with VLAN Network \u00b6 Users can now create a new VM using the above configured VLAN network, Click the Create button on the Virtual Machines page. Specify the required parameters and click the Networks tab. Either configure the default network to be a VLAN network or select an additional network to add. Note Only the first NIC will be enabled by default. Users can either choose to use a management network or a VLAN network. You need to be careful to configure virtual machines with multiple NICs to avoid connectivity issues. You can refer to the knowledge base for more details. You will need to select the Install guest agent option in the Advanced Options tab to get the VLAN network IP address from the Harvester UI. You can choose to add one or multiple network interface cards. The additional network interface cards can be enabled by default via the cloud-init network data setting. e.g., version : 1 config : - type : physical name : enp1s0 # name is varies upon OS image subnets : - type : dhcp - type : physical name : enp2s0 subnets : - type : static address : 10.0.0.100/24 # IP address is varies upon your environment Harvester is fully compatible with the cloud-init network configs . You can refer to the documentation for more details. Note If you add additional NICs after the VM has started, you will need to manually configure IPs for the additional NICs. Configure DHCP servers on Networks \u00b6 By default, the Harvester VLAN network would expect your router to provide a DHCP server that VMs can request and assign IP addresses automatically. If you are running Harvester in a virtual environment that does not contain a DHCP server, you may consider deploying a DHCP server manually on a node or using a containerized method, e.g, like #947 .","title":"Harvester Network"},{"location":"networking/harvester-network/#harvester-network","text":"Harvester is built on top of Kubernetes and leverages its built-in CNI mechanism to provide the interface between network providers and its VM networks. We have implemented the Harvester VLAN network based on the bridge CNI to provide a pure L2-mode network, that would bridge your VMs to the host network interface and can be connected using the physical switch for both internal and external network communication. Moreover, the Harvester UI integrates the harvester-network-controller to provide user-friendly VLAN network configurations, e.g., to create and manage VLAN networks or to add a VLAN network to the VM. Currently, Harvester supports two types of networks: Management Network VLAN Network","title":"Harvester Network"},{"location":"networking/harvester-network/#management-network","text":"Harvester uses canal as its default management network. It is a built-in network that can be used directly from the cluster. By default, the management network IP of a VM can only be accessed within the cluster nodes, and the management network IP will change after the VM reboot. This is non-typical behaviour that needs to be taken note of since VM IPs are expected to remain unchanged after a reboot. However, users can leverage the Kubernetes service object to create a stable IP for your VMs with the management network.","title":"Management Network"},{"location":"networking/harvester-network/#vlan-network","text":"The Harvester network-controller leverages the multus and bridge CNI plugins to implement its customized L2 bridge VLAN network. It helps to connect your VMs to the host network interface and can be accessed from internal and external networks using the physical switch. The below diagram illustrates how the VLAN network works in Harvester. The Harvester network-controller creates a bridge for each node and a pair of veth for each VM to implement its VLAN network. The bridge acts as a switch to forward the network traffic from or to VMs and the veth pair is like the connected ports between VMs and the switch. VMs within the same VLAN can communicate with each other, while the VMs from different VLANs can't. The external switch ports connected to the hosts or other devices (such as the DHCP server) should be set as trunk or hybrid type and permit the specified VLANs. Users can use VLAN with PVID (default 1) to communicate with any normal untagged traffic.","title":"VLAN Network"},{"location":"networking/harvester-network/#enabling-default-vlan-network","text":"You can enable VLAN network via Settings > vlan . Select enabled and you will be able to select one network interface from the nodes as the default VLAN NIC config. For better network performances and isolation, we recommend to choose different network interfaces for the VLAN and the one used for the management network (i.e., harvester-mgmt ). Note When selecting the network interface, the value in parentheses represents the distribution percentage of the network interface on all hosts. If a network interface with a value less than 100% is selected, the network interface needs to be manually specified on the host where the VLAN network configuration fails. Modifying the default VLAN network setting will not update the existing configured host network. Harvester VLAN network supports bond interfaces. Currently it can only be created automatically via PXE Boot Configuration . You may also login to the node and create it manually. You can also customize each node's VLAN network via the Hosts > Network tab.","title":"Enabling Default VLAN Network"},{"location":"networking/harvester-network/#create-a-vlan-network","text":"A new VLAN network can be created via the Advanced > Networks page and clicking the Create button. Specify the name and VLAN ID that you want to create for the VLAN network (You can specify the same vlan ID on different namespaces of Rancher multi-tenancy support) . Configure a route in order to allow the hosts to connect to the VLAN network using IPv4 addresses. The CIDR and gateway of the VLAN network are mandatory parameters for the route configuration. You can configure the route by choosing one of the following options: auto(DHCP) mode: the Harvester network controller will get the CIDR and gateway values from the DHCP server using the DHCP protocol. Optionally, you can specify the DHCP server address. manual mode: You need to specify the CIDR and gateway values manually.","title":"Create a VLAN Network"},{"location":"networking/harvester-network/#create-a-vm-with-vlan-network","text":"Users can now create a new VM using the above configured VLAN network, Click the Create button on the Virtual Machines page. Specify the required parameters and click the Networks tab. Either configure the default network to be a VLAN network or select an additional network to add. Note Only the first NIC will be enabled by default. Users can either choose to use a management network or a VLAN network. You need to be careful to configure virtual machines with multiple NICs to avoid connectivity issues. You can refer to the knowledge base for more details. You will need to select the Install guest agent option in the Advanced Options tab to get the VLAN network IP address from the Harvester UI. You can choose to add one or multiple network interface cards. The additional network interface cards can be enabled by default via the cloud-init network data setting. e.g., version : 1 config : - type : physical name : enp1s0 # name is varies upon OS image subnets : - type : dhcp - type : physical name : enp2s0 subnets : - type : static address : 10.0.0.100/24 # IP address is varies upon your environment Harvester is fully compatible with the cloud-init network configs . You can refer to the documentation for more details. Note If you add additional NICs after the VM has started, you will need to manually configure IPs for the additional NICs.","title":"Create a VM with VLAN Network"},{"location":"networking/harvester-network/#configure-dhcp-servers-on-networks","text":"By default, the Harvester VLAN network would expect your router to provide a DHCP server that VMs can request and assign IP addresses automatically. If you are running Harvester in a virtual environment that does not contain a DHCP server, you may consider deploying a DHCP server manually on a node or using a containerized method, e.g, like #947 .","title":"Configure DHCP servers on Networks"},{"location":"networking/best-practice/multiple-nics-non-vlan-aware-switch/","text":"Mulitple NICs with Non VLAN-aware Switch \u00b6 In this best practice guide for \"non VLAN-aware\" switch, also known as \"dummy\" switch, we will introduce Harvester VLAN network and external switch configuration for common scenario. Architecture \u00b6 Hardware: Three Harvester servers with dual ports network card. One or more \"non VLAN-aware\" switch(es). Network Specification: The host and the VM networks are in the same subnet. Cabling: The Harvester servers are connected to the switch in a port from 1 to 6 . The following diagram illustrates the cabling used for this guide: External Switch Configuration \u00b6 Typically, a \"non VLAN-aware\" switch cannot be configured. Create a VLAN Network in Harvester \u00b6 You can create a new VLAN network on the Advanced > Networks page, and click the Create button. Specify the name and a VLAN ID that you want to create for the VLAN network (You can specify the same VLAN ID in different namespaces if you have Rancher multi-tenancy configured) . Connect a VM to the subnet of the Harvester hosts \u00b6 The \"non VLAN-aware\" switch will only send out untagged network traffic to the subnet of the Harvester hosts. In Harvester, the untagged traffic is received in VLAN 1. If you need a VM to connect to the subnet of the Harvester hosts, you have to create a VLAN Network in Harvester with VLAN ID 1. Please refer to this page for additional information on Harvester Networking. Note If you create a VLAN Network different from 1 , the connection between VMs in different nodes will fail.","title":"Mulitple NICs with Non VLAN-aware Switch"},{"location":"networking/best-practice/multiple-nics-non-vlan-aware-switch/#mulitple-nics-with-non-vlan-aware-switch","text":"In this best practice guide for \"non VLAN-aware\" switch, also known as \"dummy\" switch, we will introduce Harvester VLAN network and external switch configuration for common scenario.","title":"Mulitple NICs with Non VLAN-aware Switch"},{"location":"networking/best-practice/multiple-nics-non-vlan-aware-switch/#architecture","text":"Hardware: Three Harvester servers with dual ports network card. One or more \"non VLAN-aware\" switch(es). Network Specification: The host and the VM networks are in the same subnet. Cabling: The Harvester servers are connected to the switch in a port from 1 to 6 . The following diagram illustrates the cabling used for this guide:","title":"Architecture"},{"location":"networking/best-practice/multiple-nics-non-vlan-aware-switch/#external-switch-configuration","text":"Typically, a \"non VLAN-aware\" switch cannot be configured.","title":"External Switch Configuration"},{"location":"networking/best-practice/multiple-nics-non-vlan-aware-switch/#create-a-vlan-network-in-harvester","text":"You can create a new VLAN network on the Advanced > Networks page, and click the Create button. Specify the name and a VLAN ID that you want to create for the VLAN network (You can specify the same VLAN ID in different namespaces if you have Rancher multi-tenancy configured) .","title":"Create a VLAN Network in Harvester"},{"location":"networking/best-practice/multiple-nics-non-vlan-aware-switch/#connect-a-vm-to-the-subnet-of-the-harvester-hosts","text":"The \"non VLAN-aware\" switch will only send out untagged network traffic to the subnet of the Harvester hosts. In Harvester, the untagged traffic is received in VLAN 1. If you need a VM to connect to the subnet of the Harvester hosts, you have to create a VLAN Network in Harvester with VLAN ID 1. Please refer to this page for additional information on Harvester Networking. Note If you create a VLAN Network different from 1 , the connection between VMs in different nodes will fail.","title":"Connect a VM to the subnet of the Harvester hosts"},{"location":"networking/best-practice/multiple-nics-vlan-aware-switch/","text":"Mulitple NICs with VLAN-aware Switch \u00b6 In this best practice guide on how to configure \"VLAN-aware\", we will introduce Harvester VLAN network and external switch configuration for common scenario. Architecture \u00b6 Hardware: Three Harvester servers with daul ports network card. One or more VLAN-aware switch(es). We will use \"Cisco like\" configuration as example. Network Specification: Assume that the subnet of the Harvester hosts is in VLAN 100. Assume that the VMs are in the VLAN 101-200. Cabling: The Harvester servers are connected to the switch in a port from 1 to 6 . The following diagram illustrates the cabling used for this guide: External Switch Configuration \u00b6 For the external switch configuration, we'll use a \"Cisco-like\" configuration as an example. You can apply the following configurations to your switch: For harvester-mgmt ports: switch# config terminal switch(config)# interface ethernet1/<Port Number> switch(config-if)# switchport switch(config-if)# switchport mode access switch(config-if)# switchport access 100 switch(config-if)# no shutdown switch(config-if)# end switch# copy running-config startup-config Note In this case, you need to avoid using harvester-mgmt as the VLAN Network interface. This setting will only allow the traffic in the same subnet of harvester-mgmt and disallow other VLAN traffic. For VLAN network ports: switch# config terminal switch(config)# interface ethernet1/<Port Number> switch(config-if)# switchport switch(config-if)# switchport mode trunk switch(config-if)# switchport trunk allowed vlan 100-200 switch(config-if)# switchport trunk native vlan 1 switch(config-if)# no shutdown switch(config-if)# end switch# copy running-config startup-config Note We use the VLAN Trunk setup to set up the network ports for the VLAN Network. In this case, you can simply set VLAN 100 for the VMs in the Harvester VLAN network to connect to the same subnet of harvester-mgmt . Create a VLAN Network in Harvester \u00b6 You can create a new VLAN network in the Advanced > Networks page, and click the Create button. Specify the name and a VLAN ID that you want to create for the VLAN network (You can specify the same VLAN ID in different namespaces if you have Rancher multi-tenancy configured) . Connect a VM to the subnet of the Harvester hosts \u00b6 Once you finished the configuration in the previous section, the external switch will send out untagged network traffic to the subnet of the Harvester hosts. In Harvester, the untagged traffic is received in VLAN 1. Therefore, if you need VMs to connect to the VLAN ID 1, you need to create a VLAN ID 1 Network in Harvester also. Note We strongly recommend against using VLAN 1 in this scenario. Connect a VM to specific VLAN network \u00b6 You need to create a VLAN network with a specific VLAN ID and associate the VM with that VLAN network. Please refer to this page for additional information on Harvester Networking.","title":"Mulitple NICs with VLAN-aware Switch"},{"location":"networking/best-practice/multiple-nics-vlan-aware-switch/#mulitple-nics-with-vlan-aware-switch","text":"In this best practice guide on how to configure \"VLAN-aware\", we will introduce Harvester VLAN network and external switch configuration for common scenario.","title":"Mulitple NICs with VLAN-aware Switch"},{"location":"networking/best-practice/multiple-nics-vlan-aware-switch/#architecture","text":"Hardware: Three Harvester servers with daul ports network card. One or more VLAN-aware switch(es). We will use \"Cisco like\" configuration as example. Network Specification: Assume that the subnet of the Harvester hosts is in VLAN 100. Assume that the VMs are in the VLAN 101-200. Cabling: The Harvester servers are connected to the switch in a port from 1 to 6 . The following diagram illustrates the cabling used for this guide:","title":"Architecture"},{"location":"networking/best-practice/multiple-nics-vlan-aware-switch/#external-switch-configuration","text":"For the external switch configuration, we'll use a \"Cisco-like\" configuration as an example. You can apply the following configurations to your switch: For harvester-mgmt ports: switch# config terminal switch(config)# interface ethernet1/<Port Number> switch(config-if)# switchport switch(config-if)# switchport mode access switch(config-if)# switchport access 100 switch(config-if)# no shutdown switch(config-if)# end switch# copy running-config startup-config Note In this case, you need to avoid using harvester-mgmt as the VLAN Network interface. This setting will only allow the traffic in the same subnet of harvester-mgmt and disallow other VLAN traffic. For VLAN network ports: switch# config terminal switch(config)# interface ethernet1/<Port Number> switch(config-if)# switchport switch(config-if)# switchport mode trunk switch(config-if)# switchport trunk allowed vlan 100-200 switch(config-if)# switchport trunk native vlan 1 switch(config-if)# no shutdown switch(config-if)# end switch# copy running-config startup-config Note We use the VLAN Trunk setup to set up the network ports for the VLAN Network. In this case, you can simply set VLAN 100 for the VMs in the Harvester VLAN network to connect to the same subnet of harvester-mgmt .","title":"External Switch Configuration"},{"location":"networking/best-practice/multiple-nics-vlan-aware-switch/#create-a-vlan-network-in-harvester","text":"You can create a new VLAN network in the Advanced > Networks page, and click the Create button. Specify the name and a VLAN ID that you want to create for the VLAN network (You can specify the same VLAN ID in different namespaces if you have Rancher multi-tenancy configured) .","title":"Create a VLAN Network in Harvester"},{"location":"networking/best-practice/multiple-nics-vlan-aware-switch/#connect-a-vm-to-the-subnet-of-the-harvester-hosts","text":"Once you finished the configuration in the previous section, the external switch will send out untagged network traffic to the subnet of the Harvester hosts. In Harvester, the untagged traffic is received in VLAN 1. Therefore, if you need VMs to connect to the VLAN ID 1, you need to create a VLAN ID 1 Network in Harvester also. Note We strongly recommend against using VLAN 1 in this scenario.","title":"Connect a VM to the subnet of the Harvester hosts"},{"location":"networking/best-practice/multiple-nics-vlan-aware-switch/#connect-a-vm-to-specific-vlan-network","text":"You need to create a VLAN network with a specific VLAN ID and associate the VM with that VLAN network. Please refer to this page for additional information on Harvester Networking.","title":"Connect a VM to specific VLAN network"},{"location":"networking/best-practice/overview/","text":"Overview \u00b6 In a real production environment, we generally recommend that you have multiple NICs in your machine, one for node access and one for VM networking. If your machine has multiple NICs, please refer to multiple NICs for best practices. Otherwise, please refer to Single NIC best practice. Note If you configure a bond interface with multiple NICs, please refer to the single NIC scenario, unless the Harvester node has multiple bond interfaces. Best Practice \u00b6 Multiple NICs with VLAN-aware switch Multiple NICs with non VLAN-aware switch Single NIC with VLAN-aware switch Single NIC with non VLAN-aware switch","title":"Overview"},{"location":"networking/best-practice/overview/#overview","text":"In a real production environment, we generally recommend that you have multiple NICs in your machine, one for node access and one for VM networking. If your machine has multiple NICs, please refer to multiple NICs for best practices. Otherwise, please refer to Single NIC best practice. Note If you configure a bond interface with multiple NICs, please refer to the single NIC scenario, unless the Harvester node has multiple bond interfaces.","title":"Overview"},{"location":"networking/best-practice/overview/#best-practice","text":"Multiple NICs with VLAN-aware switch Multiple NICs with non VLAN-aware switch Single NIC with VLAN-aware switch Single NIC with non VLAN-aware switch","title":"Best Practice"},{"location":"networking/best-practice/single-nic-non-vlan-aware-switch/","text":"Single NIC with Non VLAN-aware Switch \u00b6 In this best practice guide for \"non VLAN-aware\" switch, also known as \"dummy\" switch, we will introduce Harvester VLAN network and external switch configuration for common scenario. Architecture \u00b6 Hardware: Three Harvester servers with only one single port network card. One or more \"non VLAN-aware\" switch(es). Network Specification: The host and the VM networks are in the same subnet. Cabling: The Harvester servers are connected to the switch in a port from 1 to 3 . The following diagram illustrates the cabling used for this guide: External Switch Configuration \u00b6 Typically, a \"non VLAN-aware\" switch cannot be configured. Create a VLAN Network in Harvester \u00b6 You can create a new VLAN network in the Advanced > Networks page, and click the Create button. Specify the name and VLAN ID that you want to create for the VLAN network (You can specify the same VLAN ID in different namespaces if you have Rancher multi-tenancy configured) . Connect a VM to the subnet of the Harvester hosts \u00b6 The \"non VLAN-aware\" switch will only send out untagged network traffic to the subnet of the Harvester hosts. In Harvester, the untagged traffic is received in VLAN 1. If you need a VM to connect to the subnet of the Harvester hosts, you have to create a VLAN Network in Harvester with VLAN ID 1. Please refer to this page for additional information on Harvester Networking. Note If you create a VLAN Network different from 1 , the connection between VMs in different nodes will fail.","title":"Single NIC with Non VLAN-aware Switch"},{"location":"networking/best-practice/single-nic-non-vlan-aware-switch/#single-nic-with-non-vlan-aware-switch","text":"In this best practice guide for \"non VLAN-aware\" switch, also known as \"dummy\" switch, we will introduce Harvester VLAN network and external switch configuration for common scenario.","title":"Single NIC with Non VLAN-aware Switch"},{"location":"networking/best-practice/single-nic-non-vlan-aware-switch/#architecture","text":"Hardware: Three Harvester servers with only one single port network card. One or more \"non VLAN-aware\" switch(es). Network Specification: The host and the VM networks are in the same subnet. Cabling: The Harvester servers are connected to the switch in a port from 1 to 3 . The following diagram illustrates the cabling used for this guide:","title":"Architecture"},{"location":"networking/best-practice/single-nic-non-vlan-aware-switch/#external-switch-configuration","text":"Typically, a \"non VLAN-aware\" switch cannot be configured.","title":"External Switch Configuration"},{"location":"networking/best-practice/single-nic-non-vlan-aware-switch/#create-a-vlan-network-in-harvester","text":"You can create a new VLAN network in the Advanced > Networks page, and click the Create button. Specify the name and VLAN ID that you want to create for the VLAN network (You can specify the same VLAN ID in different namespaces if you have Rancher multi-tenancy configured) .","title":"Create a VLAN Network in Harvester"},{"location":"networking/best-practice/single-nic-non-vlan-aware-switch/#connect-a-vm-to-the-subnet-of-the-harvester-hosts","text":"The \"non VLAN-aware\" switch will only send out untagged network traffic to the subnet of the Harvester hosts. In Harvester, the untagged traffic is received in VLAN 1. If you need a VM to connect to the subnet of the Harvester hosts, you have to create a VLAN Network in Harvester with VLAN ID 1. Please refer to this page for additional information on Harvester Networking. Note If you create a VLAN Network different from 1 , the connection between VMs in different nodes will fail.","title":"Connect a VM to the subnet of the Harvester hosts"},{"location":"networking/best-practice/single-nic-vlan-aware-switch/","text":"Single NIC with VLAN-aware Switch \u00b6 In this best practice guide on how to configure \"VLAN-aware\", we will introduce Harvester VLAN network and external switch configuration for common scenario. Architecture \u00b6 Hardware: Three Harvester servers with only one single port network card. One or more VLAN-aware switch(es). We will use \"Cisco like\" configuration as example. Network Specification: Assume that the subnet of the Harvester hosts is in VLAN 100. Assume that the VMs are in the VLAN 101-200. Cabling: The Harvester servers are connected to the switch in a port from 1 to 3 . The following diagram illustrates the cabling used for this guide: External Switch Configuration \u00b6 For the external switch configuration, we'll use a \"Cisco like\" configuration as example. You can apply the following configurations to your switch: switch# config terminal switch(config)# interface ethernet1/<Port Number> switch(config-if)# switchport switch(config-if)# switchport mode trunk switch(config-if)# switchport trunk allowed vlan 100-200 switch(config-if)# switchport trunk native vlan 100 switch(config-if)# no shutdown switch(config-if)# end switch# copy running-config startup-config Create a VLAN Network in Harvester \u00b6 You can create a new VLAN network in the Advanced > Networks page, and click the Create button. Specify the name and VLAN ID that you want to create for the VLAN network (You can specify the same VLAN ID in different namespaces if you have Rancher multi-tenancy configured) . Connect a VM to the subnet of the Harvester hosts \u00b6 Once you finished the configuration in the previous section, the external switch will send out untagged network traffic to the subnet of the Harvester hosts. In Harvester, the untagged traffic is received in VLAN 1. Therefore, if you need VMs to connect to the VLAN ID 100, you need to create a VLAN ID 1 Network in Harvester. The external switch will remove the VLAN 100 tag from the packet for egress and harvester-br0 will add the VLAN 1 tag to the packet and treat it as VLAN 1 as shown in the following diagram: Warning Do not create a VLAN Network with VLAN 100 and associate any VM to it. The connectivity will not always be ensured and depends on the external switch behavior to add/remove VLAN tag from packets. Connect a VM to specific VLAN network \u00b6 You need to create a VLAN Network with specific VLAN ID and associate the VM to that VLAN network. Please refer to this page for additional information on Harvester Networking.","title":"Single NIC with VLAN-aware Switch"},{"location":"networking/best-practice/single-nic-vlan-aware-switch/#single-nic-with-vlan-aware-switch","text":"In this best practice guide on how to configure \"VLAN-aware\", we will introduce Harvester VLAN network and external switch configuration for common scenario.","title":"Single NIC with VLAN-aware Switch"},{"location":"networking/best-practice/single-nic-vlan-aware-switch/#architecture","text":"Hardware: Three Harvester servers with only one single port network card. One or more VLAN-aware switch(es). We will use \"Cisco like\" configuration as example. Network Specification: Assume that the subnet of the Harvester hosts is in VLAN 100. Assume that the VMs are in the VLAN 101-200. Cabling: The Harvester servers are connected to the switch in a port from 1 to 3 . The following diagram illustrates the cabling used for this guide:","title":"Architecture"},{"location":"networking/best-practice/single-nic-vlan-aware-switch/#external-switch-configuration","text":"For the external switch configuration, we'll use a \"Cisco like\" configuration as example. You can apply the following configurations to your switch: switch# config terminal switch(config)# interface ethernet1/<Port Number> switch(config-if)# switchport switch(config-if)# switchport mode trunk switch(config-if)# switchport trunk allowed vlan 100-200 switch(config-if)# switchport trunk native vlan 100 switch(config-if)# no shutdown switch(config-if)# end switch# copy running-config startup-config","title":"External Switch Configuration"},{"location":"networking/best-practice/single-nic-vlan-aware-switch/#create-a-vlan-network-in-harvester","text":"You can create a new VLAN network in the Advanced > Networks page, and click the Create button. Specify the name and VLAN ID that you want to create for the VLAN network (You can specify the same VLAN ID in different namespaces if you have Rancher multi-tenancy configured) .","title":"Create a VLAN Network in Harvester"},{"location":"networking/best-practice/single-nic-vlan-aware-switch/#connect-a-vm-to-the-subnet-of-the-harvester-hosts","text":"Once you finished the configuration in the previous section, the external switch will send out untagged network traffic to the subnet of the Harvester hosts. In Harvester, the untagged traffic is received in VLAN 1. Therefore, if you need VMs to connect to the VLAN ID 100, you need to create a VLAN ID 1 Network in Harvester. The external switch will remove the VLAN 100 tag from the packet for egress and harvester-br0 will add the VLAN 1 tag to the packet and treat it as VLAN 1 as shown in the following diagram: Warning Do not create a VLAN Network with VLAN 100 and associate any VM to it. The connectivity will not always be ensured and depends on the external switch behavior to add/remove VLAN tag from packets.","title":"Connect a VM to the subnet of the Harvester hosts"},{"location":"networking/best-practice/single-nic-vlan-aware-switch/#connect-a-vm-to-specific-vlan-network","text":"You need to create a VLAN Network with specific VLAN ID and associate the VM to that VLAN network. Please refer to this page for additional information on Harvester Networking.","title":"Connect a VM to specific VLAN network"},{"location":"rancher/cloud-provider/","text":"Harvester Cloud Provider \u00b6 RKE1 and RKE2 clusters can be provisioned in Rancher using the built-in Harvester Node Driver. Harvester provides load balancer and cluster Persistent Storage support to the guest Kubernetes cluster. In this page we will learn: How to deploy the Harvester cloud provider in both RKE1 and RKE2. How to use the Harvester load balancer . Deploying \u00b6 Prerequisites \u00b6 The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace. Deploying to the RKE1 Cluster with Harvester Node Driver \u00b6 When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider: Select Harvester(Out-of-tree) option. Install Harvester Cloud Provider from the Rancher marketplace. Note You should specify the Cluster name . The default value kubernetes will be set if no Cluster name is entered. The Cluster name is used to distinguish the ownership of the Harvester load balancers. Install Harvester csi driver from the Rancher marketplace if needed. Deploying to the RKE2 Cluster with Harvester Node Driver [Experimental] \u00b6 When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider. The node driver will then help deploy both the CSI driver and CCM automatically. Deploying to the K3s Cluster with Harvester Node Driver [Experimental] \u00b6 Choose the Kubernetes version to be k3s and click the Edit as YAML button to config the K3s cluster YAML (For existing cluster, you can also click the Edit YAML button to update it): Edit K3s cluster YAML. Set disable-cloud-provider: true to disable default k3s cloud provider. Add cloud-provider=external to use harvester cloud provider. Generate addon configuration and put it in K3s VMs /etc/kubernetes/cloud-config . Deploy external cloud provider \u00b6 Deploying external cloud provider is similar for both RKE2 and K3s based clusters. Once the in-tree cloud provider has been disabled by following the above steps, you can deploy the external cloud provider via: A sample additional manifest is as follows: apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: harvester-cloud-provider namespace: kube-system spec: targetNamespace: kube-system bootstrap: true repo: https://charts.harvesterhci.io/ chart: harvester-cloud-provider version: 0.1.12 helmVersion: v3 The cloud provider needs a kubeconfig file to work, a limited scoped one can be generated using the generate_addon.sh script available in the harvester/cloud-provider-harvester repo. NOTE: The script needs access to the harvester cluster kubeconfig to work. In addition the namespace needs to be the namespace in which the workload cluster will be created. # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon.sh <serviceaccount name> <namespace> The output will look as follows: (\u2388 |local:default)\u279c cloud-provider-harvester git:(master) \u2717 ./deploy/generate_addon.sh harvester-cloud-provider default Creating target directory to hold files in ./tmp/kube...done Creating a service account in default namespace: harvester-cloud-provider W0506 16:44:15.429068 3008674 helpers.go:598] --dry-run is deprecated and can be replaced with --dry-run=client. serviceaccount/harvester-cloud-provider configured Creating a role in default namespace: harvester-cloud-provider role.rbac.authorization.k8s.io/harvester-cloud-provider unchanged Creating a rolebinding in default namespace: harvester-cloud-provider W0506 16:44:23.798293 3008738 helpers.go:598] --dry-run is deprecated and can be replaced with --dry-run=client. rolebinding.rbac.authorization.k8s.io/harvester-cloud-provider configured Getting secret of service account harvester-cloud-provider on default Secret name: harvester-cloud-provider-token-5zkk9 Extracting ca.crt from secret...done Getting user token from secret...done Setting current context to: local Cluster name: local Endpoint: https://HARVESTER_ENDPOINT/k8s/clusters/local Preparing k8s-harvester-cloud-provider-default-conf Setting a cluster entry in kubeconfig...Cluster \"local\" set. Setting token credentials entry in kubeconfig...User \"harvester-cloud-provider-default-local\" set. Setting a context entry in kubeconfig...Context \"harvester-cloud-provider-default-local\" created. Setting the current-context in the kubeconfig file...Switched to context \"harvester-cloud-provider-default-local\". ########## cloud config ############ apiVersion: v1 clusters: - cluster: certificate-authority-data: CACERT server: https://HARVESTER-ENDPOINT/k8s/clusters/local name: local contexts: - context: cluster: local namespace: default user: harvester-cloud-provider-default-local name: harvester-cloud-provider-default-local current-context: harvester-cloud-provider-default-local kind: Config preferences: {} users: - name: harvester-cloud-provider-default-local user: token: TOKEN This cloud-config file can now be injected via the user-data available in the advanced options for the nodepool. With these settings in place a K3s / RKE2 cluster should provision successfully while using the external cloud provider. Load Balancer Support \u00b6 After deploying the Harvester Cloud provider , you can use the Kubernetes LoadBalancer service to expose a microservice inside the guest cluster to the external world. When you create a Kubernetes LoadBalancer service, a Harvester load balancer is assigned to the service and you can edit it through the Add-on Config in the Rancher UI. IPAM \u00b6 Harvester's built-in load balancer supports both pool and dhcp modes. You can select the mode in the Rancher UI. Harvester adds the annotation cloudprovider.harvesterhci.io/ipam to the service behind. pool: You should configure an IP address pool in Harvester in advance. The Harvester LoadBalancer controller will allocate an IP address from the IP address pool for the load balancer. dhcp: A DHCP server is required. The Harvester LoadBalancer controller will request an IP address from the DHCP server. Note It is not allowed to modify the IPAM mode. You need to create a new service if you want to modify the IPAM mode. Health Checks \u00b6 The Harvester load balancer supports TCP health checks. You can specify the parameters in the Rancher UI if you enable the Health Check option. Alternatively, you can specify the parameters by adding annotations to the service manually. The following annotations are supported: Annotation Key Value Type Required Description cloudprovider.harvesterhci.io/healthcheck-port string true Specifies the port. The prober will access the address composed of the backend server IP and the port. cloudprovider.harvesterhci.io/healthcheck-success-threshold string false Specifies the health check success threshold. The default value is 1. The backend server will start forwarding traffic if the number of times the prober continuously detects an address successfully reaches the threshold. cloudprovider.harvesterhci.io/healthcheck-failure-threshold string false Specifies the health check failure threshold. The default value is 3. The backend server will stop forwarding traffic if the number of health check failures reaches the threshold. cloudprovider.harvesterhci.io/healthcheck-periodseconds string false Specifies the health check period. The default value is 5 seconds. cloudprovider.harvesterhci.io/healthcheck-timeoutseconds string false Specifies the timeout of every health check. The default value is 3 seconds.","title":"Harvester Cloud Provider"},{"location":"rancher/cloud-provider/#harvester-cloud-provider","text":"RKE1 and RKE2 clusters can be provisioned in Rancher using the built-in Harvester Node Driver. Harvester provides load balancer and cluster Persistent Storage support to the guest Kubernetes cluster. In this page we will learn: How to deploy the Harvester cloud provider in both RKE1 and RKE2. How to use the Harvester load balancer .","title":"Harvester Cloud Provider"},{"location":"rancher/cloud-provider/#deploying","text":"","title":"Deploying"},{"location":"rancher/cloud-provider/#prerequisites","text":"The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace.","title":"Prerequisites"},{"location":"rancher/cloud-provider/#deploying-to-the-rke1-cluster-with-harvester-node-driver","text":"When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider: Select Harvester(Out-of-tree) option. Install Harvester Cloud Provider from the Rancher marketplace. Note You should specify the Cluster name . The default value kubernetes will be set if no Cluster name is entered. The Cluster name is used to distinguish the ownership of the Harvester load balancers. Install Harvester csi driver from the Rancher marketplace if needed.","title":"Deploying to the RKE1 Cluster with Harvester Node Driver"},{"location":"rancher/cloud-provider/#deploying-to-the-rke2-cluster-with-harvester-node-driver-experimental","text":"When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider. The node driver will then help deploy both the CSI driver and CCM automatically.","title":"Deploying to the RKE2 Cluster with Harvester Node Driver [Experimental]"},{"location":"rancher/cloud-provider/#deploying-to-the-k3s-cluster-with-harvester-node-driver-experimental","text":"Choose the Kubernetes version to be k3s and click the Edit as YAML button to config the K3s cluster YAML (For existing cluster, you can also click the Edit YAML button to update it): Edit K3s cluster YAML. Set disable-cloud-provider: true to disable default k3s cloud provider. Add cloud-provider=external to use harvester cloud provider. Generate addon configuration and put it in K3s VMs /etc/kubernetes/cloud-config .","title":"Deploying to the K3s Cluster with Harvester Node Driver [Experimental]"},{"location":"rancher/cloud-provider/#deploy-external-cloud-provider","text":"Deploying external cloud provider is similar for both RKE2 and K3s based clusters. Once the in-tree cloud provider has been disabled by following the above steps, you can deploy the external cloud provider via: A sample additional manifest is as follows: apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: harvester-cloud-provider namespace: kube-system spec: targetNamespace: kube-system bootstrap: true repo: https://charts.harvesterhci.io/ chart: harvester-cloud-provider version: 0.1.12 helmVersion: v3 The cloud provider needs a kubeconfig file to work, a limited scoped one can be generated using the generate_addon.sh script available in the harvester/cloud-provider-harvester repo. NOTE: The script needs access to the harvester cluster kubeconfig to work. In addition the namespace needs to be the namespace in which the workload cluster will be created. # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon.sh <serviceaccount name> <namespace> The output will look as follows: (\u2388 |local:default)\u279c cloud-provider-harvester git:(master) \u2717 ./deploy/generate_addon.sh harvester-cloud-provider default Creating target directory to hold files in ./tmp/kube...done Creating a service account in default namespace: harvester-cloud-provider W0506 16:44:15.429068 3008674 helpers.go:598] --dry-run is deprecated and can be replaced with --dry-run=client. serviceaccount/harvester-cloud-provider configured Creating a role in default namespace: harvester-cloud-provider role.rbac.authorization.k8s.io/harvester-cloud-provider unchanged Creating a rolebinding in default namespace: harvester-cloud-provider W0506 16:44:23.798293 3008738 helpers.go:598] --dry-run is deprecated and can be replaced with --dry-run=client. rolebinding.rbac.authorization.k8s.io/harvester-cloud-provider configured Getting secret of service account harvester-cloud-provider on default Secret name: harvester-cloud-provider-token-5zkk9 Extracting ca.crt from secret...done Getting user token from secret...done Setting current context to: local Cluster name: local Endpoint: https://HARVESTER_ENDPOINT/k8s/clusters/local Preparing k8s-harvester-cloud-provider-default-conf Setting a cluster entry in kubeconfig...Cluster \"local\" set. Setting token credentials entry in kubeconfig...User \"harvester-cloud-provider-default-local\" set. Setting a context entry in kubeconfig...Context \"harvester-cloud-provider-default-local\" created. Setting the current-context in the kubeconfig file...Switched to context \"harvester-cloud-provider-default-local\". ########## cloud config ############ apiVersion: v1 clusters: - cluster: certificate-authority-data: CACERT server: https://HARVESTER-ENDPOINT/k8s/clusters/local name: local contexts: - context: cluster: local namespace: default user: harvester-cloud-provider-default-local name: harvester-cloud-provider-default-local current-context: harvester-cloud-provider-default-local kind: Config preferences: {} users: - name: harvester-cloud-provider-default-local user: token: TOKEN This cloud-config file can now be injected via the user-data available in the advanced options for the nodepool. With these settings in place a K3s / RKE2 cluster should provision successfully while using the external cloud provider.","title":"Deploy external cloud provider"},{"location":"rancher/cloud-provider/#load-balancer-support","text":"After deploying the Harvester Cloud provider , you can use the Kubernetes LoadBalancer service to expose a microservice inside the guest cluster to the external world. When you create a Kubernetes LoadBalancer service, a Harvester load balancer is assigned to the service and you can edit it through the Add-on Config in the Rancher UI.","title":"Load Balancer Support"},{"location":"rancher/cloud-provider/#ipam","text":"Harvester's built-in load balancer supports both pool and dhcp modes. You can select the mode in the Rancher UI. Harvester adds the annotation cloudprovider.harvesterhci.io/ipam to the service behind. pool: You should configure an IP address pool in Harvester in advance. The Harvester LoadBalancer controller will allocate an IP address from the IP address pool for the load balancer. dhcp: A DHCP server is required. The Harvester LoadBalancer controller will request an IP address from the DHCP server. Note It is not allowed to modify the IPAM mode. You need to create a new service if you want to modify the IPAM mode.","title":"IPAM"},{"location":"rancher/cloud-provider/#health-checks","text":"The Harvester load balancer supports TCP health checks. You can specify the parameters in the Rancher UI if you enable the Health Check option. Alternatively, you can specify the parameters by adding annotations to the service manually. The following annotations are supported: Annotation Key Value Type Required Description cloudprovider.harvesterhci.io/healthcheck-port string true Specifies the port. The prober will access the address composed of the backend server IP and the port. cloudprovider.harvesterhci.io/healthcheck-success-threshold string false Specifies the health check success threshold. The default value is 1. The backend server will start forwarding traffic if the number of times the prober continuously detects an address successfully reaches the threshold. cloudprovider.harvesterhci.io/healthcheck-failure-threshold string false Specifies the health check failure threshold. The default value is 3. The backend server will stop forwarding traffic if the number of health check failures reaches the threshold. cloudprovider.harvesterhci.io/healthcheck-periodseconds string false Specifies the health check period. The default value is 5 seconds. cloudprovider.harvesterhci.io/healthcheck-timeoutseconds string false Specifies the timeout of every health check. The default value is 3 seconds.","title":"Health Checks"},{"location":"rancher/csi-driver/","text":"Harvester CSI Driver \u00b6 The Harvester Container Storage Interface (CSI) Driver provides a CSI interface used by guest Kubernetes clusters in Harvester. It connects to the host cluster and hot-plugs host volumes to the virtual machines (VMs) to provide native storage performance. Deploying \u00b6 Prerequisites \u00b6 The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace. Notes Currently, the Harvester CSI driver only supports single-node read-write(RWO) volumes. Please follow the issue #1992 for future multi-node read-only (ROX) and read-write (RWX) support. Deploying with Harvester RKE1 Node Driver \u00b6 Select the external cloud provider option. Generate addon configuration and add it in the RKE config YAML. # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon.sh <serviceaccount name> <namespace> Deploying with Harvester RKE2 Node Driver \u00b6 When spinning up a Kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed when Harvester cloud provider is selected. Deploying with Harvester K3s Node Driver \u00b6 Generate addon configuration and put it in K3s VMs /etc/kubernetes/cloud-config . # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon.sh <serviceaccount name> <namespace> Install Harvester CSI Driver from the Rancher marketplace.","title":"Harvester CSI Driver"},{"location":"rancher/csi-driver/#harvester-csi-driver","text":"The Harvester Container Storage Interface (CSI) Driver provides a CSI interface used by guest Kubernetes clusters in Harvester. It connects to the host cluster and hot-plugs host volumes to the virtual machines (VMs) to provide native storage performance.","title":"Harvester CSI Driver"},{"location":"rancher/csi-driver/#deploying","text":"","title":"Deploying"},{"location":"rancher/csi-driver/#prerequisites","text":"The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace. Notes Currently, the Harvester CSI driver only supports single-node read-write(RWO) volumes. Please follow the issue #1992 for future multi-node read-only (ROX) and read-write (RWX) support.","title":"Prerequisites"},{"location":"rancher/csi-driver/#deploying-with-harvester-rke1-node-driver","text":"Select the external cloud provider option. Generate addon configuration and add it in the RKE config YAML. # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon.sh <serviceaccount name> <namespace>","title":"Deploying with Harvester RKE1 Node Driver"},{"location":"rancher/csi-driver/#deploying-with-harvester-rke2-node-driver","text":"When spinning up a Kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed when Harvester cloud provider is selected.","title":"Deploying with Harvester RKE2 Node Driver"},{"location":"rancher/csi-driver/#deploying-with-harvester-k3s-node-driver","text":"Generate addon configuration and put it in K3s VMs /etc/kubernetes/cloud-config . # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon.sh <serviceaccount name> <namespace> Install Harvester CSI Driver from the Rancher marketplace.","title":"Deploying with Harvester K3s Node Driver"},{"location":"rancher/k3s-cluster/","text":"Creating an K3s Kubernetes Cluster \u00b6 You can now provision K3s Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.3+ using the built-in Harvester node driver. Note Harvester K3s node driver is in tech preview. VLAN network is required for Harvester node driver. Create Your Cloud Credentials \u00b6 Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name Select \"Imported Harvester\" or \"External Harvester\". Click Create . Create K3s Kubernetes Cluster \u00b6 You can create a K3s Kubernetes cluster from the Cluster Management page via the K3s node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE2/K3s . Select Harvester node driver. Select a Cloud Credential . Enter Cluster Name (required). Enter Namespace (required). Enter Image (required). Enter Network Name (required). Enter SSH User (required). Click Create . Add Node Affinity \u00b6 Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules. This provides high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node Selector Set priority to Required if you wish the scheduler to schedule the machines only when the rules are met. Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key : topology.kubernetes.io/region operator : in list values : us-east-1 --- key : topology.kubernetes.io/zone operator : in list values : us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled according to the affinity rules. Using Harvester K3s Node Driver in Air Gapped Environment \u00b6 K3s provisioning relies on the qemu-guest-agent to get the IP of the virtual machine. However, it may not be feasible to install qemu-guest-agent in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent via an HTTP(S) proxy. Example of user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128","title":"Creating an K3s Kubernetes Cluster"},{"location":"rancher/k3s-cluster/#creating-an-k3s-kubernetes-cluster","text":"You can now provision K3s Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.3+ using the built-in Harvester node driver. Note Harvester K3s node driver is in tech preview. VLAN network is required for Harvester node driver.","title":"Creating an K3s Kubernetes Cluster"},{"location":"rancher/k3s-cluster/#create-your-cloud-credentials","text":"Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name Select \"Imported Harvester\" or \"External Harvester\". Click Create .","title":"Create Your Cloud Credentials"},{"location":"rancher/k3s-cluster/#create-k3s-kubernetes-cluster","text":"You can create a K3s Kubernetes cluster from the Cluster Management page via the K3s node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE2/K3s . Select Harvester node driver. Select a Cloud Credential . Enter Cluster Name (required). Enter Namespace (required). Enter Image (required). Enter Network Name (required). Enter SSH User (required). Click Create .","title":"Create K3s Kubernetes Cluster"},{"location":"rancher/k3s-cluster/#add-node-affinity","text":"Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules. This provides high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node Selector Set priority to Required if you wish the scheduler to schedule the machines only when the rules are met. Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key : topology.kubernetes.io/region operator : in list values : us-east-1 --- key : topology.kubernetes.io/zone operator : in list values : us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled according to the affinity rules.","title":"Add Node Affinity"},{"location":"rancher/k3s-cluster/#using-harvester-k3s-node-driver-in-air-gapped-environment","text":"K3s provisioning relies on the qemu-guest-agent to get the IP of the virtual machine. However, it may not be feasible to install qemu-guest-agent in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent via an HTTP(S) proxy. Example of user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128","title":"Using Harvester K3s Node Driver in Air Gapped Environment"},{"location":"rancher/node-driver/","text":"Harvester Node Driver \u00b6 The Harvester node driver is used to provision VMs in the Harvester cluster. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver , and the project repo is available at harvester/docker-machine-driver-harvester . You can now provision RKE1/RKE2 Kubernetes clusters in Rancher v2.6.3+ with the built-in Harvester node driver. Additionally, Harvester now can provide built-in Load Balancer support as well as raw cluster persistent storage support to the guest Kubernetes cluster. While you can upload and view .ISO images in the Harvester UI , the same capability is not available in the Rancher UI. For more information on this, see the Rancher docs . Note Harvester v1.0.0 is compatible with Rancher v2.6.3+ only. Harvester Node Driver \u00b6 The Harvester node driver is enabled by default from Rancher v2.6.3 . You can go to Cluster Management > Drivers > Node Drivers page to manage the Harvester node driver manually. When the Harvester node driver is enabled, you can create Kubernetes clusters on top of the Harvester cluster and manage them from Rancher. RKE1 Kubernetes Cluster \u00b6 Click to learn how to create RKE1 Kubernetes Clusters . RKE2 Kubernetes Cluster \u00b6 Click to learn how to create RKE2 Kubernetes Clusters . K3s Kubernetes Cluster \u00b6 Click to learn how to create k3s Kubernetes Clusters . Topology Spread Constraints \u00b6 Available as of v1.0.3 In your guest Kubernetes cluster, you can use topology spread constraints to control how workloads are spread across the Harvester VMs among failure-domains such as regions and zones. This can help to achieve high availability as well as efficient resource utilization of your cluster resources. Sync Topology Labels to the Guest Cluster Node \u00b6 During the cluster installation, the Harvester node driver will automatically help synchronize topology labels from VM nodes to guest cluster nodes. Currently, only region and zone typology labels are supported. Note Label synchronization will only take effect during guest node initialization. To avoid node drifts to another region or zone, it is recommended to add the node affinity rules during the cluster provisioning, so that the VMs can be scheduled to the same zone even after rebuilding. Configuring topology labels on the Harvester nodes through Hosts > Edit Config > Labels . e.g., add the topology labels as follows: topology.kubernetes.io/region : us-east-1 topology.kubernetes.io/zone : us-east-1a Creating a guest Kubernetes cluster using the Harvester node driver and it is recommended to add the node affinity rules , this will help to avoid node drifting to other zones after VM rebuilding. After the cluster is successfully deployed, confirm that guest Kubernetes node labels are successfully synchronized from the Harvester VM node. Now deploy workloads on your guest Kubernetes cluster, and you should be able to manage them using the topology spread constraints .","title":"Harvester Node Driver"},{"location":"rancher/node-driver/#harvester-node-driver","text":"The Harvester node driver is used to provision VMs in the Harvester cluster. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver , and the project repo is available at harvester/docker-machine-driver-harvester . You can now provision RKE1/RKE2 Kubernetes clusters in Rancher v2.6.3+ with the built-in Harvester node driver. Additionally, Harvester now can provide built-in Load Balancer support as well as raw cluster persistent storage support to the guest Kubernetes cluster. While you can upload and view .ISO images in the Harvester UI , the same capability is not available in the Rancher UI. For more information on this, see the Rancher docs . Note Harvester v1.0.0 is compatible with Rancher v2.6.3+ only.","title":"Harvester Node Driver"},{"location":"rancher/node-driver/#harvester-node-driver_1","text":"The Harvester node driver is enabled by default from Rancher v2.6.3 . You can go to Cluster Management > Drivers > Node Drivers page to manage the Harvester node driver manually. When the Harvester node driver is enabled, you can create Kubernetes clusters on top of the Harvester cluster and manage them from Rancher.","title":"Harvester Node Driver"},{"location":"rancher/node-driver/#rke1-kubernetes-cluster","text":"Click to learn how to create RKE1 Kubernetes Clusters .","title":"RKE1 Kubernetes Cluster"},{"location":"rancher/node-driver/#rke2-kubernetes-cluster","text":"Click to learn how to create RKE2 Kubernetes Clusters .","title":"RKE2 Kubernetes Cluster"},{"location":"rancher/node-driver/#k3s-kubernetes-cluster","text":"Click to learn how to create k3s Kubernetes Clusters .","title":"K3s Kubernetes Cluster"},{"location":"rancher/node-driver/#topology-spread-constraints","text":"Available as of v1.0.3 In your guest Kubernetes cluster, you can use topology spread constraints to control how workloads are spread across the Harvester VMs among failure-domains such as regions and zones. This can help to achieve high availability as well as efficient resource utilization of your cluster resources.","title":"Topology Spread Constraints"},{"location":"rancher/node-driver/#sync-topology-labels-to-the-guest-cluster-node","text":"During the cluster installation, the Harvester node driver will automatically help synchronize topology labels from VM nodes to guest cluster nodes. Currently, only region and zone typology labels are supported. Note Label synchronization will only take effect during guest node initialization. To avoid node drifts to another region or zone, it is recommended to add the node affinity rules during the cluster provisioning, so that the VMs can be scheduled to the same zone even after rebuilding. Configuring topology labels on the Harvester nodes through Hosts > Edit Config > Labels . e.g., add the topology labels as follows: topology.kubernetes.io/region : us-east-1 topology.kubernetes.io/zone : us-east-1a Creating a guest Kubernetes cluster using the Harvester node driver and it is recommended to add the node affinity rules , this will help to avoid node drifting to other zones after VM rebuilding. After the cluster is successfully deployed, confirm that guest Kubernetes node labels are successfully synchronized from the Harvester VM node. Now deploy workloads on your guest Kubernetes cluster, and you should be able to manage them using the topology spread constraints .","title":"Sync Topology Labels to the Guest Cluster Node"},{"location":"rancher/rancher-integration/","text":"Rancher Integration \u00b6 Available as of v0.3.0 Rancher is an open-source multi-cluster management platform. Harvester has integrated Rancher by default starting with Rancher v2.6.1. Rancher & Harvester Support Matrix \u00b6 Rancher Version Harvester Version Note v2.6.6 v1.0.3 - v1.0.1 v2.6.5 v1.0.2 - v1.0.1 v2.6.4 v1.0.1 v2.6.3 v1.0.0 Note Harvester v1.0.0 is compatible with Rancher v2.6.3 or above only. Users can now import and manage multiple Harvester clusters using the Rancher Virtualization Management page and leverage the Rancher authentication feature and RBAC control for multi-tenancy support. Deploying Rancher Server \u00b6 To use Rancher with Harvester, please install the Rancher and Harvester in two separated servers. If you want to try out the integration features, you can create a VM in Harvester and install Rancher v2.6.3 or above(the latest stable version is recommended). Use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform) AWS Marketplace (uses Amazon EKS) Azure (uses Terraform) DigitalOcean (uses Terraform) GCP (uses Terraform) Hetzner Cloud (uses Terraform) Vagrant Equinix Metal Warning Do not install Rancher with Docker in production . Otherwise, your environment may be damaged and your cluster may not be recovered. Installing Rancher in Docker should only be used for quick evaluation and testing purposes. To install Rancher with Docker: Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM) An on-premises VM A bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection. From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart = unless-stopped -p 80 :80 -p 443 :443 --privileged rancher/rancher:v2.6.6 Virtualization Management \u00b6 With Rancher's Virtualization Management feature, you can now import and manage Harvester clusters. By clicking on one of the clusters, you are able to view and manage the imported Harvester cluster resources like Hosts, VMs, images, volumes, etc. Additionally, the Virtualization Management leverages existing Rancher features such as authentication with various auth providers and multi-tenant support. For more details, please check the virtualization management page. Creating Kubernetes Clusters using the Harvester Node Driver \u00b6 Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage guest Kubernetes clusters. Starting with Rancher v2.6.1 , the Harvester node driver has been added by default. Users can reference the node-driver page for more details.","title":"Rancher integration"},{"location":"rancher/rancher-integration/#rancher-integration","text":"Available as of v0.3.0 Rancher is an open-source multi-cluster management platform. Harvester has integrated Rancher by default starting with Rancher v2.6.1.","title":"Rancher Integration"},{"location":"rancher/rancher-integration/#rancher-harvester-support-matrix","text":"Rancher Version Harvester Version Note v2.6.6 v1.0.3 - v1.0.1 v2.6.5 v1.0.2 - v1.0.1 v2.6.4 v1.0.1 v2.6.3 v1.0.0 Note Harvester v1.0.0 is compatible with Rancher v2.6.3 or above only. Users can now import and manage multiple Harvester clusters using the Rancher Virtualization Management page and leverage the Rancher authentication feature and RBAC control for multi-tenancy support.","title":"Rancher &amp; Harvester Support Matrix"},{"location":"rancher/rancher-integration/#deploying-rancher-server","text":"To use Rancher with Harvester, please install the Rancher and Harvester in two separated servers. If you want to try out the integration features, you can create a VM in Harvester and install Rancher v2.6.3 or above(the latest stable version is recommended). Use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform) AWS Marketplace (uses Amazon EKS) Azure (uses Terraform) DigitalOcean (uses Terraform) GCP (uses Terraform) Hetzner Cloud (uses Terraform) Vagrant Equinix Metal Warning Do not install Rancher with Docker in production . Otherwise, your environment may be damaged and your cluster may not be recovered. Installing Rancher in Docker should only be used for quick evaluation and testing purposes. To install Rancher with Docker: Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM) An on-premises VM A bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection. From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart = unless-stopped -p 80 :80 -p 443 :443 --privileged rancher/rancher:v2.6.6","title":"Deploying Rancher Server"},{"location":"rancher/rancher-integration/#virtualization-management","text":"With Rancher's Virtualization Management feature, you can now import and manage Harvester clusters. By clicking on one of the clusters, you are able to view and manage the imported Harvester cluster resources like Hosts, VMs, images, volumes, etc. Additionally, the Virtualization Management leverages existing Rancher features such as authentication with various auth providers and multi-tenant support. For more details, please check the virtualization management page.","title":"Virtualization Management"},{"location":"rancher/rancher-integration/#creating-kubernetes-clusters-using-the-harvester-node-driver","text":"Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage guest Kubernetes clusters. Starting with Rancher v2.6.1 , the Harvester node driver has been added by default. Users can reference the node-driver page for more details.","title":"Creating Kubernetes Clusters using the Harvester Node Driver"},{"location":"rancher/rke1-cluster/","text":"Creating an RKE1 Kubernetes Cluster \u00b6 You can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.3+ with the built-in Harvester node driver. Note VLAN network is required for Harvester node driver. When you create a Kubernetes cluster hosted by the Harvester infrastructure, node templates are used to provision the cluster nodes. These templates use Docker Machine configuration options to define an operating system image and settings/parameters for the node. Node templates can use cloud credentials to access the credentials information required to provision nodes in the infrastructure providers. The same cloud credentials can be used by multiple node templates. By using cloud credentials , you do not have to re-enter access keys for the same cloud provider. Cloud credentials are stored as Kubernetes secrets. You can create cloud credentials in two contexts: During the creation of a node template for a cluster. In the User Settings page All cloud credentials are bound to your user profile and cannot be shared with other users. Create Your Cloud Credentials \u00b6 Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name. Select \"Imported Harvester\" or \"External Harvester\". Click Create . Create Node Template \u00b6 You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials . Configure Instance Options : Configure the CPU, memory, and disk Select an OS image that is compatible with the cloud-init config. Select a network that the node driver is able to connect to; currently, only VLAN is supported. Enter the SSH User; the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu . (Optional) Configure Advanced Options if you want to customise the cloud-init config of the VMs: Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more information. Add Node Affinity \u00b6 Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the node template during the cluster creation, click Add Node Template or edit your existing node template via RKE1 Configuration > Node Templates : Check the Advanced Options tab and click Add Node Selector Set priority to Required if you wish the scheduler to schedule the machines only when the rules are met. Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key : topology.kubernetes.io/region operator : in list values : us-east-1 --- key : topology.kubernetes.io/zone operator : in list values : us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled accordingly to the affinity rules. Create RKE1 Kubernetes Cluster \u00b6 Users can create an RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE1 . Select Harvester node driver. Enter Cluster Name (required). Enter Name Prefix (required). Enter Template (required). Select etcd and Control Plane (required). On the Cluster Options configure Cloud Provider to Harvester if you want to use the Harvester Cloud Provider and CSI Diver . Click Create . Using Harvester RKE1 Node Driver in Air Gapped Environment \u00b6 RKE1 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine, and docker to set up the RKE cluster. However, It may not be feasible to install qemu-guest-agent and docker in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent and docker installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent and docker via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 write_files: - path: /etc/environment content: | HTTP_PROXY=\"http://192.168.0.1:3128\" HTTPS_PROXY=\"http://192.168.0.1:3128\" append: true","title":"Creating an RKE1 Kubernetes Cluster"},{"location":"rancher/rke1-cluster/#creating-an-rke1-kubernetes-cluster","text":"You can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.3+ with the built-in Harvester node driver. Note VLAN network is required for Harvester node driver. When you create a Kubernetes cluster hosted by the Harvester infrastructure, node templates are used to provision the cluster nodes. These templates use Docker Machine configuration options to define an operating system image and settings/parameters for the node. Node templates can use cloud credentials to access the credentials information required to provision nodes in the infrastructure providers. The same cloud credentials can be used by multiple node templates. By using cloud credentials , you do not have to re-enter access keys for the same cloud provider. Cloud credentials are stored as Kubernetes secrets. You can create cloud credentials in two contexts: During the creation of a node template for a cluster. In the User Settings page All cloud credentials are bound to your user profile and cannot be shared with other users.","title":"Creating an RKE1 Kubernetes Cluster"},{"location":"rancher/rke1-cluster/#create-your-cloud-credentials","text":"Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name. Select \"Imported Harvester\" or \"External Harvester\". Click Create .","title":"Create Your Cloud Credentials"},{"location":"rancher/rke1-cluster/#create-node-template","text":"You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials . Configure Instance Options : Configure the CPU, memory, and disk Select an OS image that is compatible with the cloud-init config. Select a network that the node driver is able to connect to; currently, only VLAN is supported. Enter the SSH User; the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu . (Optional) Configure Advanced Options if you want to customise the cloud-init config of the VMs: Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more information.","title":"Create Node Template"},{"location":"rancher/rke1-cluster/#add-node-affinity","text":"Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the node template during the cluster creation, click Add Node Template or edit your existing node template via RKE1 Configuration > Node Templates : Check the Advanced Options tab and click Add Node Selector Set priority to Required if you wish the scheduler to schedule the machines only when the rules are met. Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key : topology.kubernetes.io/region operator : in list values : us-east-1 --- key : topology.kubernetes.io/zone operator : in list values : us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled accordingly to the affinity rules.","title":"Add Node Affinity"},{"location":"rancher/rke1-cluster/#create-rke1-kubernetes-cluster","text":"Users can create an RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE1 . Select Harvester node driver. Enter Cluster Name (required). Enter Name Prefix (required). Enter Template (required). Select etcd and Control Plane (required). On the Cluster Options configure Cloud Provider to Harvester if you want to use the Harvester Cloud Provider and CSI Diver . Click Create .","title":"Create RKE1 Kubernetes Cluster"},{"location":"rancher/rke1-cluster/#using-harvester-rke1-node-driver-in-air-gapped-environment","text":"RKE1 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine, and docker to set up the RKE cluster. However, It may not be feasible to install qemu-guest-agent and docker in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent and docker installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent and docker via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 write_files: - path: /etc/environment content: | HTTP_PROXY=\"http://192.168.0.1:3128\" HTTPS_PROXY=\"http://192.168.0.1:3128\" append: true","title":"Using Harvester RKE1 Node Driver in Air Gapped Environment"},{"location":"rancher/rke2-cluster/","text":"Creating an RKE2 Kubernetes Cluster \u00b6 Users can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1+ using the built-in Harvester node driver. Note Harvester RKE2 node driver is in tech preview. VLAN network is required for Harvester node driver. Create Your Cloud Credentials \u00b6 Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name Select \"Imported Harvester\" or \"External Harvester\". Click Create . Create RKE2 Kubernetes Cluster \u00b6 Users can create a RKE2 Kubernetes cluster from the Cluster Management page via the RKE2 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE2/K3s . Select Harvester node driver. Select a Cloud Credential . Enter Cluster Name (required). Enter Namespace (required). Enter Image (required). Enter Network Name (required). Enter SSH User (required). Click Create . Note RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration. Currently only imported Harvester clusters are supported automatically. Add Node Affinity \u00b6 Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node Selector Set priority to Required if you wish the scheduler to schedule the machines only when the rules are met. Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key : topology.kubernetes.io/region operator : in list values : us-east-1 --- key : topology.kubernetes.io/zone operator : in list values : us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled accordingly to the affinity rules. Using Harvester RKE2 Node Driver in Air Gapped Environment \u00b6 RKE2 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine. However, it may not be feasible to install qemu-guest-agent in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128","title":"Creating an RKE2 Kubernetes Cluster"},{"location":"rancher/rke2-cluster/#creating-an-rke2-kubernetes-cluster","text":"Users can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1+ using the built-in Harvester node driver. Note Harvester RKE2 node driver is in tech preview. VLAN network is required for Harvester node driver.","title":"Creating an RKE2 Kubernetes Cluster"},{"location":"rancher/rke2-cluster/#create-your-cloud-credentials","text":"Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name Select \"Imported Harvester\" or \"External Harvester\". Click Create .","title":"Create Your Cloud Credentials"},{"location":"rancher/rke2-cluster/#create-rke2-kubernetes-cluster","text":"Users can create a RKE2 Kubernetes cluster from the Cluster Management page via the RKE2 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE2/K3s . Select Harvester node driver. Select a Cloud Credential . Enter Cluster Name (required). Enter Namespace (required). Enter Image (required). Enter Network Name (required). Enter SSH User (required). Click Create . Note RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration. Currently only imported Harvester clusters are supported automatically.","title":"Create RKE2 Kubernetes Cluster"},{"location":"rancher/rke2-cluster/#add-node-affinity","text":"Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node Selector Set priority to Required if you wish the scheduler to schedule the machines only when the rules are met. Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key : topology.kubernetes.io/region operator : in list values : us-east-1 --- key : topology.kubernetes.io/zone operator : in list values : us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled accordingly to the affinity rules.","title":"Add Node Affinity"},{"location":"rancher/rke2-cluster/#using-harvester-rke2-node-driver-in-air-gapped-environment","text":"RKE2 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine. However, it may not be feasible to install qemu-guest-agent in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128","title":"Using Harvester RKE2 Node Driver in Air Gapped Environment"},{"location":"rancher/virtualization-management/","text":"Virtualization Management \u00b6 For Harvester v0.3.0 and above, virtualization management with the multi-cluster management feature will be supported using Rancher v2.6 and above. As a prerequisite, Harvester v1.0.0 integration requires Rancher server v2.6.3 or above. In production, use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform) AWS Marketplace (uses Amazon EKS) Azure (uses Terraform) DigitalOcean (uses Terraform) GCP (uses Terraform) Hetzner Cloud (uses Terraform) Vagrant Equinix Metal Warning Do not install Rancher with Docker in production . Otherwise, your environment may be damaged and your cluster may not be recovered. Installing Rancher in Docker should only be used for quick evaluation and testing purposes. To install Rancher with Docker: Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM) An on-premises VM A bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection. From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart = unless-stopped -p 80 :80 -p 443 :443 --privileged rancher/rancher:v2.6.6 Once the Rancher server is up and running, log in and click the hamburger menu and choose the Virtualization Management tab. Select Import Existing to import the downstream Harvester cluster into the Rancher server. Specify the Cluster Name and click Create . You will then see the registration guide; please open the dashboard of the target Harvester cluster and follow the guide accordingly. Once the agent node is ready, you should be able to view and access the imported Harvester cluster from the Rancher server and manage your VMs accordingly. From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster management page. Multi-Tenancy \u00b6 In Harvester, we have leveraged the existing Rancher RBAC authorization such that users can view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants a user access to Rancher. As mentioned in Authentication , users can either be local or external. Once the user logs into Rancher, their authorization, also known as access rights, is determined by global permissions and cluster and project roles. Global Permissions : Define user authorization outside the scope of any particular cluster. Cluster and Project Roles : Define user authorization inside the specific cluster or project where users are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC . Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner has full control over the cluster and all resources inside it, e.g., hosts, VMs, volumes, images, networks, backups, and settings. A project user can be assigned to a specific project with permission to manage the resources inside the project. Multi-Tenancy Example \u00b6 The following example provides a good explanation of how the multi-tenant feature works: First, add new users via the Rancher Users & Authentication page. Then click Create to add two new separated users, such as project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project, e.g., the default project. A project-readonly is a user with read-only permission of a particular project, e.g., the default project. Click one of the imported Harvester clusters after navigating to the Harvester UI. Click the Projects/Namespaces tab. Select a project such as default and click the Edit Config menu to assign the users to this project with appropriate permissions. For example, the project-owner user will be assigned the project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click Save . Open an incognito browser and log in as project-owner . After logging in as the project-owner user, click the Virtualization Management tab. There you should be able to view the cluster to which you have been assigned. Click the Images tab to view a list of images previously uploaded to the harvester-public namespace. You can also upload your own image if needed. Create a VM with one of the images that you have uploaded. Log in with another user, e.g., project-readonly , and this user will only have the read permission of this project. Delete Imported Harvester Cluster \u00b6 Users can delete the imported Harvester cluster from the Rancher UI via Virtualization Management > Harvester Clusters . Select the cluster you want to remove and click the Delete button to delete the imported Harvester cluster. Warning Please do not run the kubectl delete -f ... command to delete the imported Harvester cluster as it will remove the entire cattle-system namespace which is required of the Harvester cluster.","title":"Virtualization management"},{"location":"rancher/virtualization-management/#virtualization-management","text":"For Harvester v0.3.0 and above, virtualization management with the multi-cluster management feature will be supported using Rancher v2.6 and above. As a prerequisite, Harvester v1.0.0 integration requires Rancher server v2.6.3 or above. In production, use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform) AWS Marketplace (uses Amazon EKS) Azure (uses Terraform) DigitalOcean (uses Terraform) GCP (uses Terraform) Hetzner Cloud (uses Terraform) Vagrant Equinix Metal Warning Do not install Rancher with Docker in production . Otherwise, your environment may be damaged and your cluster may not be recovered. Installing Rancher in Docker should only be used for quick evaluation and testing purposes. To install Rancher with Docker: Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM) An on-premises VM A bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection. From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart = unless-stopped -p 80 :80 -p 443 :443 --privileged rancher/rancher:v2.6.6 Once the Rancher server is up and running, log in and click the hamburger menu and choose the Virtualization Management tab. Select Import Existing to import the downstream Harvester cluster into the Rancher server. Specify the Cluster Name and click Create . You will then see the registration guide; please open the dashboard of the target Harvester cluster and follow the guide accordingly. Once the agent node is ready, you should be able to view and access the imported Harvester cluster from the Rancher server and manage your VMs accordingly. From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster management page.","title":"Virtualization Management"},{"location":"rancher/virtualization-management/#multi-tenancy","text":"In Harvester, we have leveraged the existing Rancher RBAC authorization such that users can view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants a user access to Rancher. As mentioned in Authentication , users can either be local or external. Once the user logs into Rancher, their authorization, also known as access rights, is determined by global permissions and cluster and project roles. Global Permissions : Define user authorization outside the scope of any particular cluster. Cluster and Project Roles : Define user authorization inside the specific cluster or project where users are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC . Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner has full control over the cluster and all resources inside it, e.g., hosts, VMs, volumes, images, networks, backups, and settings. A project user can be assigned to a specific project with permission to manage the resources inside the project.","title":"Multi-Tenancy"},{"location":"rancher/virtualization-management/#multi-tenancy-example","text":"The following example provides a good explanation of how the multi-tenant feature works: First, add new users via the Rancher Users & Authentication page. Then click Create to add two new separated users, such as project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project, e.g., the default project. A project-readonly is a user with read-only permission of a particular project, e.g., the default project. Click one of the imported Harvester clusters after navigating to the Harvester UI. Click the Projects/Namespaces tab. Select a project such as default and click the Edit Config menu to assign the users to this project with appropriate permissions. For example, the project-owner user will be assigned the project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click Save . Open an incognito browser and log in as project-owner . After logging in as the project-owner user, click the Virtualization Management tab. There you should be able to view the cluster to which you have been assigned. Click the Images tab to view a list of images previously uploaded to the harvester-public namespace. You can also upload your own image if needed. Create a VM with one of the images that you have uploaded. Log in with another user, e.g., project-readonly , and this user will only have the read permission of this project.","title":"Multi-Tenancy Example"},{"location":"rancher/virtualization-management/#delete-imported-harvester-cluster","text":"Users can delete the imported Harvester cluster from the Rancher UI via Virtualization Management > Harvester Clusters . Select the cluster you want to remove and click the Delete button to delete the imported Harvester cluster. Warning Please do not run the kubectl delete -f ... command to delete the imported Harvester cluster as it will remove the entire cattle-system namespace which is required of the Harvester cluster.","title":"Delete Imported Harvester Cluster"},{"location":"reference/api/","text":"API Reference \u00b6 SwaggerUIBundle({ url: 'swagger.json', dom_id: '#swagger-ui', })","title":"API Reference"},{"location":"reference/api/#api-reference","text":"SwaggerUIBundle({ url: 'swagger.json', dom_id: '#swagger-ui', })","title":"API Reference"},{"location":"settings/settings/","text":"Settings \u00b6 This page contains a list of advanced settings which can be used in Harvester. You can modify the custom resource settings.harvesterhci.io from the Dashboard UI or with the kubectl command. additional-ca \u00b6 This setting allows you to configure additional trusted CA certificates for Harvester to access external services. Default: none Example \u00b6 -----BEGIN CERTIFICATE----- SOME-CA-CERTIFICATES -----END CERTIFICATE----- Warning Changing this setting might cause a short downtime for single-node clusters. backup-target \u00b6 This setting allows you to set a custom backup target to store VM backups. It supports NFS and S3. For further information, please refer to the Longhorn documentation . Default: none Example \u00b6 { \"type\" : \"s3\" , \"endpoint\" : \"https://s3.endpoint.svc\" , \"accessKeyId\" : \"test-access-key-id\" , \"secretAccessKey\" : \"test-access-key\" , \"bucketName\" : \"test-bup\" , \"bucketRegion\" : \"us\u2011east\u20112\" , \"cert\" : \"\" , \"virtualHostedStyle\" : false } cluster-registration-url \u00b6 This setting allows you to import the Harvester cluster to Rancher for multi-cluster management. Default: none Example \u00b6 https://172.16.0.1/v3/import/w6tp7dgwjj549l88pr7xmxb4x6m54v5kcplvhbp9vv2wzqrrjhrc7c_c-m-zxbbbck9.yaml http-proxy \u00b6 This setting allows you to configure an HTTP proxy to access external services, including the download of images and backup to s3 services. Default: {} The following options and values can be set: Proxy URL for HTTP requests: \"httpProxy\": \"http://<username>:<pswd>@<ip>:<port>\" Proxy URL for HTTPS requests: \"httpsProxy\": \"https://<username>:<pswd>@<ip>:<port>\" Comma-separated list of hostnames and/or CIDRs: \"noProxy\": \"<hostname | CIDR>\" Example \u00b6 { \"httpProxy\" : \"http://my.proxy\" , \"httpsProxy\" : \"https://my.proxy\" , \"noProxy\" : \"some.internal.svc,172.16.0.0/16\" } Note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,.svc,.cluster.local Warning Changing this setting might cause a short downtime for single-node clusters. log-level \u00b6 This setting allows you to configure the log level for the Harvester server. Default: info The following values can be set. The list goes from the least to most verbose log level: panic fatal error warn , warning info debug trace Example \u00b6 debug overcommit-config \u00b6 This setting allows you to configure the percentage for resources overcommit on CPU, memory, and storage. By setting resources overcommit, this will permit to schedule additional virtual machines even if the the physical resources are already fully utilized. Default: { \"cpu\":1600, \"memory\":150, \"storage\":200 } The default CPU overcommit with 1600% means, for example, if the CPU resources limit of a virtual machine is 1600m core, Harvester would only request 100m CPU for it from Kubernetes scheduler. Example \u00b6 { \"cpu\" : 1000 , \"memory\" : 200 , \"storage\" : 300 } release-download-url \u00b6 Available as of v1.0.1 This setting allows you to configure the upgrade release download URL address. Harvester will get the ISO URL and checksum value from the ${URL}/${VERSION}/version.yaml file hosted by the configured URL. Default: https://releases.rancher.com/harvester Example of the version.yaml \u00b6 apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: ${VERSION} namespace: harvester-system spec: isoChecksum: ${ISO_CHECKSUM} isoURL: ${ISO_URL} server-version \u00b6 This setting displays the version of Harvester server. Example \u00b6 v1.0.0-abcdef-head ssl-certificates \u00b6 This setting allows you to configure serving certificates for Harvester UI/API. Default: {} Example \u00b6 { \"ca\" : \"-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----\" , \"publicCertificate\" : \"-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----\" , \"privateKey\" : \"-----BEGIN RSA PRIVATE KEY-----\\nSOME-PRIVATE-KEY-ENCODED-IN-PEM-FORMAT\\n-----END RSA PRIVATE KEY-----\" } Warning Changing this setting might cause a short downtime on single-node clusters. ssl-parameters \u00b6 This setting allows you to change the enabled SSL/TLS protocols and ciphers of Harvester GUI and API. The following options and values can be set: protocols : Enabled protocols. See NGINX Ingress Controller's configs ssl-protocols for supported input. ciphers : Enabled ciphers. See NGINX Ingress Controller's configs ssl-ciphers for supported input. If no value is provided, protocols is set to TLSv1.2 only and the ciphers list is ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305 . Default: none Note See Troubleshooting if you have misconfigured this setting and no longer have access to Harvester GUI and API. Example \u00b6 The following example sets the enabled SSL/TLS protocols to TLSv1.2 and TLSv1.3 and the ciphers list to ECDHE-ECDSA-AES128-GCM-SHA256 and ECDHE-ECDSA-CHACHA20-POLY1305 . { \"protocols\": \"TLSv1.2 TLSv1.3\", \"ciphers\": \"ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-CHACHA20-POLY1305\" } ui-index \u00b6 This setting allows you to configure HTML index location for the UI. Default: https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Example \u00b6 https://your.static.dashboard-ui/index.html ui-source \u00b6 This setting allows you to configure how to load the UI source. The following values can be set: auto : The default. Auto-detect whether to use bundled UI or not. external : Use external UI source. bundled : Use the bundled UI source. Example \u00b6 external upgrade-checker-enabled \u00b6 This setting allows you to automatically check if there's an upgrade available for Harvester. Default: true Example \u00b6 false upgrade-checker-url \u00b6 This setting allows you to configure the URL for the upgrade check of Harvester. Can only be used if the upgrade-checker-enabled setting is set to true. Default: https://harvester-upgrade-responder.rancher.io/v1/checkupgrade Example \u00b6 https://your.upgrade.checker-url/v99/checkupgrade auto-disk-provision-paths [Experimental] \u00b6 This setting allows Harvester to automatically add disks that match the given glob pattern as VM storage. It's possible to provide multiple patterns by separating them with a comma. Warning This setting is applied to every Node in the cluster. All the data in these storage devices will be destroyed . Use at your own risk. Default: none Example \u00b6 The following example will add disks matching the glob pattern /dev/sd* or /dev/hd* : /dev/sd*,/dev/hd* vm-force-reset-policy \u00b6 This setting allows you to force reschedule VMs when a node is unavailable. When a node turns to be Not Ready , it will force delete the VM on that node and reschedule it to another available node after a period of seconds. Default: {\"enable\":true, \"period\":300} Example \u00b6 { \"enable\" : \"true\" , \"period\" : 300 }","title":"Settings"},{"location":"settings/settings/#settings","text":"This page contains a list of advanced settings which can be used in Harvester. You can modify the custom resource settings.harvesterhci.io from the Dashboard UI or with the kubectl command.","title":"Settings"},{"location":"settings/settings/#additional-ca","text":"This setting allows you to configure additional trusted CA certificates for Harvester to access external services. Default: none","title":"additional-ca"},{"location":"settings/settings/#example","text":"-----BEGIN CERTIFICATE----- SOME-CA-CERTIFICATES -----END CERTIFICATE----- Warning Changing this setting might cause a short downtime for single-node clusters.","title":"Example"},{"location":"settings/settings/#backup-target","text":"This setting allows you to set a custom backup target to store VM backups. It supports NFS and S3. For further information, please refer to the Longhorn documentation . Default: none","title":"backup-target"},{"location":"settings/settings/#example_1","text":"{ \"type\" : \"s3\" , \"endpoint\" : \"https://s3.endpoint.svc\" , \"accessKeyId\" : \"test-access-key-id\" , \"secretAccessKey\" : \"test-access-key\" , \"bucketName\" : \"test-bup\" , \"bucketRegion\" : \"us\u2011east\u20112\" , \"cert\" : \"\" , \"virtualHostedStyle\" : false }","title":"Example"},{"location":"settings/settings/#cluster-registration-url","text":"This setting allows you to import the Harvester cluster to Rancher for multi-cluster management. Default: none","title":"cluster-registration-url"},{"location":"settings/settings/#example_2","text":"https://172.16.0.1/v3/import/w6tp7dgwjj549l88pr7xmxb4x6m54v5kcplvhbp9vv2wzqrrjhrc7c_c-m-zxbbbck9.yaml","title":"Example"},{"location":"settings/settings/#http-proxy","text":"This setting allows you to configure an HTTP proxy to access external services, including the download of images and backup to s3 services. Default: {} The following options and values can be set: Proxy URL for HTTP requests: \"httpProxy\": \"http://<username>:<pswd>@<ip>:<port>\" Proxy URL for HTTPS requests: \"httpsProxy\": \"https://<username>:<pswd>@<ip>:<port>\" Comma-separated list of hostnames and/or CIDRs: \"noProxy\": \"<hostname | CIDR>\"","title":"http-proxy"},{"location":"settings/settings/#example_3","text":"{ \"httpProxy\" : \"http://my.proxy\" , \"httpsProxy\" : \"https://my.proxy\" , \"noProxy\" : \"some.internal.svc,172.16.0.0/16\" } Note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,.svc,.cluster.local Warning Changing this setting might cause a short downtime for single-node clusters.","title":"Example"},{"location":"settings/settings/#log-level","text":"This setting allows you to configure the log level for the Harvester server. Default: info The following values can be set. The list goes from the least to most verbose log level: panic fatal error warn , warning info debug trace","title":"log-level"},{"location":"settings/settings/#example_4","text":"debug","title":"Example"},{"location":"settings/settings/#overcommit-config","text":"This setting allows you to configure the percentage for resources overcommit on CPU, memory, and storage. By setting resources overcommit, this will permit to schedule additional virtual machines even if the the physical resources are already fully utilized. Default: { \"cpu\":1600, \"memory\":150, \"storage\":200 } The default CPU overcommit with 1600% means, for example, if the CPU resources limit of a virtual machine is 1600m core, Harvester would only request 100m CPU for it from Kubernetes scheduler.","title":"overcommit-config"},{"location":"settings/settings/#example_5","text":"{ \"cpu\" : 1000 , \"memory\" : 200 , \"storage\" : 300 }","title":"Example"},{"location":"settings/settings/#release-download-url","text":"Available as of v1.0.1 This setting allows you to configure the upgrade release download URL address. Harvester will get the ISO URL and checksum value from the ${URL}/${VERSION}/version.yaml file hosted by the configured URL. Default: https://releases.rancher.com/harvester","title":"release-download-url"},{"location":"settings/settings/#example-of-the-versionyaml","text":"apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: ${VERSION} namespace: harvester-system spec: isoChecksum: ${ISO_CHECKSUM} isoURL: ${ISO_URL}","title":"Example of the version.yaml"},{"location":"settings/settings/#server-version","text":"This setting displays the version of Harvester server.","title":"server-version"},{"location":"settings/settings/#example_6","text":"v1.0.0-abcdef-head","title":"Example"},{"location":"settings/settings/#ssl-certificates","text":"This setting allows you to configure serving certificates for Harvester UI/API. Default: {}","title":"ssl-certificates"},{"location":"settings/settings/#example_7","text":"{ \"ca\" : \"-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----\" , \"publicCertificate\" : \"-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----\" , \"privateKey\" : \"-----BEGIN RSA PRIVATE KEY-----\\nSOME-PRIVATE-KEY-ENCODED-IN-PEM-FORMAT\\n-----END RSA PRIVATE KEY-----\" } Warning Changing this setting might cause a short downtime on single-node clusters.","title":"Example"},{"location":"settings/settings/#ssl-parameters","text":"This setting allows you to change the enabled SSL/TLS protocols and ciphers of Harvester GUI and API. The following options and values can be set: protocols : Enabled protocols. See NGINX Ingress Controller's configs ssl-protocols for supported input. ciphers : Enabled ciphers. See NGINX Ingress Controller's configs ssl-ciphers for supported input. If no value is provided, protocols is set to TLSv1.2 only and the ciphers list is ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305 . Default: none Note See Troubleshooting if you have misconfigured this setting and no longer have access to Harvester GUI and API.","title":"ssl-parameters"},{"location":"settings/settings/#example_8","text":"The following example sets the enabled SSL/TLS protocols to TLSv1.2 and TLSv1.3 and the ciphers list to ECDHE-ECDSA-AES128-GCM-SHA256 and ECDHE-ECDSA-CHACHA20-POLY1305 . { \"protocols\": \"TLSv1.2 TLSv1.3\", \"ciphers\": \"ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-CHACHA20-POLY1305\" }","title":"Example"},{"location":"settings/settings/#ui-index","text":"This setting allows you to configure HTML index location for the UI. Default: https://releases.rancher.com/harvester-ui/dashboard/latest/index.html","title":"ui-index"},{"location":"settings/settings/#example_9","text":"https://your.static.dashboard-ui/index.html","title":"Example"},{"location":"settings/settings/#ui-source","text":"This setting allows you to configure how to load the UI source. The following values can be set: auto : The default. Auto-detect whether to use bundled UI or not. external : Use external UI source. bundled : Use the bundled UI source.","title":"ui-source"},{"location":"settings/settings/#example_10","text":"external","title":"Example"},{"location":"settings/settings/#upgrade-checker-enabled","text":"This setting allows you to automatically check if there's an upgrade available for Harvester. Default: true","title":"upgrade-checker-enabled"},{"location":"settings/settings/#example_11","text":"false","title":"Example"},{"location":"settings/settings/#upgrade-checker-url","text":"This setting allows you to configure the URL for the upgrade check of Harvester. Can only be used if the upgrade-checker-enabled setting is set to true. Default: https://harvester-upgrade-responder.rancher.io/v1/checkupgrade","title":"upgrade-checker-url"},{"location":"settings/settings/#example_12","text":"https://your.upgrade.checker-url/v99/checkupgrade","title":"Example"},{"location":"settings/settings/#auto-disk-provision-paths-experimental","text":"This setting allows Harvester to automatically add disks that match the given glob pattern as VM storage. It's possible to provide multiple patterns by separating them with a comma. Warning This setting is applied to every Node in the cluster. All the data in these storage devices will be destroyed . Use at your own risk. Default: none","title":"auto-disk-provision-paths [Experimental]"},{"location":"settings/settings/#example_13","text":"The following example will add disks matching the glob pattern /dev/sd* or /dev/hd* : /dev/sd*,/dev/hd*","title":"Example"},{"location":"settings/settings/#vm-force-reset-policy","text":"This setting allows you to force reschedule VMs when a node is unavailable. When a node turns to be Not Ready , it will force delete the VM on that node and reschedule it to another available node after a period of seconds. Default: {\"enable\":true, \"period\":300}","title":"vm-force-reset-policy"},{"location":"settings/settings/#example_14","text":"{ \"enable\" : \"true\" , \"period\" : 300 }","title":"Example"},{"location":"terraform/terraform/","text":"Harvester Terraform Provider \u00b6 Requirements \u00b6 Terraform >= 0.13.x Go 1.18 to build the provider plugin Install The Provider \u00b6 copy and paste this code into your Terraform configuration. Then, run terraform init to initialize it. terraform { required_providers { harvester = { source = \"harvester/harvester\" version = \"<replace to the latest release version>\" } } } provider \"harvester\" { # Configuration options } Using the provider \u00b6 More details about the provider-specific configurations can be found in the docs . Github Repo: https://github.com/harvester/terraform-provider-harvester","title":"Harvester Terraform Provider"},{"location":"terraform/terraform/#harvester-terraform-provider","text":"","title":"Harvester Terraform Provider"},{"location":"terraform/terraform/#requirements","text":"Terraform >= 0.13.x Go 1.18 to build the provider plugin","title":"Requirements"},{"location":"terraform/terraform/#install-the-provider","text":"copy and paste this code into your Terraform configuration. Then, run terraform init to initialize it. terraform { required_providers { harvester = { source = \"harvester/harvester\" version = \"<replace to the latest release version>\" } } } provider \"harvester\" { # Configuration options }","title":"Install The Provider"},{"location":"terraform/terraform/#using-the-provider","text":"More details about the provider-specific configurations can be found in the docs . Github Repo: https://github.com/harvester/terraform-provider-harvester","title":"Using the provider"},{"location":"troubleshooting/harvester/","text":"Generate a support bundle \u00b6 Users can generate a support bundle in the Harvester GUI with the following steps: Click the Support link at the bottom-left of Harvester Web UI. Click Generate Support Bundle button. Enter a useful description for the support bundle and click Create to generate and download a support bundle. Access Embedded Rancher \u00b6 You can access the embedded Rancher dashboard via https://{{HARVESTER_IP}}/dashboard/c/local/explorer . Note We only support to use the embedded Rancher dashboard for debugging and validation purpose. For Rancher's multi-cluster and multi-tenant integration, please refer to the docs here . Access Embedded Longhorn \u00b6 You can access the embedded Longhorn UI via https://{{HARVESTER_IP}}/dashboard/c/local/longhorn . Note We only support to use the embedded Longhorn UI for debugging and validation purpose . I can't access Harvester after I changed SSL/TLS enabled protocols and ciphers \u00b6 If you changed SSL/TLS enabled protocols and ciphers settings and you no longer have access to Harvester GUI and API, it's highly possible that NGINX Ingress Controller has stopped working due to the misconfigured SSL/TLS protocols and ciphers. Follow these steps to reset the setting: Following FAQ to SSH into Harvester node and switch to root user. $ sudo -s Editing setting ssl-parameters manually using kubectl : # kubectl edit settings ssl-parameters Deleting the line value: ... so that NGINX Ingress Controller will use the default protocols and ciphers. apiVersion: harvesterhci.io/v1beta1 default: '{}' kind: Setting metadata: name: ssl-parameters ... value: '{\"protocols\":\"TLS99\",\"ciphers\":\"WRONG_CIPHER\"}' # <- Delete this line Save the change and you should see the following response after exit from the editor: setting.harvesterhci.io/ssl-parameters edited You can further check the logs of Pod rke2-ingress-nginx-controller to see if NGINX Ingress Controller is working correctly.","title":"Harvester"},{"location":"troubleshooting/harvester/#generate-a-support-bundle","text":"Users can generate a support bundle in the Harvester GUI with the following steps: Click the Support link at the bottom-left of Harvester Web UI. Click Generate Support Bundle button. Enter a useful description for the support bundle and click Create to generate and download a support bundle.","title":"Generate a support bundle"},{"location":"troubleshooting/harvester/#access-embedded-rancher","text":"You can access the embedded Rancher dashboard via https://{{HARVESTER_IP}}/dashboard/c/local/explorer . Note We only support to use the embedded Rancher dashboard for debugging and validation purpose. For Rancher's multi-cluster and multi-tenant integration, please refer to the docs here .","title":"Access Embedded Rancher"},{"location":"troubleshooting/harvester/#access-embedded-longhorn","text":"You can access the embedded Longhorn UI via https://{{HARVESTER_IP}}/dashboard/c/local/longhorn . Note We only support to use the embedded Longhorn UI for debugging and validation purpose .","title":"Access Embedded Longhorn"},{"location":"troubleshooting/harvester/#i-cant-access-harvester-after-i-changed-ssltls-enabled-protocols-and-ciphers","text":"If you changed SSL/TLS enabled protocols and ciphers settings and you no longer have access to Harvester GUI and API, it's highly possible that NGINX Ingress Controller has stopped working due to the misconfigured SSL/TLS protocols and ciphers. Follow these steps to reset the setting: Following FAQ to SSH into Harvester node and switch to root user. $ sudo -s Editing setting ssl-parameters manually using kubectl : # kubectl edit settings ssl-parameters Deleting the line value: ... so that NGINX Ingress Controller will use the default protocols and ciphers. apiVersion: harvesterhci.io/v1beta1 default: '{}' kind: Setting metadata: name: ssl-parameters ... value: '{\"protocols\":\"TLS99\",\"ciphers\":\"WRONG_CIPHER\"}' # <- Delete this line Save the change and you should see the following response after exit from the editor: setting.harvesterhci.io/ssl-parameters edited You can further check the logs of Pod rke2-ingress-nginx-controller to see if NGINX Ingress Controller is working correctly.","title":"I can't access Harvester after I changed SSL/TLS enabled protocols and ciphers"},{"location":"troubleshooting/installation/","text":"Installation \u00b6 The following sections contain tips to troubleshoot or get assistance with failed installations. Logging into the Harvester Installer (a live OS) \u00b6 Users can press the key combination CTRL + ALT + F2 to switch to another TTY and log in with the following credentials: User: rancher Password: rancher Meeting hardware requirements \u00b6 Check that your hardware meets the minimum requirements to complete installation. Receiving the message \"Loading images. This may take a few minutes...\" \u00b6 Because the system doesn't have a default route, your installer may become \"stuck\" in this state. You can check your route status by executing the following command: $ ip route default via 10 .10.0.10 dev harvester-mgmt proto dhcp <-- Does a default route exist? 10 .10.0.0/24 dev harvester-mgmt proto kernel scope link src 10 .10.0.15 Check that your DHCP server offers a default route option. Attaching content from /run/cos/target/rke2.log is helpful too. Modifying cluster token on agent nodes \u00b6 When an agent node fails to join the cluster, it can be related to the cluster token not being identical to the server node token. In order to confirm the issue, connect to your agent node (i.e. with SSH ) and check the rancherd service log with the following command: $ sudo journalctl -b -u rancherd If the cluster token setup in the agent node is not matching the server node token, you will find several entries of the following message: msg = \"Bootstrapping Rancher (master-head/v1.21.5+rke2r1)\" msg = \"failed to bootstrap system, will retry: generating plan: insecure cacerts download from https://192.168.122.115:443/cacerts: Get \\\"https://192.168.122.115:443/cacerts\\\": EOF\" To fix the issue, you need to update the token value in the rancherd configuration file /etc/rancher/rancherd/config.yaml . For example, if the cluster token setup in the server node is ThisIsTheCorrectOne , you will update the token value as follow: token : 'ThisIsTheCorrectOne' To ensure the change is persistent across reboots, update the token value of the OS configuration file /oem/99_custom.yaml : name : Harvester Configuration stages : ... initramfs : - commands : - rm -f /etc/sysconfig/network/ifroute-harvester-mgmt files : - path : /etc/rancher/rancherd/config.yaml permissions : 384 owner : 0 group : 0 content : | role: cluster-init token: 'ThisIsTheCorrectOne' # <- Update this value kubernetesVersion: v1.21.5+rke2r1 labels: - harvesterhci.io/managed=true encoding : \"\" ownerstring : \"\" Note To see what is the current cluster token value, log in your server node (i.e. with SSH) and look in the file /etc/rancher/rancherd/config.yaml . For example, you can run the following command to only display the token's value: $ sudo yq eval .token /etc/rancher/rancherd/config.yaml Collecting troubleshooting information \u00b6 Please include the following information in a bug report when reporting a failed installation: A failed installation screenshot. System information and logs. Available as of v1.0.2 Please follow the guide in Logging into the Harvester Installer (a live OS) to log in. And run the command to generate a tarball that contains troubleshooting information: supportconfig -k -c The command output messages contain the generated tarball path. For example the path is /var/loq/scc_aaa_220520_1021 804d65d-c9ba-4c54-b12d-859631f892c5.txz in the following example: Note A failure PXE Boot installation automatically generates a tarball if the install.debug field is set to true in the Harvester configuration file. Before v1.0.2 Please help capture the content of these files: /var/log/console.log /run/cos/target/rke2.log /tmp/harvester.* /tmp/cos.* And output of these commands: blkid dmesg","title":"Installation"},{"location":"troubleshooting/installation/#installation","text":"The following sections contain tips to troubleshoot or get assistance with failed installations.","title":"Installation"},{"location":"troubleshooting/installation/#logging-into-the-harvester-installer-a-live-os","text":"Users can press the key combination CTRL + ALT + F2 to switch to another TTY and log in with the following credentials: User: rancher Password: rancher","title":"Logging into the Harvester Installer (a live OS)"},{"location":"troubleshooting/installation/#meeting-hardware-requirements","text":"Check that your hardware meets the minimum requirements to complete installation.","title":"Meeting hardware requirements"},{"location":"troubleshooting/installation/#receiving-the-message-loading-images-this-may-take-a-few-minutes","text":"Because the system doesn't have a default route, your installer may become \"stuck\" in this state. You can check your route status by executing the following command: $ ip route default via 10 .10.0.10 dev harvester-mgmt proto dhcp <-- Does a default route exist? 10 .10.0.0/24 dev harvester-mgmt proto kernel scope link src 10 .10.0.15 Check that your DHCP server offers a default route option. Attaching content from /run/cos/target/rke2.log is helpful too.","title":"Receiving the message \"Loading images. This may take a few minutes...\""},{"location":"troubleshooting/installation/#modifying-cluster-token-on-agent-nodes","text":"When an agent node fails to join the cluster, it can be related to the cluster token not being identical to the server node token. In order to confirm the issue, connect to your agent node (i.e. with SSH ) and check the rancherd service log with the following command: $ sudo journalctl -b -u rancherd If the cluster token setup in the agent node is not matching the server node token, you will find several entries of the following message: msg = \"Bootstrapping Rancher (master-head/v1.21.5+rke2r1)\" msg = \"failed to bootstrap system, will retry: generating plan: insecure cacerts download from https://192.168.122.115:443/cacerts: Get \\\"https://192.168.122.115:443/cacerts\\\": EOF\" To fix the issue, you need to update the token value in the rancherd configuration file /etc/rancher/rancherd/config.yaml . For example, if the cluster token setup in the server node is ThisIsTheCorrectOne , you will update the token value as follow: token : 'ThisIsTheCorrectOne' To ensure the change is persistent across reboots, update the token value of the OS configuration file /oem/99_custom.yaml : name : Harvester Configuration stages : ... initramfs : - commands : - rm -f /etc/sysconfig/network/ifroute-harvester-mgmt files : - path : /etc/rancher/rancherd/config.yaml permissions : 384 owner : 0 group : 0 content : | role: cluster-init token: 'ThisIsTheCorrectOne' # <- Update this value kubernetesVersion: v1.21.5+rke2r1 labels: - harvesterhci.io/managed=true encoding : \"\" ownerstring : \"\" Note To see what is the current cluster token value, log in your server node (i.e. with SSH) and look in the file /etc/rancher/rancherd/config.yaml . For example, you can run the following command to only display the token's value: $ sudo yq eval .token /etc/rancher/rancherd/config.yaml","title":"Modifying cluster token on agent nodes"},{"location":"troubleshooting/installation/#collecting-troubleshooting-information","text":"Please include the following information in a bug report when reporting a failed installation: A failed installation screenshot. System information and logs. Available as of v1.0.2 Please follow the guide in Logging into the Harvester Installer (a live OS) to log in. And run the command to generate a tarball that contains troubleshooting information: supportconfig -k -c The command output messages contain the generated tarball path. For example the path is /var/loq/scc_aaa_220520_1021 804d65d-c9ba-4c54-b12d-859631f892c5.txz in the following example: Note A failure PXE Boot installation automatically generates a tarball if the install.debug field is set to true in the Harvester configuration file. Before v1.0.2 Please help capture the content of these files: /var/log/console.log /run/cos/target/rke2.log /tmp/harvester.* /tmp/cos.* And output of these commands: blkid dmesg","title":"Collecting troubleshooting information"},{"location":"troubleshooting/monitoring/","text":"Monitoring \u00b6 The following sections contain tips to troubleshoot Harvester Monitoring. Monitoring is unusable \u00b6 When the Harvester Dashboard is not showing any monitoring metrics, it can be caused by the following reasons. Monitoring is unusable due to Pod being stuck in Terminating status \u00b6 Harvester Monitoring pods are deployed randomly on the cluster Nodes. When the Node hosting the pods accidentally goes down, the related pods may become stuck in the Terminating status rendering the Monitoring unusable from the WebUI. $ kubectl get pods -n cattle-monitoring-system NAMESPACE NAME READY STATUS RESTARTS AGE cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 3 /3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 0 /1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-crd-create-9wtzf 0 /1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz 3 /3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz 0 /3 Init:0/2 0 132m cattle-monitoring-system rancher-monitoring-kube-state-metrics-5bc8bb48bd-nbd92 1 /1 Running 4 4d1h ... Monitoring can be recovered using CLI commands to force delete the related pods. The cluster will redeploy new pods to replace them. # Delete each none-running Pod in namespace cattle-monitoring-system. $ kubectl delete pod --force -n cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 pod \"prometheus-rancher-monitoring-prometheus-0\" force deleted $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-crd-create-9wtzf $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz Wait for a few minutes so that the new pods are created and readied for the Monitoring dashboard to be usable again. $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 0/3 Init:0/1 0 98s rancher-monitoring-grafana-d9c56d79b-cp86w 0/3 Init:0/2 0 27s ... $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 3/3 Running 0 7m57s rancher-monitoring-grafana-d9c56d79b-cp86w 3/3 Running 0 6m46s ...","title":"Monitoring"},{"location":"troubleshooting/monitoring/#monitoring","text":"The following sections contain tips to troubleshoot Harvester Monitoring.","title":"Monitoring"},{"location":"troubleshooting/monitoring/#monitoring-is-unusable","text":"When the Harvester Dashboard is not showing any monitoring metrics, it can be caused by the following reasons.","title":"Monitoring is unusable"},{"location":"troubleshooting/monitoring/#monitoring-is-unusable-due-to-pod-being-stuck-in-terminating-status","text":"Harvester Monitoring pods are deployed randomly on the cluster Nodes. When the Node hosting the pods accidentally goes down, the related pods may become stuck in the Terminating status rendering the Monitoring unusable from the WebUI. $ kubectl get pods -n cattle-monitoring-system NAMESPACE NAME READY STATUS RESTARTS AGE cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 3 /3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 0 /1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-crd-create-9wtzf 0 /1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz 3 /3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz 0 /3 Init:0/2 0 132m cattle-monitoring-system rancher-monitoring-kube-state-metrics-5bc8bb48bd-nbd92 1 /1 Running 4 4d1h ... Monitoring can be recovered using CLI commands to force delete the related pods. The cluster will redeploy new pods to replace them. # Delete each none-running Pod in namespace cattle-monitoring-system. $ kubectl delete pod --force -n cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 pod \"prometheus-rancher-monitoring-prometheus-0\" force deleted $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-crd-create-9wtzf $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz Wait for a few minutes so that the new pods are created and readied for the Monitoring dashboard to be usable again. $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 0/3 Init:0/1 0 98s rancher-monitoring-grafana-d9c56d79b-cp86w 0/3 Init:0/2 0 27s ... $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 3/3 Running 0 7m57s rancher-monitoring-grafana-d9c56d79b-cp86w 3/3 Running 0 6m46s ...","title":"Monitoring is unusable due to Pod being stuck in Terminating status"},{"location":"troubleshooting/os/","text":"Operating System \u00b6 Harvester runs on an OpenSUSE-based OS. The OS is an artifact produced by the cOS toolkit . The following sections contain information and tips to help users troubleshoot OS-related issues. How to log into a Harvester node \u00b6 Users can log into a Harvester node with the username rancher and the password or SSH keypair provided during installation. The user rancher can execute privileged commands without entering a password: # Run a privileged command rancher@node1:~> sudo blkid # Or become root rancher@node1:~> sudo -i node1:~ # blkid How can I install packages? Why are some paths read-only? \u00b6 The OS file system, like a container image, is image-based and immutable except in some directories. To temporarily enable the read-write mode, please use the following steps: Warning Enabling read-write mode might break your system if files are modified. Please use it at your own risk. For version v0.3.0 , we need to apply a workaround first to make some directories non-overlaid after enabling read-write mode. On a running Harvester node, run the following command as root: cat > /oem/91_hack.yaml <<'EOF' name: \"Rootfs Layout Settings for debugrw\" stages: rootfs: - if: 'grep -q root=LABEL=COS_ACTIVE /proc/cmdline && grep -q rd.cos.debugrw /proc/cmdline' name: \"Layout configuration for debugrw\" environment_file: /run/cos/cos-layout.env environment: RW_PATHS: \" \" EOF Reboot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append rd.cos.debugrw to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. How to permanently edit kernel parameters \u00b6 Note The following steps are a workaround. Harvester will inform the community once a permanent resolution is in place. Re-mount state directory in rw mode: # blkid -L COS_STATE /dev/vda2 # mount -o remount,rw /dev/vda2 /run/initramfs/cos-state Edit the grub config file and append parameters to the linux (loop0)$kernel $kernelcmd line. The following example adds a nomodeset parameter: # vim /run/initramfs/cos-state/grub2/grub.cfg menuentry \"Harvester ea6e7f5-dirty\" --id cos { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd nomodeset initrd (loop0)$initramfs } Reboot for changes to take effect. How to change the default GRUB boot menu entry \u00b6 To change the default entry, first check the --id attribute of a menu entry, as in the following example: # cat /run/initramfs/cos-state/grub2/grub.cfg <...> menuentry \"Harvester ea6e7f5-dirty (debug)\" --id cos-debug { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img The id of the above entry is cos-debug . We can then set the default entry by: # grub2-editenv /oem/grubenv set saved_entry=cos-debug How to debug a system crash or hang \u00b6 Collect crash log \u00b6 If kernel panic traces are not recorded in the system log when a system crashes, one reliable way to locate the crash log is to use a serial console. To enable outputting of kernel messages to a serial console, please use the following steps: Boot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append console=ttyS0,115200n8 to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. Note Adjust the console options according to your environment. Make sure to append the console= string at the end of the line. Connect to the serial port to capture logs. Collect crash dumps \u00b6 For kernel panic crashes, you can use kdump to collect crash dumps. By default, the OS is booted without the kdump feature enabled. Users can enable the feature by selecting the debug menuentry when booting, as in the following example: When a system crashes, a crash dump will be stored in the /var/crash/<time> directory. Providing the crash dump to developers helps them to troubleshoot and resolve issues.","title":"Operating System"},{"location":"troubleshooting/os/#operating-system","text":"Harvester runs on an OpenSUSE-based OS. The OS is an artifact produced by the cOS toolkit . The following sections contain information and tips to help users troubleshoot OS-related issues.","title":"Operating System"},{"location":"troubleshooting/os/#how-to-log-into-a-harvester-node","text":"Users can log into a Harvester node with the username rancher and the password or SSH keypair provided during installation. The user rancher can execute privileged commands without entering a password: # Run a privileged command rancher@node1:~> sudo blkid # Or become root rancher@node1:~> sudo -i node1:~ # blkid","title":"How to log into a Harvester node"},{"location":"troubleshooting/os/#how-can-i-install-packages-why-are-some-paths-read-only","text":"The OS file system, like a container image, is image-based and immutable except in some directories. To temporarily enable the read-write mode, please use the following steps: Warning Enabling read-write mode might break your system if files are modified. Please use it at your own risk. For version v0.3.0 , we need to apply a workaround first to make some directories non-overlaid after enabling read-write mode. On a running Harvester node, run the following command as root: cat > /oem/91_hack.yaml <<'EOF' name: \"Rootfs Layout Settings for debugrw\" stages: rootfs: - if: 'grep -q root=LABEL=COS_ACTIVE /proc/cmdline && grep -q rd.cos.debugrw /proc/cmdline' name: \"Layout configuration for debugrw\" environment_file: /run/cos/cos-layout.env environment: RW_PATHS: \" \" EOF Reboot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append rd.cos.debugrw to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system.","title":"How can I install packages? Why are some paths read-only?"},{"location":"troubleshooting/os/#how-to-permanently-edit-kernel-parameters","text":"Note The following steps are a workaround. Harvester will inform the community once a permanent resolution is in place. Re-mount state directory in rw mode: # blkid -L COS_STATE /dev/vda2 # mount -o remount,rw /dev/vda2 /run/initramfs/cos-state Edit the grub config file and append parameters to the linux (loop0)$kernel $kernelcmd line. The following example adds a nomodeset parameter: # vim /run/initramfs/cos-state/grub2/grub.cfg menuentry \"Harvester ea6e7f5-dirty\" --id cos { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd nomodeset initrd (loop0)$initramfs } Reboot for changes to take effect.","title":"How to permanently edit kernel parameters"},{"location":"troubleshooting/os/#how-to-change-the-default-grub-boot-menu-entry","text":"To change the default entry, first check the --id attribute of a menu entry, as in the following example: # cat /run/initramfs/cos-state/grub2/grub.cfg <...> menuentry \"Harvester ea6e7f5-dirty (debug)\" --id cos-debug { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img The id of the above entry is cos-debug . We can then set the default entry by: # grub2-editenv /oem/grubenv set saved_entry=cos-debug","title":"How to change the default GRUB boot menu entry"},{"location":"troubleshooting/os/#how-to-debug-a-system-crash-or-hang","text":"","title":"How to debug a system crash or hang"},{"location":"troubleshooting/os/#collect-crash-log","text":"If kernel panic traces are not recorded in the system log when a system crashes, one reliable way to locate the crash log is to use a serial console. To enable outputting of kernel messages to a serial console, please use the following steps: Boot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append console=ttyS0,115200n8 to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. Note Adjust the console options according to your environment. Make sure to append the console= string at the end of the line. Connect to the serial port to capture logs.","title":"Collect crash log"},{"location":"troubleshooting/os/#collect-crash-dumps","text":"For kernel panic crashes, you can use kdump to collect crash dumps. By default, the OS is booted without the kdump feature enabled. Users can enable the feature by selecting the debug menuentry when booting, as in the following example: When a system crashes, a crash dump will be stored in the /var/crash/<time> directory. Providing the crash dump to developers helps them to troubleshoot and resolve issues.","title":"Collect crash dumps"},{"location":"upgrade/automatic/","text":"Upgrading Harvester \u00b6 Upgrade support matrix \u00b6 The following table shows the upgrade path of all supported versions. Upgrade from version Supported new version(s) v1.0.2 v1.0.3 v1.0.1 v1.0.2 v1.0.0 v1.0.1 Start an upgrade \u00b6 Note we are still working towards zero-downtime upgrade, due to some known issues please follow the steps below before you upgrade your Harvester cluster: Warning Before you upgrade your Harvester cluster, we highly recommend: Shutting down all your VMs (Harvester GUI -> Virtual Machines -> Select VMs -> Actions -> Stop). Back up your VMs. Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc. Make sure your hardware meets the preferred hardware requirements . This is due to there will be intermediate resources consumed by an upgrade. Make sure each node has at least 25 GB of free space ( df -h /usr/local/ ). Warning Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server on each node : $ sudo -i # Add time servers $ vim /etc/systemd/timesyncd.conf [ntp] NTP=0.pool.ntp.org # Enable and start the systemd-timesyncd $ timedatectl set-ntp true # Check status $ sudo timedatectl status Warning NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the knowledge base article for further information. Make sure to read the Warning paragraph at the top of this document first. Harvester checks if there are new upgradable versions periodically. If there are new versions, an upgrade button shows up on the Dashboard page. If the cluster is in an air-gapped environment, please see Prepare an air-gapped upgrade section first. You can also speed up the ISO download by using the approach in that section. Navigate to Harvester GUI and click the upgrade button on the Dashboard page. Select a version to start upgrading. Click the circle on the top to display the upgrade progress. Prepare an air-gapped upgrade \u00b6 Warning Make sure to check Upgrade support matrix section first about upgradable versions. Download a Harvester ISO file from release pages . Save the ISO to a local HTTP server. Assume the file is hosted at http://10.10.0.1/harvester.iso . Download the version file from release pages, for example, https://releases.rancher.com/harvester/{version}/version.yaml Replace isoURL value in the version.yaml file: apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: v1.0.2 namespace: harvester-system spec: isoChecksum: <SHA-512 checksum of the ISO> isoURL: http://10.10.0.1/harvester.iso # change to local ISO URL releaseDate: '20220512' Assume the file is hosted at http://10.10.0.1/version.yaml . Log in to one of your control plane nodes. Become root and create a version: rancher@node1:~> sudo -i rancher@node1:~> kubectl create -f http://10.10.0.1/version.yaml An upgrade button should show up on the Harvester GUI Dashboard page.","title":"Upgrading Harvester"},{"location":"upgrade/automatic/#upgrading-harvester","text":"","title":"Upgrading Harvester"},{"location":"upgrade/automatic/#upgrade-support-matrix","text":"The following table shows the upgrade path of all supported versions. Upgrade from version Supported new version(s) v1.0.2 v1.0.3 v1.0.1 v1.0.2 v1.0.0 v1.0.1","title":"Upgrade support matrix"},{"location":"upgrade/automatic/#start-an-upgrade","text":"Note we are still working towards zero-downtime upgrade, due to some known issues please follow the steps below before you upgrade your Harvester cluster: Warning Before you upgrade your Harvester cluster, we highly recommend: Shutting down all your VMs (Harvester GUI -> Virtual Machines -> Select VMs -> Actions -> Stop). Back up your VMs. Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc. Make sure your hardware meets the preferred hardware requirements . This is due to there will be intermediate resources consumed by an upgrade. Make sure each node has at least 25 GB of free space ( df -h /usr/local/ ). Warning Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server on each node : $ sudo -i # Add time servers $ vim /etc/systemd/timesyncd.conf [ntp] NTP=0.pool.ntp.org # Enable and start the systemd-timesyncd $ timedatectl set-ntp true # Check status $ sudo timedatectl status Warning NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the knowledge base article for further information. Make sure to read the Warning paragraph at the top of this document first. Harvester checks if there are new upgradable versions periodically. If there are new versions, an upgrade button shows up on the Dashboard page. If the cluster is in an air-gapped environment, please see Prepare an air-gapped upgrade section first. You can also speed up the ISO download by using the approach in that section. Navigate to Harvester GUI and click the upgrade button on the Dashboard page. Select a version to start upgrading. Click the circle on the top to display the upgrade progress.","title":"Start an upgrade"},{"location":"upgrade/automatic/#prepare-an-air-gapped-upgrade","text":"Warning Make sure to check Upgrade support matrix section first about upgradable versions. Download a Harvester ISO file from release pages . Save the ISO to a local HTTP server. Assume the file is hosted at http://10.10.0.1/harvester.iso . Download the version file from release pages, for example, https://releases.rancher.com/harvester/{version}/version.yaml Replace isoURL value in the version.yaml file: apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: v1.0.2 namespace: harvester-system spec: isoChecksum: <SHA-512 checksum of the ISO> isoURL: http://10.10.0.1/harvester.iso # change to local ISO URL releaseDate: '20220512' Assume the file is hosted at http://10.10.0.1/version.yaml . Log in to one of your control plane nodes. Become root and create a version: rancher@node1:~> sudo -i rancher@node1:~> kubectl create -f http://10.10.0.1/version.yaml An upgrade button should show up on the Harvester GUI Dashboard page.","title":"Prepare an air-gapped upgrade"},{"location":"upgrade/manual/","text":"Upgrading Harvester manually \u00b6 Manual Upgrade from v0.3.0 to v1.0.0 \u00b6 Warning Upgrading Harvester from v0.3.0 to v1.0.0 is not supported . Please use at your own risk. Overview \u00b6 The manual upgrade process consists of the following steps: Upgrade OS on each nodes. Create Harvester cluster repo. Upgrade the embedded Rancher service. Upgrade the RKE2 runtime. Upgrade the Harvester service. Preparation \u00b6 Backup your VMs. Get the new release \u00b6 Download the Harvester ISO image of a newer release from the Harvester GithHub release page and verify the checksum of the ISO. Select a controller node \u00b6 Some operations need to be run from a controller node which contains the cluster credentials. You can use the first node of the cluster or any node with the control-plane role: ssh rancher@<ip_of_the_first_node> sudo -i kubectl get nodes Example output: NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 116m v1.21.5+rke2r1 node2 Ready control-plane,etcd,master 112m v1.21.5+rke2r1 node3 Ready control-plane,etcd,master 112m v1.21.5+rke2r1 node4 Ready <none> 112m v1.21.5+rke2r1 Download the upgrade-helpers scripts to the controller node: sudo mkdir -p /usr/local/harvester-upgrade sudo chown rancher:rancher /usr/local/harvester-upgrade cd /usr/local/harvester-upgrade curl --proto '=https' --tlsv1.2 -sSfL https://github.com/harvester/upgrade-helpers/releases/latest/download/upgrade-helpers.tar.gz | tar xzvf - Shutdown all VMs: sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-stop-vms.sh Upgrade node OS \u00b6 You need to upgrade nodes one by one. Only proceed to the next node once the current node is done. For each node in the cluster: Download the upgrade-helpers scripts: sudo mkdir -p /usr/local/harvester-upgrade sudo chown rancher:rancher /usr/local/harvester-upgrade cd /usr/local/harvester-upgrade curl --proto '=https' --tlsv1.2 -sSfL https://github.com/harvester/upgrade-helpers/releases/latest/download/upgrade-helpers.tar.gz | tar xzvf - Upload the ISO to the node: $ scp <new-harvester-release-iso> rancher@<ip_of_node>:/usr/local/harvester-upgrade/harvester.iso Upgrade the node: sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-upgrade-node.sh /usr/local/harvester-upgrade/harvester.iso Reboot: sudo reboot Once the upgrade is complete, the node will reboot. Verify if the node is successfully upgraded: sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-node-post-check.sh Create Harvester cluster repo \u00b6 On the controller node: Check if all nodes are ready: sudo -i kubectl get nodes Create the Harvester cluster repo: sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-create-harvester-cluster-repo.sh Check if the Harvester cluster repo is ready: sudo -i kubectl get deployment harvester-cluster-repo -n cattle-system Example output: NAME READY UP-TO-DATE AVAILABLE AGE harvester-cluster-repo 1/1 1 1 59s Upgrade Rancher \u00b6 On the controller node : Check the new Rancher version: yq -e e '.rancher' /etc/harvester-release.yaml Example output: v2.6.3-harvester1 Upgrade Rancher: sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-upgrade-rancher.sh Verify if Rancher is successfully upgraded: sudo -i kubectl get settings.management.cattle.io server-version An example output: NAME VALUE server-version v2.6.3-harvester1 Upgrade RKE2 \u00b6 On the controller node : Check the new RKE2 version: yq -e e '.kubernetes' /etc/harvester-release.yaml Example output: v1.21.7+rke2r1 Upgrade RKE2: sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-upgrade-rke2.sh Wait for RKE2 to be upgraded in all nodes and make sure all nodes are ready (this will take some time to complete): sudo -i watch kubectl get nodes Example output: NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 3h6m v1.21.7+rke2r2 node2 Ready control-plane,etcd,master 3h3m v1.21.7+rke2r2 node3 Ready control-plane,etcd,master 3h3m v1.21.5+rke2r1 <--- not upgrade yet node4 Ready <none> 3h3m v1.21.5+rke2r1 <--- not upgrade yet Upgrade Harvester and Monitoring services \u00b6 On the controller node : sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-upgrade-harvester.sh","title":"Upgrading Harvester manually"},{"location":"upgrade/manual/#upgrading-harvester-manually","text":"","title":"Upgrading Harvester manually"},{"location":"upgrade/manual/#manual-upgrade-from-v030-to-v100","text":"Warning Upgrading Harvester from v0.3.0 to v1.0.0 is not supported . Please use at your own risk.","title":"Manual Upgrade from v0.3.0 to v1.0.0"},{"location":"upgrade/manual/#overview","text":"The manual upgrade process consists of the following steps: Upgrade OS on each nodes. Create Harvester cluster repo. Upgrade the embedded Rancher service. Upgrade the RKE2 runtime. Upgrade the Harvester service.","title":"Overview"},{"location":"upgrade/manual/#preparation","text":"Backup your VMs.","title":"Preparation"},{"location":"upgrade/manual/#get-the-new-release","text":"Download the Harvester ISO image of a newer release from the Harvester GithHub release page and verify the checksum of the ISO.","title":"Get the new release"},{"location":"upgrade/manual/#select-a-controller-node","text":"Some operations need to be run from a controller node which contains the cluster credentials. You can use the first node of the cluster or any node with the control-plane role: ssh rancher@<ip_of_the_first_node> sudo -i kubectl get nodes Example output: NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 116m v1.21.5+rke2r1 node2 Ready control-plane,etcd,master 112m v1.21.5+rke2r1 node3 Ready control-plane,etcd,master 112m v1.21.5+rke2r1 node4 Ready <none> 112m v1.21.5+rke2r1 Download the upgrade-helpers scripts to the controller node: sudo mkdir -p /usr/local/harvester-upgrade sudo chown rancher:rancher /usr/local/harvester-upgrade cd /usr/local/harvester-upgrade curl --proto '=https' --tlsv1.2 -sSfL https://github.com/harvester/upgrade-helpers/releases/latest/download/upgrade-helpers.tar.gz | tar xzvf - Shutdown all VMs: sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-stop-vms.sh","title":"Select a controller node"},{"location":"upgrade/manual/#upgrade-node-os","text":"You need to upgrade nodes one by one. Only proceed to the next node once the current node is done. For each node in the cluster: Download the upgrade-helpers scripts: sudo mkdir -p /usr/local/harvester-upgrade sudo chown rancher:rancher /usr/local/harvester-upgrade cd /usr/local/harvester-upgrade curl --proto '=https' --tlsv1.2 -sSfL https://github.com/harvester/upgrade-helpers/releases/latest/download/upgrade-helpers.tar.gz | tar xzvf - Upload the ISO to the node: $ scp <new-harvester-release-iso> rancher@<ip_of_node>:/usr/local/harvester-upgrade/harvester.iso Upgrade the node: sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-upgrade-node.sh /usr/local/harvester-upgrade/harvester.iso Reboot: sudo reboot Once the upgrade is complete, the node will reboot. Verify if the node is successfully upgraded: sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-node-post-check.sh","title":"Upgrade node OS"},{"location":"upgrade/manual/#create-harvester-cluster-repo","text":"On the controller node: Check if all nodes are ready: sudo -i kubectl get nodes Create the Harvester cluster repo: sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-create-harvester-cluster-repo.sh Check if the Harvester cluster repo is ready: sudo -i kubectl get deployment harvester-cluster-repo -n cattle-system Example output: NAME READY UP-TO-DATE AVAILABLE AGE harvester-cluster-repo 1/1 1 1 59s","title":"Create Harvester cluster repo"},{"location":"upgrade/manual/#upgrade-rancher","text":"On the controller node : Check the new Rancher version: yq -e e '.rancher' /etc/harvester-release.yaml Example output: v2.6.3-harvester1 Upgrade Rancher: sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-upgrade-rancher.sh Verify if Rancher is successfully upgraded: sudo -i kubectl get settings.management.cattle.io server-version An example output: NAME VALUE server-version v2.6.3-harvester1","title":"Upgrade Rancher"},{"location":"upgrade/manual/#upgrade-rke2","text":"On the controller node : Check the new RKE2 version: yq -e e '.kubernetes' /etc/harvester-release.yaml Example output: v1.21.7+rke2r1 Upgrade RKE2: sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-upgrade-rke2.sh Wait for RKE2 to be upgraded in all nodes and make sure all nodes are ready (this will take some time to complete): sudo -i watch kubectl get nodes Example output: NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 3h6m v1.21.7+rke2r2 node2 Ready control-plane,etcd,master 3h3m v1.21.7+rke2r2 node3 Ready control-plane,etcd,master 3h3m v1.21.5+rke2r1 <--- not upgrade yet node4 Ready <none> 3h3m v1.21.5+rke2r1 <--- not upgrade yet","title":"Upgrade RKE2"},{"location":"upgrade/manual/#upgrade-harvester-and-monitoring-services","text":"On the controller node : sudo -i /usr/local/harvester-upgrade/upgrade-helpers/bin/harv-upgrade-harvester.sh","title":"Upgrade Harvester and Monitoring services"},{"location":"upgrade/troubleshooting/","text":"Troubleshooting \u00b6 Overview \u00b6 Here are some tips to troubleshoot a failed upgrade: Check version-specific upgrade notes . You can click the version in the support matrix table to see if there are any known issues. Dive into the upgrade design proposal . The following section briefly describes phases within an upgrade and possible diagnostic methods. Diagnose the upgrade flow \u00b6 A Harvester upgrade process contains several phases. Phase 1: Provision upgrade repository VM. \u00b6 The Harvester controller downloads a Harvester release ISO file and uses it to provision a VM. During this phase you can see the upgrade status windows show: The time to complete the phase depends on the user's network speed and cluster resource utilization. We see failures in this phase due to network speed. If this happens, the user can start over the upgrade again. We can also check the repository VM (named with the format upgrade-repo-hvst-xxxx ) status and its corresponding pod: $ kubectl get vm -n harvester-system NAME AGE STATUS READY upgrade-repo-hvst-upgrade-9gmg2 101s Starting False $ kubectl get pods -n harvester-system | grep upgrade-repo-hvst virt-launcher-upgrade-repo-hvst-upgrade-9gmg2-4mnmq 1/1 Running 0 4m44s Phase 2: Preload container images \u00b6 The Harvester controller creates jobs on each Harvester node to download images from the repository VM and preload them. These are the container images required for the next release. During this stage you can see the upgrade status windows shows: It will take a while for all nodes to preload images. If the upgrade fails at this phase, the user can check job logs in the cattle-system namespace: $ kubectl get jobs -n cattle-system | grep prepare apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 0/1 47s 47s apply-hvst-upgrade-9gmg2-prepare-on-node4-with-2bbea1599a-041e4 1/1 2m3s 2m50s $ kubectl logs jobs/apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 -n cattle-system ... It's also safe to start over the upgrade if an upgrade fails at this phase. Phase 3: Upgrade system services \u00b6 In this phase, Harvester controller upgrades component Helm charts with a job. The user can check the apply-manifest job with the following command: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=manifest NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-apply-manifests 0/1 46s 46s $ kubectl logs jobs/hvst-upgrade-9gmg2-apply-manifests -n harvester-system ... Phase 4: Upgrade nodes \u00b6 The Harvester controller creates jobs on each node (one by one) to upgrade nodes' OSes and RKE2 runtime. For multi-node clusters, there are two kinds of jobs to update a node: pre-drain job: live-migrate or shutdown VMs on a node. When the job completes, the embedded Rancher service upgrades RKE2 runtime on a node. post-drain job: upgrade OS and reboot. For single-node clusters, there is only one single-node-upgrade type job for each node (named with the format hvst-upgrade-xxx-single-node-upgrade-<hostname> ). The user can check node jobs by: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=node NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-post-drain-node1 1/1 118s 6m34s hvst-upgrade-9gmg2-post-drain-node2 0/1 9s 9s hvst-upgrade-9gmg2-pre-drain-node1 1/1 3s 8m14s hvst-upgrade-9gmg2-pre-drain-node2 1/1 7s 85s $ kubectl logs -n harvester-system jobs/hvst-upgrade-9gmg2-post-drain-node2 ... Warning Please do not start over an upgrade if the upgrade fails at this phase. Phase 5: Clean-up \u00b6 The Harvester controller deletes the upgrade repository VM and all files that are no longer needed. Common operations \u00b6 Start over an upgrade \u00b6 Log in to a control plane node. List Upgrade CRs in the cluster: # become root $ sudo -i # list the on-going upgrade $ kubectl get upgrade.harvesterhci.io -n harvester-system -l harvesterhci.io/latestUpgrade=true NAME AGE hvst-upgrade-9gmg2 10m Delete the Upgrade CR $ kubectl delete upgrade.harvesterhci.io/hvst-upgrade-9gmg2 -n harvester-system Click the upgrade button in the Harvester dashboard to start an upgrade again.","title":"Troubleshooting"},{"location":"upgrade/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"upgrade/troubleshooting/#overview","text":"Here are some tips to troubleshoot a failed upgrade: Check version-specific upgrade notes . You can click the version in the support matrix table to see if there are any known issues. Dive into the upgrade design proposal . The following section briefly describes phases within an upgrade and possible diagnostic methods.","title":"Overview"},{"location":"upgrade/troubleshooting/#diagnose-the-upgrade-flow","text":"A Harvester upgrade process contains several phases.","title":"Diagnose the upgrade flow"},{"location":"upgrade/troubleshooting/#phase-1-provision-upgrade-repository-vm","text":"The Harvester controller downloads a Harvester release ISO file and uses it to provision a VM. During this phase you can see the upgrade status windows show: The time to complete the phase depends on the user's network speed and cluster resource utilization. We see failures in this phase due to network speed. If this happens, the user can start over the upgrade again. We can also check the repository VM (named with the format upgrade-repo-hvst-xxxx ) status and its corresponding pod: $ kubectl get vm -n harvester-system NAME AGE STATUS READY upgrade-repo-hvst-upgrade-9gmg2 101s Starting False $ kubectl get pods -n harvester-system | grep upgrade-repo-hvst virt-launcher-upgrade-repo-hvst-upgrade-9gmg2-4mnmq 1/1 Running 0 4m44s","title":"Phase 1: Provision upgrade repository VM."},{"location":"upgrade/troubleshooting/#phase-2-preload-container-images","text":"The Harvester controller creates jobs on each Harvester node to download images from the repository VM and preload them. These are the container images required for the next release. During this stage you can see the upgrade status windows shows: It will take a while for all nodes to preload images. If the upgrade fails at this phase, the user can check job logs in the cattle-system namespace: $ kubectl get jobs -n cattle-system | grep prepare apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 0/1 47s 47s apply-hvst-upgrade-9gmg2-prepare-on-node4-with-2bbea1599a-041e4 1/1 2m3s 2m50s $ kubectl logs jobs/apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 -n cattle-system ... It's also safe to start over the upgrade if an upgrade fails at this phase.","title":"Phase 2: Preload container images"},{"location":"upgrade/troubleshooting/#phase-3-upgrade-system-services","text":"In this phase, Harvester controller upgrades component Helm charts with a job. The user can check the apply-manifest job with the following command: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=manifest NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-apply-manifests 0/1 46s 46s $ kubectl logs jobs/hvst-upgrade-9gmg2-apply-manifests -n harvester-system ...","title":"Phase 3: Upgrade system services"},{"location":"upgrade/troubleshooting/#phase-4-upgrade-nodes","text":"The Harvester controller creates jobs on each node (one by one) to upgrade nodes' OSes and RKE2 runtime. For multi-node clusters, there are two kinds of jobs to update a node: pre-drain job: live-migrate or shutdown VMs on a node. When the job completes, the embedded Rancher service upgrades RKE2 runtime on a node. post-drain job: upgrade OS and reboot. For single-node clusters, there is only one single-node-upgrade type job for each node (named with the format hvst-upgrade-xxx-single-node-upgrade-<hostname> ). The user can check node jobs by: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=node NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-post-drain-node1 1/1 118s 6m34s hvst-upgrade-9gmg2-post-drain-node2 0/1 9s 9s hvst-upgrade-9gmg2-pre-drain-node1 1/1 3s 8m14s hvst-upgrade-9gmg2-pre-drain-node2 1/1 7s 85s $ kubectl logs -n harvester-system jobs/hvst-upgrade-9gmg2-post-drain-node2 ... Warning Please do not start over an upgrade if the upgrade fails at this phase.","title":"Phase 4: Upgrade nodes"},{"location":"upgrade/troubleshooting/#phase-5-clean-up","text":"The Harvester controller deletes the upgrade repository VM and all files that are no longer needed.","title":"Phase 5: Clean-up"},{"location":"upgrade/troubleshooting/#common-operations","text":"","title":"Common operations"},{"location":"upgrade/troubleshooting/#start-over-an-upgrade","text":"Log in to a control plane node. List Upgrade CRs in the cluster: # become root $ sudo -i # list the on-going upgrade $ kubectl get upgrade.harvesterhci.io -n harvester-system -l harvesterhci.io/latestUpgrade=true NAME AGE hvst-upgrade-9gmg2 10m Delete the Upgrade CR $ kubectl delete upgrade.harvesterhci.io/hvst-upgrade-9gmg2 -n harvester-system Click the upgrade button in the Harvester dashboard to start an upgrade again.","title":"Start over an upgrade"},{"location":"upgrade/v1-0-0-to-v1-0-1/","text":"Upgrade from v1.0.0 to v1.0.1 \u00b6 This document describes how to upgrade from Harvester v1.0.0 to v1.0.1 . Note we are still working towards zero-downtime upgrade, due to some known issues please follow the steps below before you upgrade your Harvester cluster: Warning Before you upgrade your Harvester cluster, we highly recommend: Shutting down all your VMs (Harvester GUI -> Virtual Machines -> Select VMs -> Actions -> Stop). Back up your VMs. Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc. Make sure your hardware meets the preferred hardware requirements . This is due to there will be intermediate resources consumed by an upgrade. Make sure each node has at least 25 GB of free space ( df -h /usr/local/ ). Warning Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server on each node : $ sudo -i # Add time servers $ vim /etc/systemd/timesyncd.conf [ntp] NTP=0.pool.ntp.org # Enable and start the systemd-timesyncd $ timedatectl set-ntp true # Check status $ timedatectl status Warning NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the knowledge base article for further information. Create a version \u00b6 Log in to one of your server nodes. Become root and create a version: rancher@node1:~> sudo -i node1:~ # kubectl create -f https://releases.rancher.com/harvester/v1.0.1/version.yaml version.harvesterhci.io/1.0.1 created Note By default, the ISO image is downloaded from the Harvester release server. To speed up the upgrade and make the upgrade progress smoother, the user can also download the ISO file to a local HTTP server first and substitute the isoURL value in the version.yaml manifest. e.g., # Download the ISO from release server first, assume it's store in http://10.10.0.1/harvester.iso $ sudo -i $ curl -fL https://releases.rancher.com/harvester/v1.0.1/version.yaml -o version.yaml $ vim version.yaml apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: v1.0.1 namespace: harvester-system spec: isoChecksum: <SHA-512 checksum of the ISO> isoURL: http://10.10.0.1/harvester.iso releaseDate: '20220408' Start the upgrade \u00b6 Make sure to read the Warning paragraph at the top of this document first. Navigate to Harvester GUI and click the upgrade button on the Dashboard page. Select a version to start upgrading. Click the circle on the top to display the upgrade progress. Known issues \u00b6 Fail to download upgrade image \u00b6 Description Downloading the upgrade image can't complete. Workaround We can delete the current upgrade and start over. # log in to one of the server nodes $ sudo -i # list current upgrade, the name changes between deployments $ kubectl get upgrades.harvesterhci.io -n harvester-system NAMESPACE NAME AGE harvester-system hvst-upgrade-77cks 119m $ kubectl delete upgrades.harvesterhci.io hvst-upgrade-77cks -n harvester-system We recommend mirroring the ISO file to a local webserver, please check the notes in the previous section . Stuck in Upgrading System Service \u00b6 Description The upgrade is stuck at Upgrading System service . Similar logs are found in rancher pods: [ERROR] available chart version (100.0.2+up0.3.8) for fleet is less than the min version (100.0.3+up0.3.9-rc1) [ERROR] Failed to find system chart fleet will try again in 5 seconds: no chart name found Workaround Delete rancher cluster repositories and restart rancher pods. # login to a server node and become root first kubectl delete clusterrepos.catalog.cattle.io rancher-charts kubectl delete clusterrepos.catalog.cattle.io rancher-rke2-charts kubectl delete clusterrepos.catalog.cattle.io rancher-partner-charts kubectl delete settings.management.cattle.io chart-default-branch kubectl rollout restart deployment rancher -n cattle-system Related issues [BUG] Rancher upgrade fail: Failed to find system chart \"fleet\" VMs fail to migrate \u00b6 Description A node keeps waiting in Pre-draining state. There are VMs on that node (checking for virt-launcher-xxx pods) and they can't be live-migrated out of the node. Workaround Shutdown the VMs, you can do this by: Using the GUI. Using the virtctl command. Related issues [BUG] Upgrade: VMs fail to live-migrate to other hosts in some cases fleet-local/local: another operation (install/upgrade/rollback) is in progress \u00b6 Description You see bundles have fleet-local/local: another operation (install/upgrade/rollback) is in progress status in the output: kubectl get bundles -A Related issues [BUG] Upgrade: rancher-monitoring charts can't be upgraded Single node upgrade might fail if node name is too long (>24 characters) \u00b6 Related issues https://github.com/harvester/harvester/issues/2114","title":"Upgrade from v1.0.0 to v1.0.1"},{"location":"upgrade/v1-0-0-to-v1-0-1/#upgrade-from-v100-to-v101","text":"This document describes how to upgrade from Harvester v1.0.0 to v1.0.1 . Note we are still working towards zero-downtime upgrade, due to some known issues please follow the steps below before you upgrade your Harvester cluster: Warning Before you upgrade your Harvester cluster, we highly recommend: Shutting down all your VMs (Harvester GUI -> Virtual Machines -> Select VMs -> Actions -> Stop). Back up your VMs. Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc. Make sure your hardware meets the preferred hardware requirements . This is due to there will be intermediate resources consumed by an upgrade. Make sure each node has at least 25 GB of free space ( df -h /usr/local/ ). Warning Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server on each node : $ sudo -i # Add time servers $ vim /etc/systemd/timesyncd.conf [ntp] NTP=0.pool.ntp.org # Enable and start the systemd-timesyncd $ timedatectl set-ntp true # Check status $ timedatectl status Warning NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the knowledge base article for further information.","title":"Upgrade from v1.0.0 to v1.0.1"},{"location":"upgrade/v1-0-0-to-v1-0-1/#create-a-version","text":"Log in to one of your server nodes. Become root and create a version: rancher@node1:~> sudo -i node1:~ # kubectl create -f https://releases.rancher.com/harvester/v1.0.1/version.yaml version.harvesterhci.io/1.0.1 created Note By default, the ISO image is downloaded from the Harvester release server. To speed up the upgrade and make the upgrade progress smoother, the user can also download the ISO file to a local HTTP server first and substitute the isoURL value in the version.yaml manifest. e.g., # Download the ISO from release server first, assume it's store in http://10.10.0.1/harvester.iso $ sudo -i $ curl -fL https://releases.rancher.com/harvester/v1.0.1/version.yaml -o version.yaml $ vim version.yaml apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: v1.0.1 namespace: harvester-system spec: isoChecksum: <SHA-512 checksum of the ISO> isoURL: http://10.10.0.1/harvester.iso releaseDate: '20220408'","title":"Create a version"},{"location":"upgrade/v1-0-0-to-v1-0-1/#start-the-upgrade","text":"Make sure to read the Warning paragraph at the top of this document first. Navigate to Harvester GUI and click the upgrade button on the Dashboard page. Select a version to start upgrading. Click the circle on the top to display the upgrade progress.","title":"Start the upgrade"},{"location":"upgrade/v1-0-0-to-v1-0-1/#known-issues","text":"","title":"Known issues"},{"location":"upgrade/v1-0-0-to-v1-0-1/#fail-to-download-upgrade-image","text":"Description Downloading the upgrade image can't complete. Workaround We can delete the current upgrade and start over. # log in to one of the server nodes $ sudo -i # list current upgrade, the name changes between deployments $ kubectl get upgrades.harvesterhci.io -n harvester-system NAMESPACE NAME AGE harvester-system hvst-upgrade-77cks 119m $ kubectl delete upgrades.harvesterhci.io hvst-upgrade-77cks -n harvester-system We recommend mirroring the ISO file to a local webserver, please check the notes in the previous section .","title":"Fail to download upgrade image"},{"location":"upgrade/v1-0-0-to-v1-0-1/#stuck-in-upgrading-system-service","text":"Description The upgrade is stuck at Upgrading System service . Similar logs are found in rancher pods: [ERROR] available chart version (100.0.2+up0.3.8) for fleet is less than the min version (100.0.3+up0.3.9-rc1) [ERROR] Failed to find system chart fleet will try again in 5 seconds: no chart name found Workaround Delete rancher cluster repositories and restart rancher pods. # login to a server node and become root first kubectl delete clusterrepos.catalog.cattle.io rancher-charts kubectl delete clusterrepos.catalog.cattle.io rancher-rke2-charts kubectl delete clusterrepos.catalog.cattle.io rancher-partner-charts kubectl delete settings.management.cattle.io chart-default-branch kubectl rollout restart deployment rancher -n cattle-system Related issues [BUG] Rancher upgrade fail: Failed to find system chart \"fleet\"","title":"Stuck in Upgrading System Service"},{"location":"upgrade/v1-0-0-to-v1-0-1/#vms-fail-to-migrate","text":"Description A node keeps waiting in Pre-draining state. There are VMs on that node (checking for virt-launcher-xxx pods) and they can't be live-migrated out of the node. Workaround Shutdown the VMs, you can do this by: Using the GUI. Using the virtctl command. Related issues [BUG] Upgrade: VMs fail to live-migrate to other hosts in some cases","title":"VMs fail to migrate"},{"location":"upgrade/v1-0-0-to-v1-0-1/#fleet-locallocal-another-operation-installupgraderollback-is-in-progress","text":"Description You see bundles have fleet-local/local: another operation (install/upgrade/rollback) is in progress status in the output: kubectl get bundles -A Related issues [BUG] Upgrade: rancher-monitoring charts can't be upgraded","title":"fleet-local/local: another operation (install/upgrade/rollback) is in progress"},{"location":"upgrade/v1-0-0-to-v1-0-1/#single-node-upgrade-might-fail-if-node-name-is-too-long-24-characters","text":"Related issues https://github.com/harvester/harvester/issues/2114","title":"Single node upgrade might fail if node name is too long (&gt;24 characters)"},{"location":"upgrade/v1-0-1-to-v1-0-2/","text":"Upgrade from v1.0.1 to v1.0.2 \u00b6 General information \u00b6 The Harvester GUI Dashboard page should have an upgrade button to perform an upgrade. For more details please refer to start an upgrade . For the air-gap env upgrade, please refer to prepare an air-gapped upgrade . Known issues \u00b6 Please check Known issues here.","title":"Upgrade from v1.0.1 to v1.0.2"},{"location":"upgrade/v1-0-1-to-v1-0-2/#upgrade-from-v101-to-v102","text":"","title":"Upgrade from v1.0.1 to v1.0.2"},{"location":"upgrade/v1-0-1-to-v1-0-2/#general-information","text":"The Harvester GUI Dashboard page should have an upgrade button to perform an upgrade. For more details please refer to start an upgrade . For the air-gap env upgrade, please refer to prepare an air-gapped upgrade .","title":"General information"},{"location":"upgrade/v1-0-1-to-v1-0-2/#known-issues","text":"Please check Known issues here.","title":"Known issues"},{"location":"upgrade/v1-0-2-to-v1-0-3/","text":"Upgrade from v1.0.2 to v1.0.3 \u00b6 General information \u00b6 The Harvester GUI Dashboard page should have an upgrade button to perform an upgrade. For more details please refer to start an upgrade . For the air-gap env upgrade, please refer to prepare an air-gapped upgrade . Known issues \u00b6 1. Fail to download the upgrade image \u00b6 Description Downloading the upgrade image can't be complete or fails with an error. Related issues [BUG] failed to create upgrade image Workaround Delete the current upgrade and start over. Please see \"Start over an upgrade\" . 2. An upgrade is stuck, a node is in \"Pre-drained\" state (case 1) \u00b6 Description Users might see a node is stuck at the Pre-drained state for a while (> 30 minutes). This might be caused by instance-manager-r-* pod on node harvester-z7j2g can\u2019t be drained. To verify the above case: Check rancher server logs: kubectl logs deployment/rancher -n cattle-system Example output: error when evicting pods/\"instance-manager-r-10dd59c4\" -n \"longhorn-system\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod longhorn-system/instance-manager-r-10dd59c4 error when evicting pods/\"instance-manager-r-10dd59c4\" -n \"longhorn-system\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod longhorn-system/instance-manager-r-10dd59c4 error when evicting pods/\"instance-manager-r-10dd59c4\" -n \"longhorn-system\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod longhorn-system/instance-manager-r-10dd59c4 error when evicting pods/\"instance-manager-r-10dd59c4\" -n \"longhorn-system\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Verify the pod longhorn-system/instance-manager-r-10dd59c4 is on the stuck node: kubectl get pod instance-manager-r-10dd59c4 -n longhorn-system -o=jsonpath='{.spec.nodeName}' Example output: harvester-z7j2g Check degraded volumes: kubectl get volumes -n longhorn-system Example output: NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE pvc-08c34593-8225-4be6-9899-10a978df6ea1 attached healthy True 10485760 harvester-279l2 3d13h pvc-526600f5-bde2-4244-bb8e-7910385cbaeb attached healthy True 21474836480 harvester-x9jqw 3d1h pvc-7b3fc2c3-30eb-48b8-8a98-11913f8314c2 attached healthy True 10737418240 harvester-x9jqw 3d pvc-8065ed6c-a077-472c-920e-5fe9eacff96e attached healthy True 21474836480 harvester-x9jqw 3d pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599 attached degraded True 10737418240 harvester-x9jqw 2d23h pvc-9a6539b8-44e5-430e-9b24-ea8290cb13b7 attached healthy True 53687091200 harvester-x9jqw 3d13h Here we can see volume pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599 is degraded. Notes the user needs to check all degraded volumes one by one. Check degraded volume\u2019s replica state: kubectl get replicas -n longhorn-system --selector longhornvolume = pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599 -o json | jq '.items[] | {replica: .metadata.name, healthyAt: .spec.healthyAt, nodeID: .spec.nodeID, state: .status.currentState}' Example output: { \"replica\": \"pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-15e31246\", \"healthyAt\": \"2022-07-25T07:33:16Z\", \"nodeID\": \"harvester-z7j2g\", \"state\": \"running\" } { \"replica\": \"pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-22974d0f\", \"healthyAt\": \"\", \"nodeID\": \"harvester-279l2\", \"state\": \"running\" } { \"replica\": \"pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5\", \"healthyAt\": \"\", \"nodeID\": \"harvester-x9jqw\", \"state\": \"stopped\" } Here the only healthy replica is pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-15e31246 and it\u2019s on node harvester-z7j2g . So we can confirm the instance-manager-r-* pod resides on node harvester-z7j2g and avoids the drain. Related issues [BUG] Upgrade: longhorn-system can't be evicted Workaround We need to start the \u201cStopped\u201d replica, from the previous example, the stopped replica\u2019s name is pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 . Check the Longhorn manager log, we should see a replica waiting for the backing image. First, we need to get the manager's name: kubectl get pods -n longhorn-system --selector app=longhorn-manager --field-selector spec.nodeName=harvester-x9jqw Example output: NAME READY STATUS RESTARTS AGE longhorn-manager-zmfbw 1 /1 Running 0 3d10h Get pod log: kubectl logs longhorn-manager-zmfbw -n longhorn-system | grep pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 Example output: (...) time=\"2022-07-28T04:35:34Z\" level=debug msg=\"Prepare to create instance pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5\" time=\"2022-07-28T04:35:34Z\" level=debug msg=\"Replica pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 is waiting for backing image harvester-system-harvester-iso-n7bxh downloading file to node harvester-x9jqw disk 3830342d-c13d-4e55-ac74-99cad529e9d4, the current state is in-progress\" controller=longhorn-replica dataPath= node=harvester-x9jqw nodeID=harvester-x9jqw ownerID=harvester-x9jqw replica=pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 time=\"2022-07-28T04:35:34Z\" level=info msg=\"Event(v1.ObjectReference{Kind:\\\"Replica\\\", Namespace:\\\"longhorn-system\\\", Name:\\\"pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5\\\", UID:\\\"c511630f-2fe2-4cf9-97a4-21bce73782b1\\\", APIVersion:\\\"longhorn.io/v1beta1\\\", ResourceVersion:\\\"632926\\\", FieldPath:\\\"\\\"}): type: 'Normal' reason: 'Start' Starts pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5\" Here we can determine the replica is waiting for the backing image harvester-system-harvester-iso-n7bxh . Get the disk file map from the backing image: kubectl describe backingimage harvester-system-harvester-iso-n7bxh -n longhorn-system Example output: ( ... ) Disk File Status Map: 3830342d-c13d-4e55-ac74-99cad529e9d4: Last State Transition Time: 2022 -07-25T08:30:34Z Message: Progress: 29 State: in -progress 3aa804e1-229d-4141-8816-1f6a7c6c3096: Last State Transition Time: 2022 -07-25T08:33:20Z Message: Progress: 100 State: ready 92726efa-bfb3-478e-8553-3206ad34ce70: Last State Transition Time: 2022 -07-28T04:31:49Z Message: Progress: 100 State: ready The disk file with UUID 3830342d-c13d-4e55-ac74-99cad529e9d4 has the state in-progress . Next, we need to find backing-image-manager that contains this disk file: kubectl get pod -n longhorn-system --selector = longhorn.io/disk-uuid = 3830342d-c13d-4e55-ac74-99cad529e9d4 Example output: NAME READY STATUS RESTARTS AGE backing-image-manager-c00e-3830 1/1 Running 0 3d1h Restart the backing-image-manager by deleting its pod: kubectl delete pod -n longhorn-system backing-image-manager-c00e-3830 3. An upgrade is stuck, a node is in \"Pre-drained\" state (case 2) \u00b6 Description Users might see a node is stuck at the Pre-drained state for a while (> 30 minutes). Here are some steps to verify this issue has happened: Visit the Longhorn GUI: https://{{VIP}}/k8s/clusters/local/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/#/volume (replace VIP with an appropriate value) and check degraded volumes. The degraded volume might contain a healthy replica only (with blue background) and the healthy replica resides on the \"Pre-drained\" node: Hover the mouse to the red scheduled icon, the reason is toomanysnapshots : Related issues [BUG] Upgrade is stuck in \"Pre-drained\" state (Volume has too many system snapshots) Workaround In the \"Snapshots and Backup\" panel, toggle the \"Show System Hidden\" switch and delete the latest system snapshot (which is just before the \"Volume Head\"). The volume will continue rebuilding to resume the upgrade.","title":"Upgrade from v1.0.2 to v1.0.3"},{"location":"upgrade/v1-0-2-to-v1-0-3/#upgrade-from-v102-to-v103","text":"","title":"Upgrade from v1.0.2 to v1.0.3"},{"location":"upgrade/v1-0-2-to-v1-0-3/#general-information","text":"The Harvester GUI Dashboard page should have an upgrade button to perform an upgrade. For more details please refer to start an upgrade . For the air-gap env upgrade, please refer to prepare an air-gapped upgrade .","title":"General information"},{"location":"upgrade/v1-0-2-to-v1-0-3/#known-issues","text":"","title":"Known issues"},{"location":"upgrade/v1-0-2-to-v1-0-3/#1-fail-to-download-the-upgrade-image","text":"Description Downloading the upgrade image can't be complete or fails with an error. Related issues [BUG] failed to create upgrade image Workaround Delete the current upgrade and start over. Please see \"Start over an upgrade\" .","title":"1. Fail to download the upgrade image"},{"location":"upgrade/v1-0-2-to-v1-0-3/#2-an-upgrade-is-stuck-a-node-is-in-pre-drained-state-case-1","text":"Description Users might see a node is stuck at the Pre-drained state for a while (> 30 minutes). This might be caused by instance-manager-r-* pod on node harvester-z7j2g can\u2019t be drained. To verify the above case: Check rancher server logs: kubectl logs deployment/rancher -n cattle-system Example output: error when evicting pods/\"instance-manager-r-10dd59c4\" -n \"longhorn-system\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod longhorn-system/instance-manager-r-10dd59c4 error when evicting pods/\"instance-manager-r-10dd59c4\" -n \"longhorn-system\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod longhorn-system/instance-manager-r-10dd59c4 error when evicting pods/\"instance-manager-r-10dd59c4\" -n \"longhorn-system\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod longhorn-system/instance-manager-r-10dd59c4 error when evicting pods/\"instance-manager-r-10dd59c4\" -n \"longhorn-system\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Verify the pod longhorn-system/instance-manager-r-10dd59c4 is on the stuck node: kubectl get pod instance-manager-r-10dd59c4 -n longhorn-system -o=jsonpath='{.spec.nodeName}' Example output: harvester-z7j2g Check degraded volumes: kubectl get volumes -n longhorn-system Example output: NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE pvc-08c34593-8225-4be6-9899-10a978df6ea1 attached healthy True 10485760 harvester-279l2 3d13h pvc-526600f5-bde2-4244-bb8e-7910385cbaeb attached healthy True 21474836480 harvester-x9jqw 3d1h pvc-7b3fc2c3-30eb-48b8-8a98-11913f8314c2 attached healthy True 10737418240 harvester-x9jqw 3d pvc-8065ed6c-a077-472c-920e-5fe9eacff96e attached healthy True 21474836480 harvester-x9jqw 3d pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599 attached degraded True 10737418240 harvester-x9jqw 2d23h pvc-9a6539b8-44e5-430e-9b24-ea8290cb13b7 attached healthy True 53687091200 harvester-x9jqw 3d13h Here we can see volume pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599 is degraded. Notes the user needs to check all degraded volumes one by one. Check degraded volume\u2019s replica state: kubectl get replicas -n longhorn-system --selector longhornvolume = pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599 -o json | jq '.items[] | {replica: .metadata.name, healthyAt: .spec.healthyAt, nodeID: .spec.nodeID, state: .status.currentState}' Example output: { \"replica\": \"pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-15e31246\", \"healthyAt\": \"2022-07-25T07:33:16Z\", \"nodeID\": \"harvester-z7j2g\", \"state\": \"running\" } { \"replica\": \"pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-22974d0f\", \"healthyAt\": \"\", \"nodeID\": \"harvester-279l2\", \"state\": \"running\" } { \"replica\": \"pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5\", \"healthyAt\": \"\", \"nodeID\": \"harvester-x9jqw\", \"state\": \"stopped\" } Here the only healthy replica is pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-15e31246 and it\u2019s on node harvester-z7j2g . So we can confirm the instance-manager-r-* pod resides on node harvester-z7j2g and avoids the drain. Related issues [BUG] Upgrade: longhorn-system can't be evicted Workaround We need to start the \u201cStopped\u201d replica, from the previous example, the stopped replica\u2019s name is pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 . Check the Longhorn manager log, we should see a replica waiting for the backing image. First, we need to get the manager's name: kubectl get pods -n longhorn-system --selector app=longhorn-manager --field-selector spec.nodeName=harvester-x9jqw Example output: NAME READY STATUS RESTARTS AGE longhorn-manager-zmfbw 1 /1 Running 0 3d10h Get pod log: kubectl logs longhorn-manager-zmfbw -n longhorn-system | grep pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 Example output: (...) time=\"2022-07-28T04:35:34Z\" level=debug msg=\"Prepare to create instance pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5\" time=\"2022-07-28T04:35:34Z\" level=debug msg=\"Replica pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 is waiting for backing image harvester-system-harvester-iso-n7bxh downloading file to node harvester-x9jqw disk 3830342d-c13d-4e55-ac74-99cad529e9d4, the current state is in-progress\" controller=longhorn-replica dataPath= node=harvester-x9jqw nodeID=harvester-x9jqw ownerID=harvester-x9jqw replica=pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 time=\"2022-07-28T04:35:34Z\" level=info msg=\"Event(v1.ObjectReference{Kind:\\\"Replica\\\", Namespace:\\\"longhorn-system\\\", Name:\\\"pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5\\\", UID:\\\"c511630f-2fe2-4cf9-97a4-21bce73782b1\\\", APIVersion:\\\"longhorn.io/v1beta1\\\", ResourceVersion:\\\"632926\\\", FieldPath:\\\"\\\"}): type: 'Normal' reason: 'Start' Starts pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5\" Here we can determine the replica is waiting for the backing image harvester-system-harvester-iso-n7bxh . Get the disk file map from the backing image: kubectl describe backingimage harvester-system-harvester-iso-n7bxh -n longhorn-system Example output: ( ... ) Disk File Status Map: 3830342d-c13d-4e55-ac74-99cad529e9d4: Last State Transition Time: 2022 -07-25T08:30:34Z Message: Progress: 29 State: in -progress 3aa804e1-229d-4141-8816-1f6a7c6c3096: Last State Transition Time: 2022 -07-25T08:33:20Z Message: Progress: 100 State: ready 92726efa-bfb3-478e-8553-3206ad34ce70: Last State Transition Time: 2022 -07-28T04:31:49Z Message: Progress: 100 State: ready The disk file with UUID 3830342d-c13d-4e55-ac74-99cad529e9d4 has the state in-progress . Next, we need to find backing-image-manager that contains this disk file: kubectl get pod -n longhorn-system --selector = longhorn.io/disk-uuid = 3830342d-c13d-4e55-ac74-99cad529e9d4 Example output: NAME READY STATUS RESTARTS AGE backing-image-manager-c00e-3830 1/1 Running 0 3d1h Restart the backing-image-manager by deleting its pod: kubectl delete pod -n longhorn-system backing-image-manager-c00e-3830","title":"2. An upgrade is stuck, a node is in \"Pre-drained\" state (case 1)"},{"location":"upgrade/v1-0-2-to-v1-0-3/#3-an-upgrade-is-stuck-a-node-is-in-pre-drained-state-case-2","text":"Description Users might see a node is stuck at the Pre-drained state for a while (> 30 minutes). Here are some steps to verify this issue has happened: Visit the Longhorn GUI: https://{{VIP}}/k8s/clusters/local/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/#/volume (replace VIP with an appropriate value) and check degraded volumes. The degraded volume might contain a healthy replica only (with blue background) and the healthy replica resides on the \"Pre-drained\" node: Hover the mouse to the red scheduled icon, the reason is toomanysnapshots : Related issues [BUG] Upgrade is stuck in \"Pre-drained\" state (Volume has too many system snapshots) Workaround In the \"Snapshots and Backup\" panel, toggle the \"Show System Hidden\" switch and delete the latest system snapshot (which is just before the \"Volume Head\"). The volume will continue rebuilding to resume the upgrade.","title":"3. An upgrade is stuck, a node is in \"Pre-drained\" state (case 2)"},{"location":"vm/access-to-the-vm/","text":"Access to the Virtual Machine \u00b6 Once the VM is up and running, you can access it using either the Virtual Network Computing (VNC) client or the serial console from the Harvester UI. Additionally, you can connect directly from your computer's SSH client. Access with the Harvester UI \u00b6 VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM, as with the Ubuntu-minimal-cloud image, the VM can only be accessed with the serial console. SSH Access \u00b6 Harvester provides two ways to inject SSH public keys into virtual machines. Generally, these methods fall into two categories. Static key injection , which places keys in the cloud-init script when the virtual machine is first powered on; dynamic injection , which allows keys or basic auth to be updated dynamically at runtime. Static SSH Key Injection via cloud-init \u00b6 You can provide ssh keys to your virtual machines during the creation time on the Basics tab. Additionally, you can place the public ssh keys into your cloud-init script to allow it to take place. Example of SSH key cloud-init configuration: \u00b6 #cloud-config ssh_authorized_keys : - >- ssh-rsa #replace with your public key Dynamic SSH Key Injection via Qemu guest agent \u00b6 Available as of v1.0.1 Harvester supports dynamically injecting public ssh keys at run time through the use of the qemu guest agent . This is achieved through the qemuGuestAgent propagation method. Note This method requires the qemu guest agent to be installed within the guest VM. When using qemuGuestAgent propagation, the /home/$USER/.ssh/authorized_keys file will be owned by the guest agent. Changes to that file that are made outside of the qemu guest agent's control will get deleted. You can inject your access credentials via the Harvester dashboard as below: Select the VM and click \u22ee button. Click the Edit Config button and go to the Access Credentials tab. Click to add either basic auth credentials or ssh keys, (e.g., add opensuse as the user and select your ssh keys if your guest OS is OpenSUSE). Make sure your qemu guest agent is already installed and the VM should be restarted after the credentials are added. Note You need to enter the VM to edit password or remove SSH-Key after deleting the credentials from the UI. Access with the SSH Client \u00b6 Once the VM is up and running, you can enter the IP address of the VM in a terminal emulation client, such as PuTTY. You may also run the following command to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@<ip-address-or-hostname>","title":"Access to the Virtual Machine"},{"location":"vm/access-to-the-vm/#access-to-the-virtual-machine","text":"Once the VM is up and running, you can access it using either the Virtual Network Computing (VNC) client or the serial console from the Harvester UI. Additionally, you can connect directly from your computer's SSH client.","title":"Access to the Virtual Machine"},{"location":"vm/access-to-the-vm/#access-with-the-harvester-ui","text":"VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM, as with the Ubuntu-minimal-cloud image, the VM can only be accessed with the serial console.","title":"Access with the Harvester UI"},{"location":"vm/access-to-the-vm/#ssh-access","text":"Harvester provides two ways to inject SSH public keys into virtual machines. Generally, these methods fall into two categories. Static key injection , which places keys in the cloud-init script when the virtual machine is first powered on; dynamic injection , which allows keys or basic auth to be updated dynamically at runtime.","title":"SSH Access"},{"location":"vm/access-to-the-vm/#static-ssh-key-injection-via-cloud-init","text":"You can provide ssh keys to your virtual machines during the creation time on the Basics tab. Additionally, you can place the public ssh keys into your cloud-init script to allow it to take place.","title":"Static SSH Key Injection via cloud-init"},{"location":"vm/access-to-the-vm/#example-of-ssh-key-cloud-init-configuration","text":"#cloud-config ssh_authorized_keys : - >- ssh-rsa #replace with your public key","title":"Example of SSH key cloud-init configuration:"},{"location":"vm/access-to-the-vm/#dynamic-ssh-key-injection-via-qemu-guest-agent","text":"Available as of v1.0.1 Harvester supports dynamically injecting public ssh keys at run time through the use of the qemu guest agent . This is achieved through the qemuGuestAgent propagation method. Note This method requires the qemu guest agent to be installed within the guest VM. When using qemuGuestAgent propagation, the /home/$USER/.ssh/authorized_keys file will be owned by the guest agent. Changes to that file that are made outside of the qemu guest agent's control will get deleted. You can inject your access credentials via the Harvester dashboard as below: Select the VM and click \u22ee button. Click the Edit Config button and go to the Access Credentials tab. Click to add either basic auth credentials or ssh keys, (e.g., add opensuse as the user and select your ssh keys if your guest OS is OpenSUSE). Make sure your qemu guest agent is already installed and the VM should be restarted after the credentials are added. Note You need to enter the VM to edit password or remove SSH-Key after deleting the credentials from the UI.","title":"Dynamic SSH Key Injection via Qemu guest agent"},{"location":"vm/access-to-the-vm/#access-with-the-ssh-client","text":"Once the VM is up and running, you can enter the IP address of the VM in a terminal emulation client, such as PuTTY. You may also run the following command to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@<ip-address-or-hostname>","title":"Access with the SSH Client"},{"location":"vm/backup-restore/","text":"VM Backup & Restore \u00b6 Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server), and they can be used to either restore a new VM or replace an existing VM. Note A backup target must be set up. For more information, see Configure Backup Target . If the backup target has not been set, you\u2019ll be prompted with a message to do so. Configure Backup Target \u00b6 A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings > backup-target . Parameter Type Description Type string Choose S3 or NFS Endpoint string A hostname or an IP address. It can be left empty for AWS S3. BucketName string Name of the bucket BucketRegion string Region of the bucket AccessKeyID string A user-id that uniquely identifies your account SecretAccessKey string The password to your account Certificate string Paste to use a self-signed SSL certificate of your S3 server VirtualHostedStyle bool Use VirtualHostedStyle access only; e.g., Alibaba Cloud (Aliyun) OSS Create a VM backup \u00b6 Once the backup target is set, go to the Virtual Machines page. Click Take Backup of the VM actions to create a new VM backup. Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. You will receive a notification message, and you can also go to the Advanced > Backups page to view all VM backups. The ReadyToUse status will be set to true once the Backup is complete. Users can either choose to restore a new VM or replace an existing VM using this backup. Restore a new VM using a backup \u00b6 To restore a new VM from a backup, follow these steps: Go to the Backups page. Specify the new VM name and click Create . A new VM will be restored using the backup volumes and metadata, and you can access it from the Virtual Machines page. Replace an Existing VM using a backup \u00b6 You can replace an existing VM using the backup with the same VM backup target. You can choose to either delete or retain the previous volumes. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the Backups page. Click Create . The restore process can be viewed from the Virtual Machines page. Restore a new VM on another Harvester cluster \u00b6 Available as of v1.0.0 Users can now restore a new VM on another cluster by leveraging the VM metadata & content backup feature. Prerequisites You must manually configure the virtual machine images with the same name on the new cluster first, otherwise the virtual machines will be failed to recover. Upload the same VM images to a new cluster \u00b6 Check the existing image name (normally starts with image- ) and create the same one on the new cluster. $ kubectl get vmimages -A NAMESPACE NAME DISPLAY-NAME SIZE AGE default image-79hdq focal-server-cloudimg-amd64.img 566886400 5h36m default image-l7924 harvester-v1.0.0-rc2-amd64.iso 3964551168 137m default image-lvqxn opensuse-leap-15.3.x86_64-nocloud.qcow2 568524800 5h35m Apply a VM image YAML with the same name and content in the new cluster. $ cat <<EOF | kubectl apply -f - apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: name: image-lvqxn namespace: default spec: displayName: opensuse-leap-15.3.x86_64-nocloud.qcow2 pvcName: \"\" pvcNamespace: \"\" sourceType: download url: http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 EOF Restore a new VM in a new cluster \u00b6 Setup the same backup target in a new cluster. And the backup controller will automatically sync the backup metadata to the new cluster. Go to the Backups page. Select the synced VM backup metadata and choose to restore a new VM with a specified VM name. A new VM will be restored using the backup volumes and metadata. You can access it from the Virtual Machines page.","title":"VM Backup & Restore"},{"location":"vm/backup-restore/#vm-backup-restore","text":"Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server), and they can be used to either restore a new VM or replace an existing VM. Note A backup target must be set up. For more information, see Configure Backup Target . If the backup target has not been set, you\u2019ll be prompted with a message to do so.","title":"VM Backup &amp; Restore"},{"location":"vm/backup-restore/#configure-backup-target","text":"A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings > backup-target . Parameter Type Description Type string Choose S3 or NFS Endpoint string A hostname or an IP address. It can be left empty for AWS S3. BucketName string Name of the bucket BucketRegion string Region of the bucket AccessKeyID string A user-id that uniquely identifies your account SecretAccessKey string The password to your account Certificate string Paste to use a self-signed SSL certificate of your S3 server VirtualHostedStyle bool Use VirtualHostedStyle access only; e.g., Alibaba Cloud (Aliyun) OSS","title":"Configure Backup Target"},{"location":"vm/backup-restore/#create-a-vm-backup","text":"Once the backup target is set, go to the Virtual Machines page. Click Take Backup of the VM actions to create a new VM backup. Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. You will receive a notification message, and you can also go to the Advanced > Backups page to view all VM backups. The ReadyToUse status will be set to true once the Backup is complete. Users can either choose to restore a new VM or replace an existing VM using this backup.","title":"Create a VM backup"},{"location":"vm/backup-restore/#restore-a-new-vm-using-a-backup","text":"To restore a new VM from a backup, follow these steps: Go to the Backups page. Specify the new VM name and click Create . A new VM will be restored using the backup volumes and metadata, and you can access it from the Virtual Machines page.","title":"Restore a new VM using a backup"},{"location":"vm/backup-restore/#replace-an-existing-vm-using-a-backup","text":"You can replace an existing VM using the backup with the same VM backup target. You can choose to either delete or retain the previous volumes. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the Backups page. Click Create . The restore process can be viewed from the Virtual Machines page.","title":"Replace an Existing VM using a backup"},{"location":"vm/backup-restore/#restore-a-new-vm-on-another-harvester-cluster","text":"Available as of v1.0.0 Users can now restore a new VM on another cluster by leveraging the VM metadata & content backup feature. Prerequisites You must manually configure the virtual machine images with the same name on the new cluster first, otherwise the virtual machines will be failed to recover.","title":"Restore a new VM on another Harvester cluster"},{"location":"vm/backup-restore/#upload-the-same-vm-images-to-a-new-cluster","text":"Check the existing image name (normally starts with image- ) and create the same one on the new cluster. $ kubectl get vmimages -A NAMESPACE NAME DISPLAY-NAME SIZE AGE default image-79hdq focal-server-cloudimg-amd64.img 566886400 5h36m default image-l7924 harvester-v1.0.0-rc2-amd64.iso 3964551168 137m default image-lvqxn opensuse-leap-15.3.x86_64-nocloud.qcow2 568524800 5h35m Apply a VM image YAML with the same name and content in the new cluster. $ cat <<EOF | kubectl apply -f - apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: name: image-lvqxn namespace: default spec: displayName: opensuse-leap-15.3.x86_64-nocloud.qcow2 pvcName: \"\" pvcNamespace: \"\" sourceType: download url: http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 EOF","title":"Upload the same VM images to a new cluster"},{"location":"vm/backup-restore/#restore-a-new-vm-in-a-new-cluster","text":"Setup the same backup target in a new cluster. And the backup controller will automatically sync the backup metadata to the new cluster. Go to the Backups page. Select the synced VM backup metadata and choose to restore a new VM with a specified VM name. A new VM will be restored using the backup volumes and metadata. You can access it from the Virtual Machines page.","title":"Restore a new VM in a new cluster"},{"location":"vm/create-vm/","text":"Create a Virtual Machine \u00b6 How to Create a VM \u00b6 You can create one or more virtual machines from the Virtual Machines page. Note Please refer to this page for creating Windows virtual machines. Choose the option to create either one or multiple VM instances. Select the namespace of your VMs, only the harvester-public namespace is visible to all users. The VM Name is a required field. (Optional) VM template is optional, you can choose iso-image , raw-image or windows-iso-image template to speed up your VM instance creation. Configure the virtual machine's CPU and memory (see overcommit settings if you want to over-provision). Select SSH keys or upload new keys. Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM. To configure networks, go to the Networks tab. The Management Network is added by default, you can remove it if the VLAN network is configured. You can also add additional networks to the VMs using VLAN networks. You may configure the VLAN networks on Advanced > Networks first. Advanced options such as run strategy, os type and cloud-init data are optional. You may configure these in the Advanced Options section when applicable. Volumes \u00b6 You can add one or more additional volumes via the Volumes tab, by default the first disk will be the root disk, you can change the boot order by dragging and dropping volumes, or using the arrow buttons. A disk can be made accessible via the following types: type description disk A disk disk will expose the volume as an ordinary disk to the VM. cd-rom A cd-rom disk will expose the volume as a cd-rom drive to the VM. It is read-only by default. Container Disk Container disks are ephemeral storage devices that can be assigned to any number of VMs. This makes them an ideal tool for users who want to replicate a large number of VM workloads or inject machine drivers that do not require persistent data. Note: Container disks are not a good solution for any workload that requires persistent root disks across VM restarts. Networks \u00b6 You can choose to add both the management network or VLAN network to your VM instances via the Networks tab, the management network is optional if you have the VLAN network configured. Network interfaces are configured through the Type field. They describe the properties of the virtual interfaces seen inside the guest OS: type description bridge Connect using a Linux bridge masquerade Connect using iptables rules to NAT the traffic Management Network \u00b6 A management network represents the default VM eth0 interface configured by the cluster network solution that is present in each VM. By default, VMs are accessible through the management network within the cluster nodes. Secondary Network \u00b6 It is also possible to connect VMs using additional networks with Harvester's built-in VLAN networks . In bridge VLAN, virtual machines are connected to the host network through a linux bridge . The network IPv4 address is delegated to the virtual machine via DHCPv4. The virtual machine should be configured to use DHCP to acquire IPv4 addresses. Advanced Options \u00b6 Run Strategy \u00b6 Available as of v1.0.2 Prior to v1.0.2, Harvester used the Running (a boolean) field to determine if the VM instance should be running. However, a simple boolean value is not always sufficient to fully describe the user's desired behavior. For example, in some cases the user wants to be able to shut down the instance from inside the virtual machine. If the running field is used, the VM will be restarted immediately. In order to meet the scenario requirements of more users, the RunStrategy field is introduced. This is mutually exclusive with Running because their conditions overlap somewhat. There are currently four RunStrategies defined: Always: The VM instance will always exist. If VM instance crashes, a new one will be spawned. This is the same behavior as Running: true . RerunOnFailure (default): If the previous instance failed in an error state, a VM instance will be respawned. If the guest is successfully stopped (e.g. shut down from inside the guest), it will not be recreated. Manual: The presence or absence of a VM instance is controlled only by the start/stop/restart VirtualMachine actions. Stop: There will be no VM instance. If the guest is already running, it will be stopped. This is the same behavior as Running: false . Cloud Configuration \u00b6 Harvester supports the ability to assign a startup script to a virtual machine instance which is executed automatically when the VM initializes. These scripts are commonly used to automate injection of users and SSH keys into VMs in order to provide remote access to the machine. For example, a startup script can be used to inject credentials into a VM that allows an Ansible job running on a remote host to access and provision the VM. Cloud-init \u00b6 Cloud-init is a widely adopted project and the industry standard multi-distribution method for cross-platform cloud instance initialization. It is supported across all major cloud image provider like SUSE, Redhat, Ubuntu and etc., cloud-init has established itself as the defacto method of providing startup scripts to VMs. Harvester supports injecting your custom cloud-init startup scripts into a VM instance through the use of an ephemeral disk. VMs with the cloud-init package installed will detect the ephemeral disk and execute custom user-data and network-data scripts at boot. Example of password configuration for the default user: #cloud-config password : password chpasswd : { expire : False } ssh_pwauth : True Example of network-data configuration using DHCP: version : 1 config : - type : physical name : eth0 subnets : - type : dhcp - type : physical name : eth1 subnets : - type : dhcp You can also use the Advanced > Cloud Config Templates feature to create a pre-defined cloud-init configuration template for the VM. Installing the QEMU guest agent \u00b6 The QEMU guest agent is a daemon that runs on the virtual machine instance and passes information to the host about the VM, users, file systems, and secondary networks. Install guest agent checkbox is enabled by default when a new VM is created. Note If your OS is openSUSE and the version is less than 15.3, please replace qemu-guest-agent.service with qemu-ga.service .","title":"Create a Virtual Machine"},{"location":"vm/create-vm/#create-a-virtual-machine","text":"","title":"Create a Virtual Machine"},{"location":"vm/create-vm/#how-to-create-a-vm","text":"You can create one or more virtual machines from the Virtual Machines page. Note Please refer to this page for creating Windows virtual machines. Choose the option to create either one or multiple VM instances. Select the namespace of your VMs, only the harvester-public namespace is visible to all users. The VM Name is a required field. (Optional) VM template is optional, you can choose iso-image , raw-image or windows-iso-image template to speed up your VM instance creation. Configure the virtual machine's CPU and memory (see overcommit settings if you want to over-provision). Select SSH keys or upload new keys. Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM. To configure networks, go to the Networks tab. The Management Network is added by default, you can remove it if the VLAN network is configured. You can also add additional networks to the VMs using VLAN networks. You may configure the VLAN networks on Advanced > Networks first. Advanced options such as run strategy, os type and cloud-init data are optional. You may configure these in the Advanced Options section when applicable.","title":"How to Create a VM"},{"location":"vm/create-vm/#volumes","text":"You can add one or more additional volumes via the Volumes tab, by default the first disk will be the root disk, you can change the boot order by dragging and dropping volumes, or using the arrow buttons. A disk can be made accessible via the following types: type description disk A disk disk will expose the volume as an ordinary disk to the VM. cd-rom A cd-rom disk will expose the volume as a cd-rom drive to the VM. It is read-only by default. Container Disk Container disks are ephemeral storage devices that can be assigned to any number of VMs. This makes them an ideal tool for users who want to replicate a large number of VM workloads or inject machine drivers that do not require persistent data. Note: Container disks are not a good solution for any workload that requires persistent root disks across VM restarts.","title":"Volumes"},{"location":"vm/create-vm/#networks","text":"You can choose to add both the management network or VLAN network to your VM instances via the Networks tab, the management network is optional if you have the VLAN network configured. Network interfaces are configured through the Type field. They describe the properties of the virtual interfaces seen inside the guest OS: type description bridge Connect using a Linux bridge masquerade Connect using iptables rules to NAT the traffic","title":"Networks"},{"location":"vm/create-vm/#management-network","text":"A management network represents the default VM eth0 interface configured by the cluster network solution that is present in each VM. By default, VMs are accessible through the management network within the cluster nodes.","title":"Management Network"},{"location":"vm/create-vm/#secondary-network","text":"It is also possible to connect VMs using additional networks with Harvester's built-in VLAN networks . In bridge VLAN, virtual machines are connected to the host network through a linux bridge . The network IPv4 address is delegated to the virtual machine via DHCPv4. The virtual machine should be configured to use DHCP to acquire IPv4 addresses.","title":"Secondary Network"},{"location":"vm/create-vm/#advanced-options","text":"","title":"Advanced Options"},{"location":"vm/create-vm/#run-strategy","text":"Available as of v1.0.2 Prior to v1.0.2, Harvester used the Running (a boolean) field to determine if the VM instance should be running. However, a simple boolean value is not always sufficient to fully describe the user's desired behavior. For example, in some cases the user wants to be able to shut down the instance from inside the virtual machine. If the running field is used, the VM will be restarted immediately. In order to meet the scenario requirements of more users, the RunStrategy field is introduced. This is mutually exclusive with Running because their conditions overlap somewhat. There are currently four RunStrategies defined: Always: The VM instance will always exist. If VM instance crashes, a new one will be spawned. This is the same behavior as Running: true . RerunOnFailure (default): If the previous instance failed in an error state, a VM instance will be respawned. If the guest is successfully stopped (e.g. shut down from inside the guest), it will not be recreated. Manual: The presence or absence of a VM instance is controlled only by the start/stop/restart VirtualMachine actions. Stop: There will be no VM instance. If the guest is already running, it will be stopped. This is the same behavior as Running: false .","title":"Run Strategy"},{"location":"vm/create-vm/#cloud-configuration","text":"Harvester supports the ability to assign a startup script to a virtual machine instance which is executed automatically when the VM initializes. These scripts are commonly used to automate injection of users and SSH keys into VMs in order to provide remote access to the machine. For example, a startup script can be used to inject credentials into a VM that allows an Ansible job running on a remote host to access and provision the VM.","title":"Cloud Configuration"},{"location":"vm/create-vm/#cloud-init","text":"Cloud-init is a widely adopted project and the industry standard multi-distribution method for cross-platform cloud instance initialization. It is supported across all major cloud image provider like SUSE, Redhat, Ubuntu and etc., cloud-init has established itself as the defacto method of providing startup scripts to VMs. Harvester supports injecting your custom cloud-init startup scripts into a VM instance through the use of an ephemeral disk. VMs with the cloud-init package installed will detect the ephemeral disk and execute custom user-data and network-data scripts at boot. Example of password configuration for the default user: #cloud-config password : password chpasswd : { expire : False } ssh_pwauth : True Example of network-data configuration using DHCP: version : 1 config : - type : physical name : eth0 subnets : - type : dhcp - type : physical name : eth1 subnets : - type : dhcp You can also use the Advanced > Cloud Config Templates feature to create a pre-defined cloud-init configuration template for the VM.","title":"Cloud-init"},{"location":"vm/create-vm/#installing-the-qemu-guest-agent","text":"The QEMU guest agent is a daemon that runs on the virtual machine instance and passes information to the host about the VM, users, file systems, and secondary networks. Install guest agent checkbox is enabled by default when a new VM is created. Note If your OS is openSUSE and the version is less than 15.3, please replace qemu-guest-agent.service with qemu-ga.service .","title":"Installing the QEMU guest agent"},{"location":"vm/create-windows-vm/","text":"Create a Windows Virtual Machine \u00b6 Create one or more virtual machines from the Virtual Machines page. Note For creating Linux virtual machines, please refer to this page . How to Create a Windows VM \u00b6 Header Section \u00b6 Create a single VM instance or multiple VM instances. Set the VM name. (Optional) Provide a description for the VM. (Optional) Select the VM template windows-iso-image-base-template . This template will add a volume with the virtio drivers for Windows. Basics Tab \u00b6 Configure the number of CPU cores assigned to the VM. Configure the amount of Memory assigned to the VM. (Optional) Select existing SSH keys or upload new ones. Note As mentioned above, it is recommended that you use the Windows VM template. The Volumes section will describe the options which the Windows VM template created automatically. Warning The bootOrder values need to be set with the installation image first. If you change it, your VM might not boot into the installation disk. Volumes Tab \u00b6 The first volume is an Image Volume with the following values: Name : The value cdrom-disk is set by default. You can keep it or change it. Image : Select the Windows image to be installed. See Upload Images for the full description on how to create new images. Type : Select cd-rom . Size : The value 20 is set by default. You can change it if your image has a bigger size. Bus : The value SATA is set by default. It's recommended you don't change it. The second volume is a Volume with the following values: Name : The value rootdisk is set by default. You can keep it or change it. Size : The value 32 is set by default. See the disk space requirements for Windows Server and Windows 11 before changing this value. Bus : The value VirtIO is set by default. You can keep it or change it to the other available options, SATA or SCSI . The third volume is a Container with the following values: Name : The value virtio-container-disk is set by default. You can keep it or change it. Docker Image : The value registry.suse.com/suse/vmdp/vmdp:2.5.3 is set by default. It's recommended you don't change it. Bus : The value SATA is set by default. It's recommended you don't change it. You can add additional disks using the buttons Add Volume , Add Existing Volume , Add VM Image , or Add Container . Networks Tab \u00b6 The Management Network is added by default with the following values: Name : The value default is set by default. You can keep it or change it. Network : The value management Network is set by default. You can't change this option if no other network has been created. See Harvester Network for the full description on how to create new networks. Model : The value e1000 is set by default. You can keep it or change it to the other available options from the dropdown. Type : The value masquerade is set by default. You can keep it or change it to the other available option, bridge . You can add additional networks by clicking Add Network . Warning Changing the Node Scheduling settings can impact Harvester features, such as disabling Live migration . Node Scheduling Tab \u00b6 Node Scheduling is set to Run VM on any available node by default. You can keep it or change it to the other available options from the dropdown. Advanced Options Tab \u00b6 OS Type : The value Windows is set by default. It's recommended you don't change it. Machine Type : The value None is set by default. It's recommended you don't change it. See the KubeVirt Machine Type documentation before you change this value. (Optional) Hostname : Set the VM hostname. (Optional) Cloud Config : Both User Data and Network Data values are set with default values. Currently, these configurations are not applied to Windows-based VMs. Footer Section \u00b6 Start virtual machine on creation : This option is checked by default. You can uncheck it if you don't want the VM to start once it's created. Once all the settings are in place, click on Create to create the VM. Note If you need to add advanced settings, you can edit the VM configuration directly by clicking on Edit as YAML . And if you want to cancel all changes made, click Cancel . Installation of Windows \u00b6 Select the VM you just created, and click Start to boot up the VM.(If you checked Start virtual machine on creation the VM will start automatically once it's created) Boot into the installer, and follow the instructions given by the installer. (Optional) If you are using virtio based volumes, you will need to load the specific driver to allow the installer to detect them. If you're using VM template windows-iso-image-base-template , the instruction is as follows: Click on Load driver , and then click Browse on the dialog box, and find a CD-ROM drive with a VMDP-WIN prefix. Next, find the driver directory according to the Windows version you're installing; for example, Windows Server 2012r2 should expand win8.1-2012r2 and choose the pvvx directory inside. Click OK to allow the installer to scan this directory for drivers, choose SUSE Block Driver for Windows , and click Next to load the driver. Wait for the installer to load up the driver. If you choose the correct driver version the virtio volumes will be detected once the driver is loaded. (Optional) If you are using other virtio based hardware like network adapter, you will need to install those drivers manually after completing the installation. To install drivers, open the VMDP driver disk, and use the installer based on your platform. The support matrix of VMDP driver pack for Windows are as follows (assume the VMDP CD-ROM drive path is E): Version Supported Driver path Windows 7 No N/A Windows Server 2008 No N/A Windows Server 2008r2 No N/A Windows 8 x86(x64) Yes E:\\win8-2012\\x86(x64)\\pvvx Windows Server 2012 x86(x64) Yes E:\\win8-2012\\x86(x64)\\pvvx Windows 8.1 x86(x64) Yes E:\\win8.1-2012r2\\x86(x64)\\pvvx Windows Server 2012r2 x86(x64) Yes E:\\win8.1-2012r2\\x86(x64)\\pvvx Windows 10 x86(x64) Yes E:\\win10-server\\x86(x64)\\pvvx Windows Server 2016 x86(x64) Yes E:\\win10-server\\x86(x64)\\pvvx Windows Server 2019 x86(x64) Yes E:\\win10-server\\x86(x64)\\pvvx Windows 11 x86(x64) Yes E:\\win10-2004\\x86(x64)\\pvvx Windows Server 2022 x86(x64) Yes E:\\win10-2004\\x86(x64)\\pvvx Note If you didn't use the windows-iso-image-base-template template, and you still need virtio devices, please make sure to add your custom Windows virtio driver to allow it to detect the hardware correctly. Known Issues \u00b6 Windows ISO unable to boot when using EFI mode \u00b6 When using EFI mode with Windows, you may find the system booted with other devices like HDD or UEFI shell like the one below: That's because Windows will prompt a Press any key to boot from CD or DVD... to let the user decide whether to boot from the installer ISO or not, and it needs human intervention to allow the system to boot from CD or DVD. Alternately if the system has already booted into the UEFI shell, you can type in reset to force the system to reboot again. Once the prompt appears you can press any key to let system boot from Windows ISO. VM crashes when reserved memory not enough \u00b6 There is a known issue with Windows VM when it is allocated more than 8GiB without enough reserve memory configured. The VM crashes without warning. This can be fixed by allocating at least 256MiB of reserved memory to the template on the Advanced Options tab. We will add a default 256MiB of reserved memory to the Windows template to prevent this problem in the future release. BSoD (Blue Screen of Death) at first boot time of Windows \u00b6 There is a known issue with Windows VM using Windows Server 2016 and above, a BSoD with error code KMODE_EXCEPTION_NOT_HANDLED may appears at the first boot time of Windows. We are still looking into it and will fix this issue in the future release. As a workaround, you can create or modify the file /etc/modprobe.d/kvm.conf within the installation of Harvester by updating /oem/99_custom.yaml like below: name : Harvester Configuration stages : initramfs : - commands : # ... files : - path : /etc/modprobe.d/kvm.conf permissions : 384 owner : 0 group : 0 content : | options kvm ignore_msrs=1 encoding : \"\" ownerstring : \"\" # ... Note This is still an experimental solution. For more information, please refer to this issue and please let us know if you have encountered any issues after applying this workaround.","title":"Create a Windows Virtual Machine"},{"location":"vm/create-windows-vm/#create-a-windows-virtual-machine","text":"Create one or more virtual machines from the Virtual Machines page. Note For creating Linux virtual machines, please refer to this page .","title":"Create a Windows Virtual Machine"},{"location":"vm/create-windows-vm/#how-to-create-a-windows-vm","text":"","title":"How to Create a Windows VM"},{"location":"vm/create-windows-vm/#header-section","text":"Create a single VM instance or multiple VM instances. Set the VM name. (Optional) Provide a description for the VM. (Optional) Select the VM template windows-iso-image-base-template . This template will add a volume with the virtio drivers for Windows.","title":"Header Section"},{"location":"vm/create-windows-vm/#basics-tab","text":"Configure the number of CPU cores assigned to the VM. Configure the amount of Memory assigned to the VM. (Optional) Select existing SSH keys or upload new ones. Note As mentioned above, it is recommended that you use the Windows VM template. The Volumes section will describe the options which the Windows VM template created automatically. Warning The bootOrder values need to be set with the installation image first. If you change it, your VM might not boot into the installation disk.","title":"Basics Tab"},{"location":"vm/create-windows-vm/#volumes-tab","text":"The first volume is an Image Volume with the following values: Name : The value cdrom-disk is set by default. You can keep it or change it. Image : Select the Windows image to be installed. See Upload Images for the full description on how to create new images. Type : Select cd-rom . Size : The value 20 is set by default. You can change it if your image has a bigger size. Bus : The value SATA is set by default. It's recommended you don't change it. The second volume is a Volume with the following values: Name : The value rootdisk is set by default. You can keep it or change it. Size : The value 32 is set by default. See the disk space requirements for Windows Server and Windows 11 before changing this value. Bus : The value VirtIO is set by default. You can keep it or change it to the other available options, SATA or SCSI . The third volume is a Container with the following values: Name : The value virtio-container-disk is set by default. You can keep it or change it. Docker Image : The value registry.suse.com/suse/vmdp/vmdp:2.5.3 is set by default. It's recommended you don't change it. Bus : The value SATA is set by default. It's recommended you don't change it. You can add additional disks using the buttons Add Volume , Add Existing Volume , Add VM Image , or Add Container .","title":"Volumes Tab"},{"location":"vm/create-windows-vm/#networks-tab","text":"The Management Network is added by default with the following values: Name : The value default is set by default. You can keep it or change it. Network : The value management Network is set by default. You can't change this option if no other network has been created. See Harvester Network for the full description on how to create new networks. Model : The value e1000 is set by default. You can keep it or change it to the other available options from the dropdown. Type : The value masquerade is set by default. You can keep it or change it to the other available option, bridge . You can add additional networks by clicking Add Network . Warning Changing the Node Scheduling settings can impact Harvester features, such as disabling Live migration .","title":"Networks Tab"},{"location":"vm/create-windows-vm/#node-scheduling-tab","text":"Node Scheduling is set to Run VM on any available node by default. You can keep it or change it to the other available options from the dropdown.","title":"Node Scheduling Tab"},{"location":"vm/create-windows-vm/#advanced-options-tab","text":"OS Type : The value Windows is set by default. It's recommended you don't change it. Machine Type : The value None is set by default. It's recommended you don't change it. See the KubeVirt Machine Type documentation before you change this value. (Optional) Hostname : Set the VM hostname. (Optional) Cloud Config : Both User Data and Network Data values are set with default values. Currently, these configurations are not applied to Windows-based VMs.","title":"Advanced Options Tab"},{"location":"vm/create-windows-vm/#footer-section","text":"Start virtual machine on creation : This option is checked by default. You can uncheck it if you don't want the VM to start once it's created. Once all the settings are in place, click on Create to create the VM. Note If you need to add advanced settings, you can edit the VM configuration directly by clicking on Edit as YAML . And if you want to cancel all changes made, click Cancel .","title":"Footer Section"},{"location":"vm/create-windows-vm/#installation-of-windows","text":"Select the VM you just created, and click Start to boot up the VM.(If you checked Start virtual machine on creation the VM will start automatically once it's created) Boot into the installer, and follow the instructions given by the installer. (Optional) If you are using virtio based volumes, you will need to load the specific driver to allow the installer to detect them. If you're using VM template windows-iso-image-base-template , the instruction is as follows: Click on Load driver , and then click Browse on the dialog box, and find a CD-ROM drive with a VMDP-WIN prefix. Next, find the driver directory according to the Windows version you're installing; for example, Windows Server 2012r2 should expand win8.1-2012r2 and choose the pvvx directory inside. Click OK to allow the installer to scan this directory for drivers, choose SUSE Block Driver for Windows , and click Next to load the driver. Wait for the installer to load up the driver. If you choose the correct driver version the virtio volumes will be detected once the driver is loaded. (Optional) If you are using other virtio based hardware like network adapter, you will need to install those drivers manually after completing the installation. To install drivers, open the VMDP driver disk, and use the installer based on your platform. The support matrix of VMDP driver pack for Windows are as follows (assume the VMDP CD-ROM drive path is E): Version Supported Driver path Windows 7 No N/A Windows Server 2008 No N/A Windows Server 2008r2 No N/A Windows 8 x86(x64) Yes E:\\win8-2012\\x86(x64)\\pvvx Windows Server 2012 x86(x64) Yes E:\\win8-2012\\x86(x64)\\pvvx Windows 8.1 x86(x64) Yes E:\\win8.1-2012r2\\x86(x64)\\pvvx Windows Server 2012r2 x86(x64) Yes E:\\win8.1-2012r2\\x86(x64)\\pvvx Windows 10 x86(x64) Yes E:\\win10-server\\x86(x64)\\pvvx Windows Server 2016 x86(x64) Yes E:\\win10-server\\x86(x64)\\pvvx Windows Server 2019 x86(x64) Yes E:\\win10-server\\x86(x64)\\pvvx Windows 11 x86(x64) Yes E:\\win10-2004\\x86(x64)\\pvvx Windows Server 2022 x86(x64) Yes E:\\win10-2004\\x86(x64)\\pvvx Note If you didn't use the windows-iso-image-base-template template, and you still need virtio devices, please make sure to add your custom Windows virtio driver to allow it to detect the hardware correctly.","title":"Installation of Windows"},{"location":"vm/create-windows-vm/#known-issues","text":"","title":"Known Issues"},{"location":"vm/create-windows-vm/#windows-iso-unable-to-boot-when-using-efi-mode","text":"When using EFI mode with Windows, you may find the system booted with other devices like HDD or UEFI shell like the one below: That's because Windows will prompt a Press any key to boot from CD or DVD... to let the user decide whether to boot from the installer ISO or not, and it needs human intervention to allow the system to boot from CD or DVD. Alternately if the system has already booted into the UEFI shell, you can type in reset to force the system to reboot again. Once the prompt appears you can press any key to let system boot from Windows ISO.","title":"Windows ISO unable to boot when using EFI mode"},{"location":"vm/create-windows-vm/#vm-crashes-when-reserved-memory-not-enough","text":"There is a known issue with Windows VM when it is allocated more than 8GiB without enough reserve memory configured. The VM crashes without warning. This can be fixed by allocating at least 256MiB of reserved memory to the template on the Advanced Options tab. We will add a default 256MiB of reserved memory to the Windows template to prevent this problem in the future release.","title":"VM crashes when reserved memory not enough"},{"location":"vm/create-windows-vm/#bsod-blue-screen-of-death-at-first-boot-time-of-windows","text":"There is a known issue with Windows VM using Windows Server 2016 and above, a BSoD with error code KMODE_EXCEPTION_NOT_HANDLED may appears at the first boot time of Windows. We are still looking into it and will fix this issue in the future release. As a workaround, you can create or modify the file /etc/modprobe.d/kvm.conf within the installation of Harvester by updating /oem/99_custom.yaml like below: name : Harvester Configuration stages : initramfs : - commands : # ... files : - path : /etc/modprobe.d/kvm.conf permissions : 384 owner : 0 group : 0 content : | options kvm ignore_msrs=1 encoding : \"\" ownerstring : \"\" # ... Note This is still an experimental solution. For more information, please refer to this issue and please let us know if you have encountered any issues after applying this workaround.","title":"BSoD (Blue Screen of Death) at first boot time of Windows"},{"location":"vm/edit-vm/","text":"Edit a Virtual Machine \u00b6 How to Edit a VM \u00b6 After creating a virtual machine, you can edit your virtual machine by clicking the \u22ee button and selecting the Edit Configurations button. Notes In addition to editing the description, a restart of the virtual machine is required for configuration changes to take effect. Basics \u00b6 On the basics tab, you can config your requested CPU and memory, a VM restart is required for this configuration to take effect. SSH Keys are injected into the cloud-init script when the virtual machine is first powered on. In order for the modified ssh key to take effect after the virtual machine is startup, the cloud-init script needs to be reinstalled from your guest OS. Networks \u00b6 You can add additional VLAN networks to your VM instances after booting, the management network is optional if you have the VLAN network configured. Additional NICs are not enabled by default unless you configure them manually in the guest OS, e.g. using wicked for your OpenSUSE Server or netplan for your Ubuntu Server . For more details about the network implementation, please refer to the Networking page. Volumes \u00b6 You can add additional volumes to the VM after booting. You can also expand the size of the volume after shutting down the VM, click the VM and go to the Volumes tab, then click Edit Image Volume to edit the size of the expanded volume. After waiting for the resize to complete and restarting the VM, your disk will automatically finish expanding. Access Credentials \u00b6 Access Credentials allow you to inject basic auth or ssh keys dynamically at run time when your guest OS has quemu guest agent installed. For more details please check the page here: Dynamic SSH Key Injection via Qemu guest agent .","title":"Edit a Virtual Machine"},{"location":"vm/edit-vm/#edit-a-virtual-machine","text":"","title":"Edit a Virtual Machine"},{"location":"vm/edit-vm/#how-to-edit-a-vm","text":"After creating a virtual machine, you can edit your virtual machine by clicking the \u22ee button and selecting the Edit Configurations button. Notes In addition to editing the description, a restart of the virtual machine is required for configuration changes to take effect.","title":"How to Edit a VM"},{"location":"vm/edit-vm/#basics","text":"On the basics tab, you can config your requested CPU and memory, a VM restart is required for this configuration to take effect. SSH Keys are injected into the cloud-init script when the virtual machine is first powered on. In order for the modified ssh key to take effect after the virtual machine is startup, the cloud-init script needs to be reinstalled from your guest OS.","title":"Basics"},{"location":"vm/edit-vm/#networks","text":"You can add additional VLAN networks to your VM instances after booting, the management network is optional if you have the VLAN network configured. Additional NICs are not enabled by default unless you configure them manually in the guest OS, e.g. using wicked for your OpenSUSE Server or netplan for your Ubuntu Server . For more details about the network implementation, please refer to the Networking page.","title":"Networks"},{"location":"vm/edit-vm/#volumes","text":"You can add additional volumes to the VM after booting. You can also expand the size of the volume after shutting down the VM, click the VM and go to the Volumes tab, then click Edit Image Volume to edit the size of the expanded volume. After waiting for the resize to complete and restarting the VM, your disk will automatically finish expanding.","title":"Volumes"},{"location":"vm/edit-vm/#access-credentials","text":"Access Credentials allow you to inject basic auth or ssh keys dynamically at run time when your guest OS has quemu guest agent installed. For more details please check the page here: Dynamic SSH Key Injection via Qemu guest agent .","title":"Access Credentials"},{"location":"vm/hotplug-volume/","text":"Hot-Plug Volumes \u00b6 Harvester supports adding hot-plug volumes to a running VM. Adding Hot-Plug Volumes to a Running VM \u00b6 The following steps assume that you have a running VM and a ready volume: Go to the Virtual Machines page. Find the VM that you want to add a volume to and select \u22ee > Add Volume . Enter the Name and select the Volume . Click Apply .","title":"Hot-Plug Volumes"},{"location":"vm/hotplug-volume/#hot-plug-volumes","text":"Harvester supports adding hot-plug volumes to a running VM.","title":"Hot-Plug Volumes"},{"location":"vm/hotplug-volume/#adding-hot-plug-volumes-to-a-running-vm","text":"The following steps assume that you have a running VM and a ready volume: Go to the Virtual Machines page. Find the VM that you want to add a volume to and select \u22ee > Add Volume . Enter the Name and select the Volume . Click Apply .","title":"Adding Hot-Plug Volumes to a Running VM"},{"location":"vm/live-migration/","text":"Live Migration \u00b6 Live migration means moving a virtual machine to a different host without downtime. Note Live migration is not allowed when the virtual machine is using a management network of bridge interface type. To support live migration, three or more hosts in the Harvester cluster are required due to a known issue . Starting a Migration \u00b6 Go to the Virtual Machines page. Find the virtual machine that you want to migrate and select \u22ee > Migrate . Choose the node to which you want to migrate the virtual machine. Click Apply . Aborting a Migration \u00b6 Go to the Virtual Machines page. Find the virtual machine in migrating status that you want to abort. Select \u22ee > Abort Migration . Migration Timeouts \u00b6 Completion Timeout \u00b6 The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied. As a result, the migration process is prevented from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout of 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds. Progress Timeout \u00b6 Live migration will also be aborted when copying memory doesn't make any progress in 150s.","title":"Live Migration"},{"location":"vm/live-migration/#live-migration","text":"Live migration means moving a virtual machine to a different host without downtime. Note Live migration is not allowed when the virtual machine is using a management network of bridge interface type. To support live migration, three or more hosts in the Harvester cluster are required due to a known issue .","title":"Live Migration"},{"location":"vm/live-migration/#starting-a-migration","text":"Go to the Virtual Machines page. Find the virtual machine that you want to migrate and select \u22ee > Migrate . Choose the node to which you want to migrate the virtual machine. Click Apply .","title":"Starting a Migration"},{"location":"vm/live-migration/#aborting-a-migration","text":"Go to the Virtual Machines page. Find the virtual machine in migrating status that you want to abort. Select \u22ee > Abort Migration .","title":"Aborting a Migration"},{"location":"vm/live-migration/#migration-timeouts","text":"","title":"Migration Timeouts"},{"location":"vm/live-migration/#completion-timeout","text":"The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied. As a result, the migration process is prevented from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout of 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds.","title":"Completion Timeout"},{"location":"vm/live-migration/#progress-timeout","text":"Live migration will also be aborted when copying memory doesn't make any progress in 150s.","title":"Progress Timeout"},{"location":"vm/resource-overcommit/","text":"Resource Overcommit \u00b6 Harvester supports global configuration of resource overload percentages on CPU, memory, and storage. By setting overcommit-config , this will allow scheduling of additional virtual machines even when physical resources are fully utilized. Harvester allows you to overcommit CPU and RAM on compute nodes. This allows you to increase the number of instances running on your cloud at the cost of reducing the performance of the instances. The Compute service uses the following ratios by default: CPU allocation ratio: 1600% RAM allocation ratio: 150% Storage allocation ratio: 200% Note Classic memory overcommitment or memory ballooning is not yet supported by this feature. In other words, memory used by a virtual machine instance cannot be returned once allocated. Configure the global setting overcommit-config \u00b6 Users can modify the global overcommit-config by following the steps below, and it will be applied to each newly created virtual machine after the change. Go to the Advanced > Settings page. Find the overcommit-config setting. Configure the desired CPU, Memory, and Storage ratio. Configure overcommit for a single virtual machine \u00b6 If you need to configure individual virtual machines without involving global configuration, consider adjusting the spec.template.spec.domain.resources.<memory|cpu> value on the target VirtualMachine resource individually. Note that by modifying these values, you are taking over control of virtual machine resource management from Harvester. Reserve more memory for the system overhead \u00b6 By default, the Harvester reserves a certain amount of system management overhead memory from the memory allocated for the virtual machine. In most cases, this will not cause any problems. However, some operating systems, such as Windows 2022, will request more memory than is reserved. To address the issue, Harvester provides an annotation harvesterhci.io/reservedMemory on VirtualMachine custom resource to let you specify the amount of memory to reserve. For instance, add harvesterhci.io/reservedMemory: 200Mi if you decide to reserve 200 MiB for the system overhead of the VM. apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: annotations: + harvesterhci.io/reservedMemory: 200Mi kubevirt.io/latest-observed-api-version: v1 kubevirt.io/storage-observed-api-version: v1alpha3 network.harvesterhci.io/ips: '[]' ... ... Why my virtual machines are scheduled unevenly? \u00b6 The scheduling of virtual machines depends on the underlying behavior of the kube-scheduler. We have a dedicated article explaining the details. If you would like to learn more, check out: Harvester Knowledge Base: VM Scheduling .","title":"Resource Overcommit"},{"location":"vm/resource-overcommit/#resource-overcommit","text":"Harvester supports global configuration of resource overload percentages on CPU, memory, and storage. By setting overcommit-config , this will allow scheduling of additional virtual machines even when physical resources are fully utilized. Harvester allows you to overcommit CPU and RAM on compute nodes. This allows you to increase the number of instances running on your cloud at the cost of reducing the performance of the instances. The Compute service uses the following ratios by default: CPU allocation ratio: 1600% RAM allocation ratio: 150% Storage allocation ratio: 200% Note Classic memory overcommitment or memory ballooning is not yet supported by this feature. In other words, memory used by a virtual machine instance cannot be returned once allocated.","title":"Resource Overcommit"},{"location":"vm/resource-overcommit/#configure-the-global-setting-overcommit-config","text":"Users can modify the global overcommit-config by following the steps below, and it will be applied to each newly created virtual machine after the change. Go to the Advanced > Settings page. Find the overcommit-config setting. Configure the desired CPU, Memory, and Storage ratio.","title":"Configure the global setting overcommit-config"},{"location":"vm/resource-overcommit/#configure-overcommit-for-a-single-virtual-machine","text":"If you need to configure individual virtual machines without involving global configuration, consider adjusting the spec.template.spec.domain.resources.<memory|cpu> value on the target VirtualMachine resource individually. Note that by modifying these values, you are taking over control of virtual machine resource management from Harvester.","title":"Configure overcommit for a single virtual machine"},{"location":"vm/resource-overcommit/#reserve-more-memory-for-the-system-overhead","text":"By default, the Harvester reserves a certain amount of system management overhead memory from the memory allocated for the virtual machine. In most cases, this will not cause any problems. However, some operating systems, such as Windows 2022, will request more memory than is reserved. To address the issue, Harvester provides an annotation harvesterhci.io/reservedMemory on VirtualMachine custom resource to let you specify the amount of memory to reserve. For instance, add harvesterhci.io/reservedMemory: 200Mi if you decide to reserve 200 MiB for the system overhead of the VM. apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: annotations: + harvesterhci.io/reservedMemory: 200Mi kubevirt.io/latest-observed-api-version: v1 kubevirt.io/storage-observed-api-version: v1alpha3 network.harvesterhci.io/ips: '[]' ... ...","title":"Reserve more memory for the system overhead"},{"location":"vm/resource-overcommit/#why-my-virtual-machines-are-scheduled-unevenly","text":"The scheduling of virtual machines depends on the underlying behavior of the kube-scheduler. We have a dedicated article explaining the details. If you would like to learn more, check out: Harvester Knowledge Base: VM Scheduling .","title":"Why my virtual machines are scheduled unevenly?"}]}