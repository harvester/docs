---
sidebar_position: 2
sidebar_label: 从 v1.1.2 升级到 v1.2.0
title: "从 v1.1.2 升级到 v1.2.0"
---

## 通用信息

:::tip

在开始升级之前，你可以运行预检查脚本以确保集群处于稳定状态。有关更多详细信息，请访问该脚本的 [URL](https://github.com/harvester/upgrade-helpers/tree/main/pre-check/v1.1.x)。
:::

一旦有了可升级的版本，Harvester GUI Dashboard 页面将显示一个升级按钮。有关详细信息，请参阅[开始升级](./automatic.md#开始升级)。

对于离线环境升级，请参阅[准备离线升级](./automatic.md#准备离线升级)。


## 已知问题

---

### 1. 升级无法启动并报告 `"validator.harvesterhci.io" denied the request: managed chart rancher-monitoring is not ready, please wait for it to be ready`

如果集群配置了**存储网络**，升级将无法开始并显示以下消息。

![](/img/v1.2/upgrade/known_issues/3839-error.png)

- 相关 issue：
   - [[Doc] upgrade stuck while upgrading system service with alertmanager and prometheus](https://github.com/harvester/harvester/issues/3839)
- 解决方法：
   - https://github.com/harvester/harvester/issues/3839#issuecomment-1534438192

---

### 2. 升级卡在 `Creating Upgrade Repository`

升级期间，**Creating Upgrade Repository** 卡在 **Pending** 状态：

![](/img/v1.2/upgrade/known_issues/4246-pending.png)

请执行以下步骤检查集群是否遇到问题：

1. 检查升级仓库 pod：

   ![](/img/v1.2/upgrade/known_issues/4246-upgrade-repo-pod.png)

   如果 `virt-launcher-upgrade-repo-hvst-<upgrade-name>` pod 停留在 `ContainerCreating` 中，你的集群可能遇到了此问题。在这种情况下，请继续步骤 2。

2. 在 Longhorn GUI 中检查升级仓库卷。

   1. [Go to Longhorn GUI](../troubleshooting/harvester.md#访问嵌入式-rancher-和-longhorn-仪表板).
   2. 导航至 **Volume** 页面。
   3. 检查升级仓库虚拟机卷。它应该附加到名为 `virt-launcher-upgrade-repo-hvst-<upgrade-name>` 的 pod。如果卷的副本之一状态为 `Stopped`（灰色），则集群遇到了问题。

   ![](/img/v1.2/upgrade/known_issues/4246-pending-replica.png)

- 相关 issue：
   - [[BUG] upgrade stuck on create upgrade VM](https://github.com/harvester/harvester/issues/4246)
- 解决方法：
   - 从 Longhorn GUI 中删除 `Stopped` 副本。或者：
   - [重新开始升级](./troubleshooting.md#重新开始升级)。

---

### 3. 升级卡在预清空节点状态

从 v1.1.0 开始，Harvester 将等待所有卷状态都是健康（节点数量 >= 3 时）后再升级节点。通常，如果升级卡在 “pre-draining” 状态，你可以检查卷的运行状况。

访问 ["Access Embedded Longhorn"](../troubleshooting/harvester.md#访问嵌入式-rancher-和-longhorn-仪表板) 了解如何访问嵌入式 Longhorn GUI。

你还可以检查预清空作业日志。请参考故障排除指南中的[阶段 4：升级节点](./troubleshooting.md#阶段-4升级节点)。

---

### 4. 升级卡在第一个节点：Job was active longer than specified deadline

升级失败，如下截图所示：

![](/img/v1.2/upgrade/known_issues/2894-deadline.png)


- 相关 issue：
   - [[BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline](https://github.com/harvester/harvester/issues/2894)
- 解决方法：
   - https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690


---

### 5. 升级卡在 Pre-drained 状态

你可能会看到升级卡在“Pre-drained”状态：

![](/img/v1.2/upgrade/known_issues/3730-stuck.png)

在此阶段，Kubernetes 应该清空节点上的工作负载，但某些原因可能会导致进程停滞。

#### 5.1 该节点包含一个为单副本卷提供服务的 Longhorn `instance-manager-r` pod

如果节点包含卷的最后一个幸存副本，Longhorn 将不允许清空该节点。要检查节点是否遇到这种情况，请按照下列步骤操作：

1. 使用以下命令列出单副本卷：

   ```
   kubectl get volumes.longhorn.io -A -o yaml | yq '.items[] | select(.spec.numberOfReplicas == 1) | .metadata.namespace + "/" + .metadata.name'
   ```

   例如：
   ```
   $ kubectl get volumes.longhorn.io -A -o yaml | yq '.items[] | select(.spec.numberOfReplicas == 1) | .metadata.namespace + "/" + .metadata.name'
   longhorn-system/pvc-d1f19bab-200e-483b-b348-c87cfbba85ab
   ```

2. 检查副本是否在卡住的节点上：

   使用以下命令列出卷副本的 NodeID：
   ```
   kubectl get replicas.longhorn.io -n longhorn-system -o yaml | yq '.items[] | select(.metadata.labels.longhornvolume == "<volume>") | .spec.nodeID'
   ```

   例如：
   ```
   $ kubectl get replicas.longhorn.io -n longhorn-system -o yaml | yq '.items[] | select(.metadata.labels.longhornvolume == "pvc-d1f19bab-200e-483b-b348-c87cfbba85ab") | .spec.nodeID'
   node1
   ```

   如果结果显示副本留在升级停滞的节点上（在本示例中为 node1），则你的集群遇到了此问题。

有几种方法可以解决这种情况。请选择最适合你的虚拟机的方法：

1. 关闭使用单副本卷的 VM 以分离该卷，从而允许升级继续。
1. 将卷的副本调整为多个。
   1. [Go to Longhorn GUI](../troubleshooting/harvester.md#访问嵌入式-rancher-和-longhorn-仪表板).
   1. 转到 **Volume** 页面。
   1. 找到有问题的卷并单击右侧的图标，然后选择 **Update Replicas Count**：
      ![](/img/v1.2/upgrade/known_issues/4249-adjust-volume-replica.png)
   1. 增加 **Number of Replicas** 并选择 **OK**。

#### 5.2 错误配置的 Longhorn `instance-manager-r` Pod Disruption Budget (PDB)

配置错误的 PDB 可能会导致此问题。要检查是否属于这种情况，请执行以下步骤：

1. 假设卡住的节点是 `harvester-node-1`。
1. 检查节点上的 `instance-manager-e` 或 `instance-manager-r` pod 名称：

   ```
   $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager
   instance-manager-r-d4ed2788          1/1     Running   0              3d8h
   ```

   上方的输出表示 `instance-manager-r-d4ed2788` pod 在节点上。

1. 检查 Rancher 日志并验证 `instance-manager-e` 或 `instance-manager-r` pod 不会被清空：

   ```
   $ kubectl logs deployment/rancher -n cattle-system
   ...
   2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def
   2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788
   2023-03-28T17:10:55.080933607Z error when evicting pods/"instance-manager-r-d4ed2788" -n "longhorn-system" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
   ```

1. 运行以下命令，检查是否存在与卡住的节点关联的 PDB：

   ```
   $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels."longhorn.io/node"=="harvester-node-1") | .metadata.name'
   instance-manager-r-466e3c7f
   ```

1. 检查此 PDB 的 instance manager 的所有者：

   ```
   $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID'
   harvester-node-2
   ```

   如果输出与卡住节点不匹配（在此示例输出中，即 `harvester-node-2` 与卡住节点 `harvester-node-1` 不匹配），我们可以认为这个问题发生了。

1. 在应用解决方法之前，请检查所有卷是否正常：

   ```
   kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == "attached")| .status.robustness'
   ```

   输出应该都是 `healthy`。如果不是这种情况，你可能需要取消封锁节点来让卷恢复 healthy 状态。

1. 删除配置错误的 PDB：

   ```
   kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system
   ```

- 相关 issue：
   - [[BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0->v1.1.2-rc4](https://github.com/harvester/harvester/issues/3730)

#### 5.3 `instance-manager-e` pod 无法被清空

在升级过程中，你可能会遇到无法清空 `instance-manager-e` pod 的问题。发生这种情况时，你将在 Rancher 日志中看到如下错误消息：

```
$ kubectl logs deployment/rancher -n cattle-system | grep "evicting pod"
evicting pod longhorn-system/instance-manager-r-a06a43f3437ab4f643eea7053b915a80
evicting pod longhorn-system/instance-manager-e-452e87d2
error when evicting pods/"instance-manager-r-a06a43f3437ab4f643eea7053b915a80" -n "Longhorn-system" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
error when evicting pods/"instance-manager-e-452e87d2" -n "longhorn-system" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
```

检查 `instance-manager-e` 查看是否还有任何引擎实例。

```
$ kubectl get instancemanager instance-manager-e-452e87d2 -n longhorn-system -o yaml | yq -e ".status.instances"
pvc-7b120d60-1577-4716-be5a-62348271025a-e-1cd53c57:
  spec:
    name: pvc-7b120d60-1577-4716-be5a-62348271025a-e-1cd53c57
  status:
    endpoint: ""
    errorMsg: ""
    listen: ""
    portEnd: 10001
    portStart: 10001
    resourceVersion: 0
    state: running
    type: ""
```

在此示例中，`instance-manager-e-452e87d2` 仍然有一个引擎实例，因此你无法清空 pod。

你需要检查引擎数量，查看是否有多余的引擎编号。每个 PVC 应该只有一个引擎。

```
# kubectl get engines -n longhorn-system -l longhornvolume=pvc-7b120d60-1577-4716-be5a-62348271025a
NAME                                                  STATE     NODE               INSTANCEMANAGER                                      IMAGE                               AGE
pvc-76120d60-1577-4716-be5a-62348271025a-e-08220662   running   harvester-qv4hd    instance-manager-e-625d715e2f2e7065d64339f9b31407c2  longhornio/longhorn-engine:v1.4.3   2d12h
pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57   running   harvester-lhlkv    instance-manager-e-452e87d2                          longhornio/longhorn-engine:v1.4.3   4d10h
```

以上示例显示同一 PVC 存在两个引擎，这是 Longhorn [#6642](https://github.com/longhorn/longhorn/issues/6642) 中的一个已知问题。要解决此问题，请删除多余的引擎以继续升级。

要确定哪个引擎是正确的，请使用以下命令：

```
$ kubectl get volumes pvc-7b120d60-1577-4716-be5a-62348271025a -n longhorn-system
NAME                                      STATE     ROBUSTNESS  SCHEDULED SIZE        NODE            AGE
pvc-7b120d60-1577-4716-be5a-62348271025a  attached  healthy               42949672960 harvester-q4vhd 4d10h
```

在此示例中，卷 `pvc-7b120d60-1577-4716-be5a-62348271025a` 在节点 `harvester-q4vhd` 上处于活动状态，表明未在此节点上运行的引擎是多余的。

要停用引擎并触发 Longhorn 自动删除，请运行以下命令：

```
$ kubectl patch engine pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 -n longhorn-system --type='json' -p='[{"op": "replace", "path": "/spec/active", "value": false}]'
engine.longhorn.io/pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 patched
```

几秒钟后，你可以验证引擎的状态：

```
$ kubectl get engine -n longhorn-system|grep pvc-7b120d60-1577-4716-be5a-62348271025a
pvc-7b120d60-1577-4716-be5a-62348271025a-e-08220b62   running  harvester-q4vhd   instance-manager-e-625d715e2f2e7065d64339f9631407c2  longhornio/longhorn-engine:v1.4.3   2d13h
```

`instance-manager-e` Pod 现在应该已成功清空，你可以继续升级。

- 相关 issue：
   - [[BUG] Upgrade (v1.1.2 -> v1.2.0-rc6) stuck in pre-drained](https://github.com/harvester/harvester/issues/4477)

---

### 6. 升级卡在 Upgrading System Service 状态

如果升级长时间停留在 **Upgrading System Service** 状态，你可能需要调查升级是否停留在 `apply-manifests` 阶段。

![](/img/v1.2/upgrade/known_issues/4484-apply-manifests-stuck.png)

1. 检查 `apply-manifests` pod 的日志以查看以下消息是否重复。

   ```
   $ kubectl -n harvester-system logs hvst-upgrade-md6wr-apply-manifests-wqslg --tail=10
   Tue Sep  5 10:20:39 UTC 2023
   there are still 1 pods in cattle-monitoring-system to be deleted
   Tue Sep  5 10:20:45 UTC 2023
   there are still 1 pods in cattle-monitoring-system to be deleted
   Tue Sep  5 10:20:50 UTC 2023
   there are still 1 pods in cattle-monitoring-system to be deleted
   Tue Sep  5 10:20:55 UTC 2023
   there are still 1 pods in cattle-monitoring-system to be deleted
   Tue Sep  5 10:21:00 UTC 2023
   there are still 1 pods in cattle-monitoring-system to be deleted
   ```

1. 检查 `prometheus-rancher-monitoring-prometheus-0` Pod 是否停留在 `Terminate` 状态。

   ```
   $ kubectl -n cattle-monitoring-system get pods                                  
   NAME                                         READY   STATUS        RESTARTS   AGE
   prometheus-rancher-monitoring-prometheus-0   0/3     Terminating   0          19d
   ```

1. 使用以下命令查找终止 pod 的 UID：

   ```
   $ kubectl -n cattle-monitoring-system get pod prometheus-rancher-monitoring-prometheus-0 -o jsonpath='{.metadata.uid}'
   33f43165-6faa-4648-927d-69097901471c
   ```

1. 通过控制台或 SSH 访问集群的任何节点。
1. 使用 Pod 的 UID 在 `/var/lib/rancher/rke2/agent/logs/kubelet.log` 中搜索相关日志消息。

   ```
   E0905 10:26:18.769199   17399 reconciler.go:208] "operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \"pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\" (UniqueName: \"kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\") pod \"33f43165-6faa-4648-927d-69097901471c\" (UID: \"33f43165-6faa-4648-927d-69097901471c\") : UnmountVolume.NewUnmounter failed for volume \"pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\" (UniqueName: \"kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\") pod \"33f43165-6faa-4648-927d-69097901471c\" (UID: \"33f43165-6faa-4648-927d-69097901471c\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json]: open /var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json: no such file or directory" err="UnmountVolume.NewUnmounter failed for volume \"pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\" (UniqueName: \"kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\") pod \"33f43165-6faa-4648-927d-69097901471c\" (UID: \"33f43165-6faa-4648-927d-69097901471c\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json]: open /var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json: no such file or directory"
   ```

   如果 kubelet 继续报告卷无法卸载，请应用以下解决方法来允许升级继续进行。

1. 使用以下命令强制删除状态为 `Terminate` 的 pod：

   ```
   kubectl delete pod prometheus-rancher-monitoring-prometheus-0 -n cattle-monitoring-system  --force
   ```

- 相关 issue：
   - [[BUG] The rancher-monitoring Pod stuck at terminating status when upgrading from v1.1.2 to v1.2.0-rc6](https://github.com/harvester/harvester/issues/4484)

---

### 7. 升级卡在 `Upgrading System Service` 状态

如果升级长时间停留在 `Upgrading System Service` 状态，则某些系统服务的证书可能已过期。要调查并解决此问题，请按照下列步骤操作：

1. 使用以下命令查找 `apply-manifest` job 的名称：

   ```
   kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=manifest
   ```

   示例输出：
   ```
   NAME                                 COMPLETIONS   DURATION   AGE
   hvst-upgrade-9gmg2-apply-manifests   0/1           46s        46s
   ```

1. 使用以下命令检查作业日志：

   ```
   kubectl logs jobs/hvst-upgrade-9gmg2-apply-manifests -n harvester-system
   ```

   如果日志中出现以下消息，请继续执行下一步：

   ```
   Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)...
   Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)...
   Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)...
   Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)...
   ```

1. 使用以下命令检查 CAPI 集群的状态：

   ```
   kubectl get clusters.provisioning.cattle.io local -n fleet-local -o yaml
   ```

   如果你看到类似以下的情况，则集群可能遇到了该问题：
   ```
       - lastUpdateTime: "2023-01-17T16:26:48Z"
         message: 'configuring bootstrap node(s) custom-24cb32ce8387: waiting for probes:
           kube-controller-manager, kube-scheduler'
         reason: Waiting
         status: Unknown
         type: Updated
   ```

1. 使用以下命令查找计算机的主机名，然后按照[解决方法](https://github.com/harvester/harvester/issues/3863#issuecomment-1539681311)查看节点上的服务证书是否过期：

   ```
   kubectl get machines.cluster.x-k8s.io -n fleet-local <machine_name> -o yaml | yq .status.nodeRef.name
   ```

   将 `<machine_name>` 替换为上一步输出中的机器名称。

   :::note

   如果多个节点几乎在同一时间加入集群，你需要在所有这些节点上执行[解决方法](https://github.com/harvester/harvester/issues/3863#issuecomment-1539681311)。

   :::


- 相关 issue：
   - [[DOC/ENHANCEMENT] need to add cert-rotate feature, otherwise upgrade may stuck on Waiting for CAPI cluster fleet-local/local to be provisioned](https://github.com/harvester/harvester/issues/3863)
- 解决方法：
   - https://github.com/harvester/harvester/issues/3863#issuecomment-1539681311

---
